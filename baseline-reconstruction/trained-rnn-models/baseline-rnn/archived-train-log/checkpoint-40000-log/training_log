2025-10-18 10:36:59,823: Requested GPU 1 not available. Using GPU 0 instead.
2025-10-18 10:37:00,003: Using device: cuda:0
2025-10-18 10:37:01,042: Using torch.compile
2025-10-18 10:37:03,020: Initialized RNN decoding model
2025-10-18 10:37:03,021: OptimizedModule(
  (_orig_mod): GRUDecoder(
    (day_layer_activation): Softsign()
    (day_weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 512x512]
        (1): Parameter containing: [torch.float32 of size 512x512]
        (2): Parameter containing: [torch.float32 of size 512x512]
        (3): Parameter containing: [torch.float32 of size 512x512]
        (4): Parameter containing: [torch.float32 of size 512x512]
        (5): Parameter containing: [torch.float32 of size 512x512]
        (6): Parameter containing: [torch.float32 of size 512x512]
        (7): Parameter containing: [torch.float32 of size 512x512]
        (8): Parameter containing: [torch.float32 of size 512x512]
        (9): Parameter containing: [torch.float32 of size 512x512]
        (10): Parameter containing: [torch.float32 of size 512x512]
        (11): Parameter containing: [torch.float32 of size 512x512]
        (12): Parameter containing: [torch.float32 of size 512x512]
        (13): Parameter containing: [torch.float32 of size 512x512]
        (14): Parameter containing: [torch.float32 of size 512x512]
        (15): Parameter containing: [torch.float32 of size 512x512]
        (16): Parameter containing: [torch.float32 of size 512x512]
        (17): Parameter containing: [torch.float32 of size 512x512]
        (18): Parameter containing: [torch.float32 of size 512x512]
        (19): Parameter containing: [torch.float32 of size 512x512]
        (20): Parameter containing: [torch.float32 of size 512x512]
        (21): Parameter containing: [torch.float32 of size 512x512]
        (22): Parameter containing: [torch.float32 of size 512x512]
        (23): Parameter containing: [torch.float32 of size 512x512]
        (24): Parameter containing: [torch.float32 of size 512x512]
        (25): Parameter containing: [torch.float32 of size 512x512]
        (26): Parameter containing: [torch.float32 of size 512x512]
        (27): Parameter containing: [torch.float32 of size 512x512]
        (28): Parameter containing: [torch.float32 of size 512x512]
        (29): Parameter containing: [torch.float32 of size 512x512]
        (30): Parameter containing: [torch.float32 of size 512x512]
        (31): Parameter containing: [torch.float32 of size 512x512]
        (32): Parameter containing: [torch.float32 of size 512x512]
        (33): Parameter containing: [torch.float32 of size 512x512]
        (34): Parameter containing: [torch.float32 of size 512x512]
        (35): Parameter containing: [torch.float32 of size 512x512]
        (36): Parameter containing: [torch.float32 of size 512x512]
        (37): Parameter containing: [torch.float32 of size 512x512]
        (38): Parameter containing: [torch.float32 of size 512x512]
        (39): Parameter containing: [torch.float32 of size 512x512]
        (40): Parameter containing: [torch.float32 of size 512x512]
        (41): Parameter containing: [torch.float32 of size 512x512]
        (42): Parameter containing: [torch.float32 of size 512x512]
        (43): Parameter containing: [torch.float32 of size 512x512]
        (44): Parameter containing: [torch.float32 of size 512x512]
    )
    (day_biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1x512]
        (1): Parameter containing: [torch.float32 of size 1x512]
        (2): Parameter containing: [torch.float32 of size 1x512]
        (3): Parameter containing: [torch.float32 of size 1x512]
        (4): Parameter containing: [torch.float32 of size 1x512]
        (5): Parameter containing: [torch.float32 of size 1x512]
        (6): Parameter containing: [torch.float32 of size 1x512]
        (7): Parameter containing: [torch.float32 of size 1x512]
        (8): Parameter containing: [torch.float32 of size 1x512]
        (9): Parameter containing: [torch.float32 of size 1x512]
        (10): Parameter containing: [torch.float32 of size 1x512]
        (11): Parameter containing: [torch.float32 of size 1x512]
        (12): Parameter containing: [torch.float32 of size 1x512]
        (13): Parameter containing: [torch.float32 of size 1x512]
        (14): Parameter containing: [torch.float32 of size 1x512]
        (15): Parameter containing: [torch.float32 of size 1x512]
        (16): Parameter containing: [torch.float32 of size 1x512]
        (17): Parameter containing: [torch.float32 of size 1x512]
        (18): Parameter containing: [torch.float32 of size 1x512]
        (19): Parameter containing: [torch.float32 of size 1x512]
        (20): Parameter containing: [torch.float32 of size 1x512]
        (21): Parameter containing: [torch.float32 of size 1x512]
        (22): Parameter containing: [torch.float32 of size 1x512]
        (23): Parameter containing: [torch.float32 of size 1x512]
        (24): Parameter containing: [torch.float32 of size 1x512]
        (25): Parameter containing: [torch.float32 of size 1x512]
        (26): Parameter containing: [torch.float32 of size 1x512]
        (27): Parameter containing: [torch.float32 of size 1x512]
        (28): Parameter containing: [torch.float32 of size 1x512]
        (29): Parameter containing: [torch.float32 of size 1x512]
        (30): Parameter containing: [torch.float32 of size 1x512]
        (31): Parameter containing: [torch.float32 of size 1x512]
        (32): Parameter containing: [torch.float32 of size 1x512]
        (33): Parameter containing: [torch.float32 of size 1x512]
        (34): Parameter containing: [torch.float32 of size 1x512]
        (35): Parameter containing: [torch.float32 of size 1x512]
        (36): Parameter containing: [torch.float32 of size 1x512]
        (37): Parameter containing: [torch.float32 of size 1x512]
        (38): Parameter containing: [torch.float32 of size 1x512]
        (39): Parameter containing: [torch.float32 of size 1x512]
        (40): Parameter containing: [torch.float32 of size 1x512]
        (41): Parameter containing: [torch.float32 of size 1x512]
        (42): Parameter containing: [torch.float32 of size 1x512]
        (43): Parameter containing: [torch.float32 of size 1x512]
        (44): Parameter containing: [torch.float32 of size 1x512]
    )
    (day_layer_dropout): Dropout(p=0.2, inplace=False)
    (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.4)
    (out): Linear(in_features=768, out_features=41, bias=True)
  )
)
2025-10-18 10:37:03,024: Model has 44,315,177 parameters
2025-10-18 10:37:03,025: Model has 11,819,520 day-specific parameters | 26.67% of total parameters
2025-10-18 10:37:31,933: Successfully initialized datasets
2025-10-18 10:38:14,466: Train batch 0: loss: 753.82 grad norm: 302.81 time: 7.967
2025-10-18 10:38:14,467: Running test after training batch: 0
2025-10-18 10:39:03,134: Val batch 0: PER (avg): 1.2154 CTC Loss (avg): 714.9868 time: 48.666
2025-10-18 10:39:03,135: t15.2023.08.13 val PER: 1.1247
2025-10-18 10:39:03,136: t15.2023.08.18 val PER: 1.1459
2025-10-18 10:39:03,137: t15.2023.08.20 val PER: 1.1541
2025-10-18 10:39:03,138: t15.2023.08.25 val PER: 1.1611
2025-10-18 10:39:03,138: t15.2023.08.27 val PER: 1.0659
2025-10-18 10:39:03,139: t15.2023.09.01 val PER: 1.2248
2025-10-18 10:39:03,141: t15.2023.09.03 val PER: 1.1176
2025-10-18 10:39:03,142: t15.2023.09.24 val PER: 1.3337
2025-10-18 10:39:03,143: t15.2023.09.29 val PER: 1.2406
2025-10-18 10:39:03,143: t15.2023.10.01 val PER: 1.0542
2025-10-18 10:39:03,144: t15.2023.10.06 val PER: 1.2465
2025-10-18 10:39:03,145: t15.2023.10.08 val PER: 1.0406
2025-10-18 10:39:03,146: t15.2023.10.13 val PER: 1.1621
2025-10-18 10:39:03,146: t15.2023.10.15 val PER: 1.2037
2025-10-18 10:39:03,147: t15.2023.10.20 val PER: 1.3557
2025-10-18 10:39:03,148: t15.2023.10.22 val PER: 1.2996
2025-10-18 10:39:03,149: t15.2023.11.03 val PER: 1.2972
2025-10-18 10:39:03,150: t15.2023.11.04 val PER: 1.3891
2025-10-18 10:39:03,151: t15.2023.11.17 val PER: 1.6454
2025-10-18 10:39:03,152: t15.2023.11.19 val PER: 1.4351
2025-10-18 10:39:03,152: t15.2023.11.26 val PER: 1.2529
2025-10-18 10:39:03,154: t15.2023.12.03 val PER: 1.1838
2025-10-18 10:39:03,154: t15.2023.12.08 val PER: 1.2557
2025-10-18 10:39:03,155: t15.2023.12.10 val PER: 1.3219
2025-10-18 10:39:03,156: t15.2023.12.17 val PER: 1.0696
2025-10-18 10:39:03,157: t15.2023.12.29 val PER: 1.1640
2025-10-18 10:39:03,157: t15.2024.02.25 val PER: 1.1320
2025-10-18 10:39:03,158: t15.2024.03.08 val PER: 1.1550
2025-10-18 10:39:03,159: t15.2024.03.15 val PER: 1.1138
2025-10-18 10:39:03,160: t15.2024.03.17 val PER: 1.1722
2025-10-18 10:39:03,160: t15.2024.05.10 val PER: 1.2065
2025-10-18 10:39:03,161: t15.2024.06.14 val PER: 1.4132
2025-10-18 10:39:03,162: t15.2024.07.19 val PER: 0.9881
2025-10-18 10:39:03,163: t15.2024.07.21 val PER: 1.4193
2025-10-18 10:39:03,163: t15.2024.07.28 val PER: 1.4654
2025-10-18 10:39:03,164: t15.2025.01.10 val PER: 0.9518
2025-10-18 10:39:03,165: t15.2025.01.12 val PER: 1.4126
2025-10-18 10:39:03,165: t15.2025.03.14 val PER: 1.0178
2025-10-18 10:39:03,166: t15.2025.03.16 val PER: 1.4202
2025-10-18 10:39:03,167: t15.2025.03.30 val PER: 1.0805
2025-10-18 10:39:03,167: t15.2025.04.13 val PER: 1.3024
2025-10-18 10:39:03,168: New best test PER inf --> 1.2154
2025-10-18 10:39:03,169: Checkpointing model
2025-10-18 10:39:03,949: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 10:42:25,563: Train batch 200: loss: 93.76 grad norm: 18.19 time: 0.903
2025-10-18 10:45:43,904: Train batch 400: loss: 74.97 grad norm: 50.62 time: 1.072
2025-10-18 10:48:59,948: Train batch 600: loss: 58.92 grad norm: 43.93 time: 0.705
2025-10-18 10:52:17,997: Train batch 800: loss: 51.95 grad norm: 45.32 time: 1.050
2025-10-18 10:55:38,151: Train batch 1000: loss: 45.50 grad norm: 48.73 time: 0.908
2025-10-18 10:58:53,145: Train batch 1200: loss: 28.15 grad norm: 31.10 time: 1.107
2025-10-18 11:02:10,438: Train batch 1400: loss: 16.88 grad norm: 29.33 time: 0.887
2025-10-18 11:05:27,481: Train batch 1600: loss: 28.82 grad norm: 40.23 time: 0.942
2025-10-18 11:08:46,091: Train batch 1800: loss: 26.20 grad norm: 51.03 time: 0.721
2025-10-18 11:12:05,687: Train batch 2000: loss: 12.81 grad norm: 33.08 time: 0.905
2025-10-18 11:12:05,689: Running test after training batch: 2000
2025-10-18 11:12:45,716: Val batch 2000: PER (avg): 0.2252 CTC Loss (avg): 22.8470 time: 40.026
2025-10-18 11:12:45,717: t15.2023.08.13 val PER: 0.1861
2025-10-18 11:12:45,718: t15.2023.08.18 val PER: 0.1844
2025-10-18 11:12:45,719: t15.2023.08.20 val PER: 0.1620
2025-10-18 11:12:45,720: t15.2023.08.25 val PER: 0.1717
2025-10-18 11:12:45,721: t15.2023.08.27 val PER: 0.2765
2025-10-18 11:12:45,722: t15.2023.09.01 val PER: 0.1502
2025-10-18 11:12:45,722: t15.2023.09.03 val PER: 0.2197
2025-10-18 11:12:45,723: t15.2023.09.24 val PER: 0.1711
2025-10-18 11:12:45,723: t15.2023.09.29 val PER: 0.1927
2025-10-18 11:12:45,724: t15.2023.10.01 val PER: 0.2398
2025-10-18 11:12:45,725: t15.2023.10.06 val PER: 0.1679
2025-10-18 11:12:45,725: t15.2023.10.08 val PER: 0.3221
2025-10-18 11:12:45,726: t15.2023.10.13 val PER: 0.2832
2025-10-18 11:12:45,728: t15.2023.10.15 val PER: 0.2274
2025-10-18 11:12:45,728: t15.2023.10.20 val PER: 0.2416
2025-10-18 11:12:45,729: t15.2023.10.22 val PER: 0.1938
2025-10-18 11:12:45,730: t15.2023.11.03 val PER: 0.2307
2025-10-18 11:12:45,730: t15.2023.11.04 val PER: 0.0410
2025-10-18 11:12:45,731: t15.2023.11.17 val PER: 0.1135
2025-10-18 11:12:45,731: t15.2023.11.19 val PER: 0.1018
2025-10-18 11:12:45,733: t15.2023.11.26 val PER: 0.2377
2025-10-18 11:12:45,734: t15.2023.12.03 val PER: 0.2059
2025-10-18 11:12:45,735: t15.2023.12.08 val PER: 0.1997
2025-10-18 11:12:45,735: t15.2023.12.10 val PER: 0.1577
2025-10-18 11:12:45,736: t15.2023.12.17 val PER: 0.2193
2025-10-18 11:12:45,737: t15.2023.12.29 val PER: 0.2107
2025-10-18 11:12:45,738: t15.2024.02.25 val PER: 0.1728
2025-10-18 11:12:45,738: t15.2024.03.08 val PER: 0.2959
2025-10-18 11:12:45,739: t15.2024.03.15 val PER: 0.2908
2025-10-18 11:12:45,741: t15.2024.03.17 val PER: 0.2204
2025-10-18 11:12:45,742: t15.2024.05.10 val PER: 0.2377
2025-10-18 11:12:45,742: t15.2024.06.14 val PER: 0.2303
2025-10-18 11:12:45,743: t15.2024.07.19 val PER: 0.3052
2025-10-18 11:12:45,743: t15.2024.07.21 val PER: 0.1621
2025-10-18 11:12:45,744: t15.2024.07.28 val PER: 0.2103
2025-10-18 11:12:45,745: t15.2025.01.10 val PER: 0.3774
2025-10-18 11:12:45,746: t15.2025.01.12 val PER: 0.2348
2025-10-18 11:12:45,746: t15.2025.03.14 val PER: 0.4053
2025-10-18 11:12:45,748: t15.2025.03.16 val PER: 0.2709
2025-10-18 11:12:45,748: t15.2025.03.30 val PER: 0.3575
2025-10-18 11:12:45,749: t15.2025.04.13 val PER: 0.2782
2025-10-18 11:12:45,749: New best test PER 1.2154 --> 0.2252
2025-10-18 11:12:45,750: Checkpointing model
2025-10-18 11:12:47,148: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 11:16:06,190: Train batch 2200: loss: 17.68 grad norm: 38.49 time: 0.976
2025-10-18 11:19:23,986: Train batch 2400: loss: 15.61 grad norm: 34.61 time: 0.944
2025-10-18 11:22:43,571: Train batch 2600: loss: 9.55 grad norm: 23.17 time: 0.719
2025-10-18 11:26:03,941: Train batch 2800: loss: 10.45 grad norm: 31.81 time: 1.158
2025-10-18 11:29:21,440: Train batch 3000: loss: 13.14 grad norm: 31.36 time: 0.921
2025-10-18 11:32:37,690: Train batch 3200: loss: 11.72 grad norm: 32.83 time: 0.773
2025-10-18 11:35:53,873: Train batch 3400: loss: 7.00 grad norm: 25.08 time: 0.924
2025-10-18 11:39:14,061: Train batch 3600: loss: 6.86 grad norm: 22.93 time: 1.017
2025-10-18 11:42:32,991: Train batch 3800: loss: 6.67 grad norm: 27.59 time: 0.844
2025-10-18 11:45:53,193: Train batch 4000: loss: 4.08 grad norm: 18.64 time: 0.873
2025-10-18 11:45:53,194: Running test after training batch: 4000
2025-10-18 11:46:35,104: Val batch 4000: PER (avg): 0.1577 CTC Loss (avg): 17.4698 time: 41.909
2025-10-18 11:46:35,105: t15.2023.08.13 val PER: 0.1175
2025-10-18 11:46:35,106: t15.2023.08.18 val PER: 0.1215
2025-10-18 11:46:35,106: t15.2023.08.20 val PER: 0.1009
2025-10-18 11:46:35,107: t15.2023.08.25 val PER: 0.1009
2025-10-18 11:46:35,108: t15.2023.08.27 val PER: 0.1833
2025-10-18 11:46:35,109: t15.2023.09.01 val PER: 0.0771
2025-10-18 11:46:35,109: t15.2023.09.03 val PER: 0.1603
2025-10-18 11:46:35,110: t15.2023.09.24 val PER: 0.1214
2025-10-18 11:46:35,111: t15.2023.09.29 val PER: 0.1436
2025-10-18 11:46:35,111: t15.2023.10.01 val PER: 0.1783
2025-10-18 11:46:35,112: t15.2023.10.06 val PER: 0.1173
2025-10-18 11:46:35,113: t15.2023.10.08 val PER: 0.2436
2025-10-18 11:46:35,113: t15.2023.10.13 val PER: 0.2188
2025-10-18 11:46:35,114: t15.2023.10.15 val PER: 0.1615
2025-10-18 11:46:35,115: t15.2023.10.20 val PER: 0.1879
2025-10-18 11:46:35,115: t15.2023.10.22 val PER: 0.1214
2025-10-18 11:46:35,118: t15.2023.11.03 val PER: 0.1818
2025-10-18 11:46:35,118: t15.2023.11.04 val PER: 0.0273
2025-10-18 11:46:35,119: t15.2023.11.17 val PER: 0.0373
2025-10-18 11:46:35,120: t15.2023.11.19 val PER: 0.0599
2025-10-18 11:46:35,120: t15.2023.11.26 val PER: 0.1406
2025-10-18 11:46:35,121: t15.2023.12.03 val PER: 0.1071
2025-10-18 11:46:35,122: t15.2023.12.08 val PER: 0.1132
2025-10-18 11:46:35,122: t15.2023.12.10 val PER: 0.1078
2025-10-18 11:46:35,123: t15.2023.12.17 val PER: 0.1507
2025-10-18 11:46:35,124: t15.2023.12.29 val PER: 0.1373
2025-10-18 11:46:35,125: t15.2024.02.25 val PER: 0.1250
2025-10-18 11:46:35,126: t15.2024.03.08 val PER: 0.2347
2025-10-18 11:46:35,126: t15.2024.03.15 val PER: 0.2045
2025-10-18 11:46:35,127: t15.2024.03.17 val PER: 0.1423
2025-10-18 11:46:35,128: t15.2024.05.10 val PER: 0.1516
2025-10-18 11:46:35,128: t15.2024.06.14 val PER: 0.1593
2025-10-18 11:46:35,130: t15.2024.07.19 val PER: 0.2340
2025-10-18 11:46:35,130: t15.2024.07.21 val PER: 0.0986
2025-10-18 11:46:35,131: t15.2024.07.28 val PER: 0.1581
2025-10-18 11:46:35,131: t15.2025.01.10 val PER: 0.3058
2025-10-18 11:46:35,133: t15.2025.01.12 val PER: 0.1647
2025-10-18 11:46:35,133: t15.2025.03.14 val PER: 0.3639
2025-10-18 11:46:35,134: t15.2025.03.16 val PER: 0.1924
2025-10-18 11:46:35,135: t15.2025.03.30 val PER: 0.2966
2025-10-18 11:46:35,136: t15.2025.04.13 val PER: 0.2068
2025-10-18 11:46:35,136: New best test PER 0.2252 --> 0.1577
2025-10-18 11:46:35,137: Checkpointing model
2025-10-18 11:46:36,580: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 11:49:54,254: Train batch 4200: loss: 5.35 grad norm: 26.12 time: 1.080
2025-10-18 11:53:17,369: Train batch 4400: loss: 7.38 grad norm: 42.27 time: 0.993
2025-10-18 11:56:36,594: Train batch 4600: loss: 4.08 grad norm: 26.55 time: 0.939
2025-10-18 11:59:58,706: Train batch 4800: loss: 7.20 grad norm: 28.44 time: 1.135
2025-10-18 12:03:19,359: Train batch 5000: loss: 4.96 grad norm: 23.47 time: 0.894
2025-10-18 12:06:38,623: Train batch 5200: loss: 7.72 grad norm: 33.87 time: 0.913
2025-10-18 12:09:56,701: Train batch 5400: loss: 2.32 grad norm: 19.99 time: 1.077
2025-10-18 12:13:12,149: Train batch 5600: loss: 4.99 grad norm: 23.93 time: 1.053
2025-10-18 12:16:31,123: Train batch 5800: loss: 3.28 grad norm: 20.84 time: 0.990
2025-10-18 12:19:52,007: Train batch 6000: loss: 4.80 grad norm: 28.62 time: 1.014
2025-10-18 12:19:52,008: Running test after training batch: 6000
2025-10-18 12:20:31,753: Val batch 6000: PER (avg): 0.1375 CTC Loss (avg): 17.0313 time: 39.744
2025-10-18 12:20:31,754: t15.2023.08.13 val PER: 0.0967
2025-10-18 12:20:31,755: t15.2023.08.18 val PER: 0.1031
2025-10-18 12:20:31,755: t15.2023.08.20 val PER: 0.0778
2025-10-18 12:20:31,756: t15.2023.08.25 val PER: 0.1024
2025-10-18 12:20:31,757: t15.2023.08.27 val PER: 0.1688
2025-10-18 12:20:31,758: t15.2023.09.01 val PER: 0.0601
2025-10-18 12:20:31,759: t15.2023.09.03 val PER: 0.1437
2025-10-18 12:20:31,760: t15.2023.09.24 val PER: 0.1068
2025-10-18 12:20:31,761: t15.2023.09.29 val PER: 0.1200
2025-10-18 12:20:31,762: t15.2023.10.01 val PER: 0.1651
2025-10-18 12:20:31,763: t15.2023.10.06 val PER: 0.1012
2025-10-18 12:20:31,764: t15.2023.10.08 val PER: 0.2287
2025-10-18 12:20:31,764: t15.2023.10.13 val PER: 0.1831
2025-10-18 12:20:31,765: t15.2023.10.15 val PER: 0.1470
2025-10-18 12:20:31,766: t15.2023.10.20 val PER: 0.1779
2025-10-18 12:20:31,766: t15.2023.10.22 val PER: 0.1069
2025-10-18 12:20:31,767: t15.2023.11.03 val PER: 0.1581
2025-10-18 12:20:31,768: t15.2023.11.04 val PER: 0.0239
2025-10-18 12:20:31,769: t15.2023.11.17 val PER: 0.0311
2025-10-18 12:20:31,770: t15.2023.11.19 val PER: 0.0279
2025-10-18 12:20:31,770: t15.2023.11.26 val PER: 0.0971
2025-10-18 12:20:31,771: t15.2023.12.03 val PER: 0.0819
2025-10-18 12:20:31,772: t15.2023.12.08 val PER: 0.0925
2025-10-18 12:20:31,773: t15.2023.12.10 val PER: 0.0920
2025-10-18 12:20:31,774: t15.2023.12.17 val PER: 0.1185
2025-10-18 12:20:31,774: t15.2023.12.29 val PER: 0.1078
2025-10-18 12:20:31,775: t15.2024.02.25 val PER: 0.0983
2025-10-18 12:20:31,776: t15.2024.03.08 val PER: 0.2134
2025-10-18 12:20:31,776: t15.2024.03.15 val PER: 0.2020
2025-10-18 12:20:31,777: t15.2024.03.17 val PER: 0.1248
2025-10-18 12:20:31,779: t15.2024.05.10 val PER: 0.1575
2025-10-18 12:20:31,779: t15.2024.06.14 val PER: 0.1372
2025-10-18 12:20:31,780: t15.2024.07.19 val PER: 0.2011
2025-10-18 12:20:31,781: t15.2024.07.21 val PER: 0.0814
2025-10-18 12:20:31,781: t15.2024.07.28 val PER: 0.1301
2025-10-18 12:20:31,782: t15.2025.01.10 val PER: 0.2645
2025-10-18 12:20:31,783: t15.2025.01.12 val PER: 0.1447
2025-10-18 12:20:31,784: t15.2025.03.14 val PER: 0.3254
2025-10-18 12:20:31,784: t15.2025.03.16 val PER: 0.1950
2025-10-18 12:20:31,785: t15.2025.03.30 val PER: 0.2598
2025-10-18 12:20:31,786: t15.2025.04.13 val PER: 0.2254
2025-10-18 12:20:31,787: New best test PER 0.1577 --> 0.1375
2025-10-18 12:20:31,787: Checkpointing model
2025-10-18 12:20:33,204: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 12:23:54,272: Train batch 6200: loss: 5.49 grad norm: 28.87 time: 1.273
2025-10-18 12:27:18,316: Train batch 6400: loss: 6.60 grad norm: 27.33 time: 1.116
2025-10-18 12:30:39,203: Train batch 6600: loss: 5.97 grad norm: 36.91 time: 1.049
2025-10-18 12:33:54,053: Train batch 6800: loss: 3.17 grad norm: 26.26 time: 0.740
2025-10-18 12:37:11,732: Train batch 7000: loss: 4.78 grad norm: 28.92 time: 0.986
2025-10-18 12:40:31,366: Train batch 7200: loss: 2.41 grad norm: 21.25 time: 1.166
2025-10-18 12:43:51,734: Train batch 7400: loss: 4.23 grad norm: 27.97 time: 1.363
2025-10-18 12:47:10,532: Train batch 7600: loss: 2.62 grad norm: 23.92 time: 1.036
2025-10-18 12:50:26,254: Train batch 7800: loss: 3.37 grad norm: 30.40 time: 1.102
2025-10-18 12:53:44,391: Train batch 8000: loss: 2.25 grad norm: 19.08 time: 0.764
2025-10-18 12:53:44,392: Running test after training batch: 8000
2025-10-18 12:54:26,291: Val batch 8000: PER (avg): 0.1310 CTC Loss (avg): 17.2807 time: 41.899
2025-10-18 12:54:26,292: t15.2023.08.13 val PER: 0.0873
2025-10-18 12:54:26,293: t15.2023.08.18 val PER: 0.0947
2025-10-18 12:54:26,294: t15.2023.08.20 val PER: 0.0715
2025-10-18 12:54:26,295: t15.2023.08.25 val PER: 0.0828
2025-10-18 12:54:26,296: t15.2023.08.27 val PER: 0.1688
2025-10-18 12:54:26,296: t15.2023.09.01 val PER: 0.0593
2025-10-18 12:54:26,297: t15.2023.09.03 val PER: 0.1485
2025-10-18 12:54:26,298: t15.2023.09.24 val PER: 0.0971
2025-10-18 12:54:26,299: t15.2023.09.29 val PER: 0.1232
2025-10-18 12:54:26,300: t15.2023.10.01 val PER: 0.1671
2025-10-18 12:54:26,300: t15.2023.10.06 val PER: 0.0904
2025-10-18 12:54:26,301: t15.2023.10.08 val PER: 0.2111
2025-10-18 12:54:26,302: t15.2023.10.13 val PER: 0.1800
2025-10-18 12:54:26,303: t15.2023.10.15 val PER: 0.1384
2025-10-18 12:54:26,303: t15.2023.10.20 val PER: 0.1812
2025-10-18 12:54:26,304: t15.2023.10.22 val PER: 0.0969
2025-10-18 12:54:26,307: t15.2023.11.03 val PER: 0.1513
2025-10-18 12:54:26,308: t15.2023.11.04 val PER: 0.0273
2025-10-18 12:54:26,309: t15.2023.11.17 val PER: 0.0404
2025-10-18 12:54:26,309: t15.2023.11.19 val PER: 0.0319
2025-10-18 12:54:26,310: t15.2023.11.26 val PER: 0.0855
2025-10-18 12:54:26,311: t15.2023.12.03 val PER: 0.0735
2025-10-18 12:54:26,312: t15.2023.12.08 val PER: 0.0739
2025-10-18 12:54:26,312: t15.2023.12.10 val PER: 0.0670
2025-10-18 12:54:26,313: t15.2023.12.17 val PER: 0.1154
2025-10-18 12:54:26,314: t15.2023.12.29 val PER: 0.1009
2025-10-18 12:54:26,315: t15.2024.02.25 val PER: 0.0969
2025-10-18 12:54:26,316: t15.2024.03.08 val PER: 0.1807
2025-10-18 12:54:26,316: t15.2024.03.15 val PER: 0.1957
2025-10-18 12:54:26,317: t15.2024.03.17 val PER: 0.1192
2025-10-18 12:54:26,318: t15.2024.05.10 val PER: 0.1471
2025-10-18 12:54:26,319: t15.2024.06.14 val PER: 0.1404
2025-10-18 12:54:26,319: t15.2024.07.19 val PER: 0.1879
2025-10-18 12:54:26,321: t15.2024.07.21 val PER: 0.0731
2025-10-18 12:54:26,322: t15.2024.07.28 val PER: 0.1110
2025-10-18 12:54:26,322: t15.2025.01.10 val PER: 0.2645
2025-10-18 12:54:26,323: t15.2025.01.12 val PER: 0.1440
2025-10-18 12:54:26,324: t15.2025.03.14 val PER: 0.3432
2025-10-18 12:54:26,324: t15.2025.03.16 val PER: 0.1846
2025-10-18 12:54:26,325: t15.2025.03.30 val PER: 0.2713
2025-10-18 12:54:26,326: t15.2025.04.13 val PER: 0.2140
2025-10-18 12:54:26,327: New best test PER 0.1375 --> 0.1310
2025-10-18 12:54:26,327: Checkpointing model
2025-10-18 12:54:27,755: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 12:57:46,827: Train batch 8200: loss: 3.11 grad norm: 31.57 time: 1.115
2025-10-18 13:01:02,725: Train batch 8400: loss: 4.32 grad norm: 25.30 time: 1.072
2025-10-18 13:04:26,449: Train batch 8600: loss: 2.08 grad norm: 18.93 time: 0.816
2025-10-18 13:07:43,057: Train batch 8800: loss: 3.24 grad norm: 27.26 time: 0.766
2025-10-18 13:11:03,190: Train batch 9000: loss: 2.53 grad norm: 26.20 time: 0.759
2025-10-18 13:14:25,566: Train batch 9200: loss: 2.16 grad norm: 18.25 time: 0.834
2025-10-18 13:17:44,437: Train batch 9400: loss: 1.61 grad norm: 16.79 time: 1.102
2025-10-18 13:21:04,421: Train batch 9600: loss: 1.91 grad norm: 22.42 time: 0.834
2025-10-18 13:24:18,109: Train batch 9800: loss: 1.36 grad norm: 13.49 time: 0.898
2025-10-18 13:27:38,169: Train batch 10000: loss: 4.35 grad norm: 34.13 time: 1.031
2025-10-18 13:27:38,170: Running test after training batch: 10000
2025-10-18 13:28:21,844: Val batch 10000: PER (avg): 0.1245 CTC Loss (avg): 18.0572 time: 43.673
2025-10-18 13:28:21,845: t15.2023.08.13 val PER: 0.0759
2025-10-18 13:28:21,846: t15.2023.08.18 val PER: 0.0964
2025-10-18 13:28:21,846: t15.2023.08.20 val PER: 0.0643
2025-10-18 13:28:21,847: t15.2023.08.25 val PER: 0.0904
2025-10-18 13:28:21,848: t15.2023.08.27 val PER: 0.1801
2025-10-18 13:28:21,849: t15.2023.09.01 val PER: 0.0625
2025-10-18 13:28:21,849: t15.2023.09.03 val PER: 0.1342
2025-10-18 13:28:21,850: t15.2023.09.24 val PER: 0.0971
2025-10-18 13:28:21,851: t15.2023.09.29 val PER: 0.1136
2025-10-18 13:28:21,851: t15.2023.10.01 val PER: 0.1546
2025-10-18 13:28:21,852: t15.2023.10.06 val PER: 0.0904
2025-10-18 13:28:21,853: t15.2023.10.08 val PER: 0.1922
2025-10-18 13:28:21,854: t15.2023.10.13 val PER: 0.1707
2025-10-18 13:28:21,854: t15.2023.10.15 val PER: 0.1332
2025-10-18 13:28:21,855: t15.2023.10.20 val PER: 0.1779
2025-10-18 13:28:21,856: t15.2023.10.22 val PER: 0.1125
2025-10-18 13:28:21,856: t15.2023.11.03 val PER: 0.1682
2025-10-18 13:28:21,857: t15.2023.11.04 val PER: 0.0273
2025-10-18 13:28:21,858: t15.2023.11.17 val PER: 0.0249
2025-10-18 13:28:21,858: t15.2023.11.19 val PER: 0.0279
2025-10-18 13:28:21,859: t15.2023.11.26 val PER: 0.0768
2025-10-18 13:28:21,861: t15.2023.12.03 val PER: 0.0683
2025-10-18 13:28:21,861: t15.2023.12.08 val PER: 0.0619
2025-10-18 13:28:21,862: t15.2023.12.10 val PER: 0.0539
2025-10-18 13:28:21,862: t15.2023.12.17 val PER: 0.1102
2025-10-18 13:28:21,863: t15.2023.12.29 val PER: 0.0933
2025-10-18 13:28:21,864: t15.2024.02.25 val PER: 0.0927
2025-10-18 13:28:21,864: t15.2024.03.08 val PER: 0.2034
2025-10-18 13:28:21,865: t15.2024.03.15 val PER: 0.1757
2025-10-18 13:28:21,866: t15.2024.03.17 val PER: 0.1123
2025-10-18 13:28:21,867: t15.2024.05.10 val PER: 0.1189
2025-10-18 13:28:21,868: t15.2024.06.14 val PER: 0.1278
2025-10-18 13:28:21,868: t15.2024.07.19 val PER: 0.1846
2025-10-18 13:28:21,869: t15.2024.07.21 val PER: 0.0731
2025-10-18 13:28:21,870: t15.2024.07.28 val PER: 0.0941
2025-10-18 13:28:21,870: t15.2025.01.10 val PER: 0.2590
2025-10-18 13:28:21,871: t15.2025.01.12 val PER: 0.1263
2025-10-18 13:28:21,873: t15.2025.03.14 val PER: 0.3210
2025-10-18 13:28:21,873: t15.2025.03.16 val PER: 0.1872
2025-10-18 13:28:21,874: t15.2025.03.30 val PER: 0.2483
2025-10-18 13:28:21,874: t15.2025.04.13 val PER: 0.2011
2025-10-18 13:28:21,875: New best test PER 0.1310 --> 0.1245
2025-10-18 13:28:21,876: Checkpointing model
2025-10-18 13:28:23,290: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 13:31:38,373: Train batch 10200: loss: 1.10 grad norm: 14.84 time: 1.035
2025-10-18 13:34:58,996: Train batch 10400: loss: 1.71 grad norm: 30.30 time: 1.051
2025-10-18 13:38:16,034: Train batch 10600: loss: 1.82 grad norm: 20.03 time: 0.885
2025-10-18 13:41:33,290: Train batch 10800: loss: 1.69 grad norm: 16.83 time: 0.970
2025-10-18 13:44:57,795: Train batch 11000: loss: 1.37 grad norm: 19.53 time: 1.161
2025-10-18 13:48:15,410: Train batch 11200: loss: 2.94 grad norm: 27.63 time: 0.867
2025-10-18 13:51:31,325: Train batch 11400: loss: 1.40 grad norm: 16.40 time: 0.719
2025-10-18 13:54:50,552: Train batch 11600: loss: 1.97 grad norm: 23.93 time: 0.745
2025-10-18 13:58:06,869: Train batch 11800: loss: 0.95 grad norm: 17.51 time: 1.117
2025-10-18 14:01:20,637: Train batch 12000: loss: 1.12 grad norm: 28.74 time: 1.116
2025-10-18 14:01:20,638: Running test after training batch: 12000
2025-10-18 14:02:02,859: Val batch 12000: PER (avg): 0.1255 CTC Loss (avg): 19.2307 time: 42.220
2025-10-18 14:02:02,860: t15.2023.08.13 val PER: 0.0863
2025-10-18 14:02:02,861: t15.2023.08.18 val PER: 0.0863
2025-10-18 14:02:02,861: t15.2023.08.20 val PER: 0.0715
2025-10-18 14:02:02,862: t15.2023.08.25 val PER: 0.0979
2025-10-18 14:02:02,863: t15.2023.08.27 val PER: 0.1817
2025-10-18 14:02:02,864: t15.2023.09.01 val PER: 0.0584
2025-10-18 14:02:02,865: t15.2023.09.03 val PER: 0.1235
2025-10-18 14:02:02,866: t15.2023.09.24 val PER: 0.0983
2025-10-18 14:02:02,867: t15.2023.09.29 val PER: 0.1321
2025-10-18 14:02:02,868: t15.2023.10.01 val PER: 0.1559
2025-10-18 14:02:02,868: t15.2023.10.06 val PER: 0.0904
2025-10-18 14:02:02,869: t15.2023.10.08 val PER: 0.2003
2025-10-18 14:02:02,870: t15.2023.10.13 val PER: 0.1746
2025-10-18 14:02:02,871: t15.2023.10.15 val PER: 0.1397
2025-10-18 14:02:02,871: t15.2023.10.20 val PER: 0.1678
2025-10-18 14:02:02,872: t15.2023.10.22 val PER: 0.1058
2025-10-18 14:02:02,873: t15.2023.11.03 val PER: 0.1459
2025-10-18 14:02:02,874: t15.2023.11.04 val PER: 0.0239
2025-10-18 14:02:02,875: t15.2023.11.17 val PER: 0.0202
2025-10-18 14:02:02,876: t15.2023.11.19 val PER: 0.0200
2025-10-18 14:02:02,877: t15.2023.11.26 val PER: 0.0710
2025-10-18 14:02:02,877: t15.2023.12.03 val PER: 0.0788
2025-10-18 14:02:02,878: t15.2023.12.08 val PER: 0.0593
2025-10-18 14:02:02,879: t15.2023.12.10 val PER: 0.0670
2025-10-18 14:02:02,879: t15.2023.12.17 val PER: 0.0988
2025-10-18 14:02:02,880: t15.2023.12.29 val PER: 0.1002
2025-10-18 14:02:02,882: t15.2024.02.25 val PER: 0.0857
2025-10-18 14:02:02,882: t15.2024.03.08 val PER: 0.2077
2025-10-18 14:02:02,883: t15.2024.03.15 val PER: 0.1907
2025-10-18 14:02:02,884: t15.2024.03.17 val PER: 0.1088
2025-10-18 14:02:02,885: t15.2024.05.10 val PER: 0.1426
2025-10-18 14:02:02,886: t15.2024.06.14 val PER: 0.1372
2025-10-18 14:02:02,887: t15.2024.07.19 val PER: 0.1721
2025-10-18 14:02:02,887: t15.2024.07.21 val PER: 0.0786
2025-10-18 14:02:02,888: t15.2024.07.28 val PER: 0.1132
2025-10-18 14:02:02,889: t15.2025.01.10 val PER: 0.2713
2025-10-18 14:02:02,890: t15.2025.01.12 val PER: 0.1216
2025-10-18 14:02:02,891: t15.2025.03.14 val PER: 0.3003
2025-10-18 14:02:02,892: t15.2025.03.16 val PER: 0.1806
2025-10-18 14:02:02,893: t15.2025.03.30 val PER: 0.2437
2025-10-18 14:02:02,894: t15.2025.04.13 val PER: 0.2011
2025-10-18 14:05:18,041: Train batch 12200: loss: 1.78 grad norm: 47.89 time: 1.054
2025-10-18 14:08:34,559: Train batch 12400: loss: 0.69 grad norm: 20.26 time: 1.347
2025-10-18 14:11:53,819: Train batch 12600: loss: 1.44 grad norm: 23.74 time: 0.963
2025-10-18 14:15:11,077: Train batch 12800: loss: 3.87 grad norm: 25.50 time: 1.166
2025-10-18 14:18:30,379: Train batch 13000: loss: 1.67 grad norm: 21.09 time: 1.263
2025-10-18 14:21:46,996: Train batch 13200: loss: 0.83 grad norm: 15.50 time: 0.866
2025-10-18 14:25:05,040: Train batch 13400: loss: 0.70 grad norm: 13.94 time: 0.850
2025-10-18 14:28:24,940: Train batch 13600: loss: 1.14 grad norm: 14.89 time: 1.151
2025-10-18 14:31:45,843: Train batch 13800: loss: 0.77 grad norm: 13.48 time: 0.905
2025-10-18 14:35:08,261: Train batch 14000: loss: 0.78 grad norm: 16.20 time: 0.905
2025-10-18 14:35:08,262: Running test after training batch: 14000
2025-10-18 14:35:51,627: Val batch 14000: PER (avg): 0.1202 CTC Loss (avg): 19.5162 time: 43.365
2025-10-18 14:35:51,628: t15.2023.08.13 val PER: 0.0811
2025-10-18 14:35:51,629: t15.2023.08.18 val PER: 0.0813
2025-10-18 14:35:51,630: t15.2023.08.20 val PER: 0.0508
2025-10-18 14:35:51,631: t15.2023.08.25 val PER: 0.0813
2025-10-18 14:35:51,631: t15.2023.08.27 val PER: 0.1559
2025-10-18 14:35:51,632: t15.2023.09.01 val PER: 0.0552
2025-10-18 14:35:51,634: t15.2023.09.03 val PER: 0.1330
2025-10-18 14:35:51,635: t15.2023.09.24 val PER: 0.0886
2025-10-18 14:35:51,635: t15.2023.09.29 val PER: 0.1149
2025-10-18 14:35:51,636: t15.2023.10.01 val PER: 0.1493
2025-10-18 14:35:51,637: t15.2023.10.06 val PER: 0.0732
2025-10-18 14:35:51,638: t15.2023.10.08 val PER: 0.1881
2025-10-18 14:35:51,639: t15.2023.10.13 val PER: 0.1606
2025-10-18 14:35:51,640: t15.2023.10.15 val PER: 0.1318
2025-10-18 14:35:51,641: t15.2023.10.20 val PER: 0.1510
2025-10-18 14:35:51,641: t15.2023.10.22 val PER: 0.1069
2025-10-18 14:35:51,642: t15.2023.11.03 val PER: 0.1526
2025-10-18 14:35:51,643: t15.2023.11.04 val PER: 0.0171
2025-10-18 14:35:51,643: t15.2023.11.17 val PER: 0.0358
2025-10-18 14:35:51,644: t15.2023.11.19 val PER: 0.0299
2025-10-18 14:35:51,645: t15.2023.11.26 val PER: 0.0768
2025-10-18 14:35:51,645: t15.2023.12.03 val PER: 0.0672
2025-10-18 14:35:51,647: t15.2023.12.08 val PER: 0.0546
2025-10-18 14:35:51,648: t15.2023.12.10 val PER: 0.0565
2025-10-18 14:35:51,649: t15.2023.12.17 val PER: 0.1071
2025-10-18 14:35:51,650: t15.2023.12.29 val PER: 0.0837
2025-10-18 14:35:51,650: t15.2024.02.25 val PER: 0.0787
2025-10-18 14:35:51,651: t15.2024.03.08 val PER: 0.2105
2025-10-18 14:35:51,652: t15.2024.03.15 val PER: 0.1832
2025-10-18 14:35:51,653: t15.2024.03.17 val PER: 0.1046
2025-10-18 14:35:51,653: t15.2024.05.10 val PER: 0.1263
2025-10-18 14:35:51,654: t15.2024.06.14 val PER: 0.1293
2025-10-18 14:35:51,655: t15.2024.07.19 val PER: 0.1819
2025-10-18 14:35:51,656: t15.2024.07.21 val PER: 0.0703
2025-10-18 14:35:51,656: t15.2024.07.28 val PER: 0.1059
2025-10-18 14:35:51,658: t15.2025.01.10 val PER: 0.2521
2025-10-18 14:35:51,658: t15.2025.01.12 val PER: 0.1316
2025-10-18 14:35:51,659: t15.2025.03.14 val PER: 0.3254
2025-10-18 14:35:51,660: t15.2025.03.16 val PER: 0.1623
2025-10-18 14:35:51,661: t15.2025.03.30 val PER: 0.2402
2025-10-18 14:35:51,661: t15.2025.04.13 val PER: 0.1983
2025-10-18 14:35:51,662: New best test PER 0.1245 --> 0.1202
2025-10-18 14:35:51,662: Checkpointing model
2025-10-18 14:35:53,092: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 14:39:08,958: Train batch 14200: loss: 0.74 grad norm: 17.02 time: 0.838
2025-10-18 14:42:27,412: Train batch 14400: loss: 0.18 grad norm: 5.85 time: 0.842
2025-10-18 14:45:47,435: Train batch 14600: loss: 0.93 grad norm: 20.83 time: 0.960
2025-10-18 14:49:02,508: Train batch 14800: loss: 0.97 grad norm: 17.89 time: 0.945
2025-10-18 14:52:20,102: Train batch 15000: loss: 0.86 grad norm: 11.16 time: 1.294
2025-10-18 14:55:37,620: Train batch 15200: loss: 1.03 grad norm: 43.11 time: 0.968
2025-10-18 14:58:53,853: Train batch 15400: loss: 0.66 grad norm: 20.36 time: 1.136
2025-10-18 15:02:15,758: Train batch 15600: loss: 1.41 grad norm: 13.31 time: 0.892
2025-10-18 15:05:33,886: Train batch 15800: loss: 0.71 grad norm: 11.13 time: 0.687
2025-10-18 15:08:50,614: Train batch 16000: loss: 0.96 grad norm: 15.05 time: 0.933
2025-10-18 15:08:50,616: Running test after training batch: 16000
2025-10-18 15:09:36,738: Val batch 16000: PER (avg): 0.1181 CTC Loss (avg): 19.6140 time: 46.122
2025-10-18 15:09:36,739: t15.2023.08.13 val PER: 0.0832
2025-10-18 15:09:36,740: t15.2023.08.18 val PER: 0.0746
2025-10-18 15:09:36,741: t15.2023.08.20 val PER: 0.0612
2025-10-18 15:09:36,742: t15.2023.08.25 val PER: 0.0783
2025-10-18 15:09:36,742: t15.2023.08.27 val PER: 0.1624
2025-10-18 15:09:36,743: t15.2023.09.01 val PER: 0.0536
2025-10-18 15:09:36,744: t15.2023.09.03 val PER: 0.1473
2025-10-18 15:09:36,745: t15.2023.09.24 val PER: 0.0910
2025-10-18 15:09:36,745: t15.2023.09.29 val PER: 0.1117
2025-10-18 15:09:36,747: t15.2023.10.01 val PER: 0.1446
2025-10-18 15:09:36,748: t15.2023.10.06 val PER: 0.0840
2025-10-18 15:09:36,748: t15.2023.10.08 val PER: 0.1949
2025-10-18 15:09:36,749: t15.2023.10.13 val PER: 0.1606
2025-10-18 15:09:36,750: t15.2023.10.15 val PER: 0.1272
2025-10-18 15:09:36,751: t15.2023.10.20 val PER: 0.1812
2025-10-18 15:09:36,752: t15.2023.10.22 val PER: 0.0980
2025-10-18 15:09:36,753: t15.2023.11.03 val PER: 0.1459
2025-10-18 15:09:36,754: t15.2023.11.04 val PER: 0.0307
2025-10-18 15:09:36,755: t15.2023.11.17 val PER: 0.0187
2025-10-18 15:09:36,755: t15.2023.11.19 val PER: 0.0240
2025-10-18 15:09:36,756: t15.2023.11.26 val PER: 0.0754
2025-10-18 15:09:36,757: t15.2023.12.03 val PER: 0.0609
2025-10-18 15:09:36,758: t15.2023.12.08 val PER: 0.0599
2025-10-18 15:09:36,759: t15.2023.12.10 val PER: 0.0342
2025-10-18 15:09:36,760: t15.2023.12.17 val PER: 0.0998
2025-10-18 15:09:36,761: t15.2023.12.29 val PER: 0.0858
2025-10-18 15:09:36,762: t15.2024.02.25 val PER: 0.0801
2025-10-18 15:09:36,762: t15.2024.03.08 val PER: 0.1935
2025-10-18 15:09:36,763: t15.2024.03.15 val PER: 0.1764
2025-10-18 15:09:36,764: t15.2024.03.17 val PER: 0.1053
2025-10-18 15:09:36,765: t15.2024.05.10 val PER: 0.1337
2025-10-18 15:09:36,766: t15.2024.06.14 val PER: 0.1309
2025-10-18 15:09:36,766: t15.2024.07.19 val PER: 0.1879
2025-10-18 15:09:36,767: t15.2024.07.21 val PER: 0.0641
2025-10-18 15:09:36,768: t15.2024.07.28 val PER: 0.1007
2025-10-18 15:09:36,768: t15.2025.01.10 val PER: 0.2548
2025-10-18 15:09:36,770: t15.2025.01.12 val PER: 0.1155
2025-10-18 15:09:36,770: t15.2025.03.14 val PER: 0.2944
2025-10-18 15:09:36,771: t15.2025.03.16 val PER: 0.1715
2025-10-18 15:09:36,772: t15.2025.03.30 val PER: 0.2471
2025-10-18 15:09:36,772: t15.2025.04.13 val PER: 0.1812
2025-10-18 15:09:36,773: New best test PER 0.1202 --> 0.1181
2025-10-18 15:09:36,773: Checkpointing model
2025-10-18 15:09:38,221: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 15:12:55,517: Train batch 16200: loss: 0.60 grad norm: 12.87 time: 0.840
2025-10-18 15:16:13,636: Train batch 16400: loss: 1.54 grad norm: 21.60 time: 0.900
2025-10-18 15:19:34,539: Train batch 16600: loss: 0.26 grad norm: 16.00 time: 0.894
2025-10-18 15:22:55,903: Train batch 16800: loss: 0.79 grad norm: 24.08 time: 1.204
2025-10-18 15:26:15,918: Train batch 17000: loss: 0.66 grad norm: 12.79 time: 0.946
2025-10-18 15:29:33,029: Train batch 17200: loss: 1.16 grad norm: 13.21 time: 0.855
2025-10-18 15:32:52,361: Train batch 17400: loss: 0.42 grad norm: 8.79 time: 1.214
2025-10-18 15:36:10,063: Train batch 17600: loss: 1.00 grad norm: 27.72 time: 0.846
2025-10-18 15:39:30,647: Train batch 17800: loss: 0.95 grad norm: 21.97 time: 1.364
2025-10-18 15:42:49,083: Train batch 18000: loss: 0.84 grad norm: 18.50 time: 1.207
2025-10-18 15:42:49,084: Running test after training batch: 18000
2025-10-18 15:43:29,370: Val batch 18000: PER (avg): 0.1181 CTC Loss (avg): 20.7056 time: 40.285
2025-10-18 15:43:29,371: t15.2023.08.13 val PER: 0.0832
2025-10-18 15:43:29,372: t15.2023.08.18 val PER: 0.0780
2025-10-18 15:43:29,372: t15.2023.08.20 val PER: 0.0556
2025-10-18 15:43:29,374: t15.2023.08.25 val PER: 0.0904
2025-10-18 15:43:29,374: t15.2023.08.27 val PER: 0.1656
2025-10-18 15:43:29,376: t15.2023.09.01 val PER: 0.0471
2025-10-18 15:43:29,376: t15.2023.09.03 val PER: 0.1318
2025-10-18 15:43:29,377: t15.2023.09.24 val PER: 0.0801
2025-10-18 15:43:29,378: t15.2023.09.29 val PER: 0.1123
2025-10-18 15:43:29,378: t15.2023.10.01 val PER: 0.1460
2025-10-18 15:43:29,379: t15.2023.10.06 val PER: 0.0807
2025-10-18 15:43:29,380: t15.2023.10.08 val PER: 0.1962
2025-10-18 15:43:29,381: t15.2023.10.13 val PER: 0.1645
2025-10-18 15:43:29,382: t15.2023.10.15 val PER: 0.1378
2025-10-18 15:43:29,383: t15.2023.10.20 val PER: 0.1611
2025-10-18 15:43:29,384: t15.2023.10.22 val PER: 0.1036
2025-10-18 15:43:29,385: t15.2023.11.03 val PER: 0.1554
2025-10-18 15:43:29,386: t15.2023.11.04 val PER: 0.0307
2025-10-18 15:43:29,387: t15.2023.11.17 val PER: 0.0140
2025-10-18 15:43:29,387: t15.2023.11.19 val PER: 0.0240
2025-10-18 15:43:29,388: t15.2023.11.26 val PER: 0.0652
2025-10-18 15:43:29,389: t15.2023.12.03 val PER: 0.0630
2025-10-18 15:43:29,389: t15.2023.12.08 val PER: 0.0519
2025-10-18 15:43:29,391: t15.2023.12.10 val PER: 0.0512
2025-10-18 15:43:29,391: t15.2023.12.17 val PER: 0.1060
2025-10-18 15:43:29,392: t15.2023.12.29 val PER: 0.0933
2025-10-18 15:43:29,393: t15.2024.02.25 val PER: 0.0801
2025-10-18 15:43:29,393: t15.2024.03.08 val PER: 0.1849
2025-10-18 15:43:29,395: t15.2024.03.15 val PER: 0.1920
2025-10-18 15:43:29,395: t15.2024.03.17 val PER: 0.0934
2025-10-18 15:43:29,396: t15.2024.05.10 val PER: 0.1322
2025-10-18 15:43:29,397: t15.2024.06.14 val PER: 0.1199
2025-10-18 15:43:29,397: t15.2024.07.19 val PER: 0.1767
2025-10-18 15:43:29,398: t15.2024.07.21 val PER: 0.0690
2025-10-18 15:43:29,399: t15.2024.07.28 val PER: 0.0897
2025-10-18 15:43:29,399: t15.2025.01.10 val PER: 0.2438
2025-10-18 15:43:29,400: t15.2025.01.12 val PER: 0.1201
2025-10-18 15:43:29,401: t15.2025.03.14 val PER: 0.3210
2025-10-18 15:43:29,402: t15.2025.03.16 val PER: 0.1702
2025-10-18 15:43:29,403: t15.2025.03.30 val PER: 0.2425
2025-10-18 15:43:29,404: t15.2025.04.13 val PER: 0.1840
2025-10-18 15:43:29,404: New best test PER 0.1181 --> 0.1181
2025-10-18 15:43:29,405: Checkpointing model
2025-10-18 15:43:30,835: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 15:46:49,437: Train batch 18200: loss: 2.03 grad norm: 31.91 time: 1.151
2025-10-18 15:50:09,607: Train batch 18400: loss: 0.35 grad norm: 6.85 time: 1.095
2025-10-18 15:53:29,322: Train batch 18600: loss: 0.67 grad norm: 15.29 time: 1.035
2025-10-18 15:56:50,438: Train batch 18800: loss: 0.57 grad norm: 13.07 time: 0.969
2025-10-18 16:00:13,155: Train batch 19000: loss: 0.38 grad norm: 8.58 time: 1.076
2025-10-18 16:03:32,395: Train batch 19200: loss: 0.20 grad norm: 5.71 time: 1.051
2025-10-18 16:06:52,341: Train batch 19400: loss: 0.32 grad norm: 16.99 time: 1.077
2025-10-18 16:10:14,110: Train batch 19600: loss: 0.32 grad norm: 8.26 time: 0.969
2025-10-18 16:13:36,044: Train batch 19800: loss: 0.20 grad norm: 7.63 time: 0.787
2025-10-18 16:16:58,479: Train batch 20000: loss: 0.44 grad norm: 10.90 time: 0.872
2025-10-18 16:16:58,481: Running test after training batch: 20000
2025-10-18 16:17:41,953: Val batch 20000: PER (avg): 0.1156 CTC Loss (avg): 20.4240 time: 43.471
2025-10-18 16:17:41,954: t15.2023.08.13 val PER: 0.0780
2025-10-18 16:17:41,955: t15.2023.08.18 val PER: 0.0763
2025-10-18 16:17:41,956: t15.2023.08.20 val PER: 0.0643
2025-10-18 16:17:41,957: t15.2023.08.25 val PER: 0.0873
2025-10-18 16:17:41,958: t15.2023.08.27 val PER: 0.1527
2025-10-18 16:17:41,959: t15.2023.09.01 val PER: 0.0552
2025-10-18 16:17:41,959: t15.2023.09.03 val PER: 0.1354
2025-10-18 16:17:41,960: t15.2023.09.24 val PER: 0.0886
2025-10-18 16:17:41,961: t15.2023.09.29 val PER: 0.1008
2025-10-18 16:17:41,962: t15.2023.10.01 val PER: 0.1460
2025-10-18 16:17:41,963: t15.2023.10.06 val PER: 0.0926
2025-10-18 16:17:41,964: t15.2023.10.08 val PER: 0.1976
2025-10-18 16:17:41,965: t15.2023.10.13 val PER: 0.1629
2025-10-18 16:17:41,965: t15.2023.10.15 val PER: 0.1180
2025-10-18 16:17:41,966: t15.2023.10.20 val PER: 0.1577
2025-10-18 16:17:41,967: t15.2023.10.22 val PER: 0.0969
2025-10-18 16:17:41,968: t15.2023.11.03 val PER: 0.1459
2025-10-18 16:17:41,968: t15.2023.11.04 val PER: 0.0273
2025-10-18 16:17:41,969: t15.2023.11.17 val PER: 0.0140
2025-10-18 16:17:41,971: t15.2023.11.19 val PER: 0.0259
2025-10-18 16:17:41,971: t15.2023.11.26 val PER: 0.0681
2025-10-18 16:17:41,972: t15.2023.12.03 val PER: 0.0693
2025-10-18 16:17:41,973: t15.2023.12.08 val PER: 0.0439
2025-10-18 16:17:41,973: t15.2023.12.10 val PER: 0.0486
2025-10-18 16:17:41,975: t15.2023.12.17 val PER: 0.0977
2025-10-18 16:17:41,975: t15.2023.12.29 val PER: 0.0851
2025-10-18 16:17:41,976: t15.2024.02.25 val PER: 0.0758
2025-10-18 16:17:41,977: t15.2024.03.08 val PER: 0.1679
2025-10-18 16:17:41,978: t15.2024.03.15 val PER: 0.1864
2025-10-18 16:17:41,979: t15.2024.03.17 val PER: 0.0934
2025-10-18 16:17:41,980: t15.2024.05.10 val PER: 0.1337
2025-10-18 16:17:41,980: t15.2024.06.14 val PER: 0.1183
2025-10-18 16:17:41,981: t15.2024.07.19 val PER: 0.1773
2025-10-18 16:17:41,982: t15.2024.07.21 val PER: 0.0586
2025-10-18 16:17:41,982: t15.2024.07.28 val PER: 0.1007
2025-10-18 16:17:41,984: t15.2025.01.10 val PER: 0.2438
2025-10-18 16:17:41,985: t15.2025.01.12 val PER: 0.1201
2025-10-18 16:17:41,985: t15.2025.03.14 val PER: 0.3033
2025-10-18 16:17:41,986: t15.2025.03.16 val PER: 0.1584
2025-10-18 16:17:41,987: t15.2025.03.30 val PER: 0.2460
2025-10-18 16:17:41,988: t15.2025.04.13 val PER: 0.1926
2025-10-18 16:17:41,988: New best test PER 0.1181 --> 0.1156
2025-10-18 16:17:41,989: Checkpointing model
2025-10-18 16:17:43,462: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 16:20:59,761: Train batch 20200: loss: 0.36 grad norm: 7.22 time: 0.921
2025-10-18 16:24:18,139: Train batch 20400: loss: 0.09 grad norm: 3.37 time: 1.369
2025-10-18 16:27:34,218: Train batch 20600: loss: 0.50 grad norm: 10.92 time: 1.170
2025-10-18 16:30:51,759: Train batch 20800: loss: 0.60 grad norm: 72.21 time: 1.057
2025-10-18 16:34:09,089: Train batch 21000: loss: 0.14 grad norm: 5.64 time: 0.872
2025-10-18 16:37:29,228: Train batch 21200: loss: 0.29 grad norm: 10.74 time: 0.892
2025-10-18 16:40:44,107: Train batch 21400: loss: 0.36 grad norm: 9.28 time: 0.782
2025-10-18 16:43:58,609: Train batch 21600: loss: 0.31 grad norm: 7.09 time: 0.914
2025-10-18 16:47:17,817: Train batch 21800: loss: 0.25 grad norm: 7.89 time: 0.802
2025-10-18 16:50:36,355: Train batch 22000: loss: 0.60 grad norm: 12.68 time: 1.095
2025-10-18 16:50:36,357: Running test after training batch: 22000
2025-10-18 16:51:15,939: Val batch 22000: PER (avg): 0.1149 CTC Loss (avg): 21.0031 time: 39.582
2025-10-18 16:51:15,940: t15.2023.08.13 val PER: 0.0769
2025-10-18 16:51:15,941: t15.2023.08.18 val PER: 0.0754
2025-10-18 16:51:15,941: t15.2023.08.20 val PER: 0.0588
2025-10-18 16:51:15,942: t15.2023.08.25 val PER: 0.0768
2025-10-18 16:51:15,943: t15.2023.08.27 val PER: 0.1576
2025-10-18 16:51:15,944: t15.2023.09.01 val PER: 0.0511
2025-10-18 16:51:15,944: t15.2023.09.03 val PER: 0.1366
2025-10-18 16:51:15,945: t15.2023.09.24 val PER: 0.0825
2025-10-18 16:51:15,946: t15.2023.09.29 val PER: 0.1047
2025-10-18 16:51:15,947: t15.2023.10.01 val PER: 0.1440
2025-10-18 16:51:15,947: t15.2023.10.06 val PER: 0.0840
2025-10-18 16:51:15,948: t15.2023.10.08 val PER: 0.1908
2025-10-18 16:51:15,948: t15.2023.10.13 val PER: 0.1637
2025-10-18 16:51:15,949: t15.2023.10.15 val PER: 0.1272
2025-10-18 16:51:15,951: t15.2023.10.20 val PER: 0.1779
2025-10-18 16:51:15,951: t15.2023.10.22 val PER: 0.1024
2025-10-18 16:51:15,952: t15.2023.11.03 val PER: 0.1425
2025-10-18 16:51:15,952: t15.2023.11.04 val PER: 0.0341
2025-10-18 16:51:15,953: t15.2023.11.17 val PER: 0.0171
2025-10-18 16:51:15,953: t15.2023.11.19 val PER: 0.0240
2025-10-18 16:51:15,954: t15.2023.11.26 val PER: 0.0623
2025-10-18 16:51:15,955: t15.2023.12.03 val PER: 0.0683
2025-10-18 16:51:15,955: t15.2023.12.08 val PER: 0.0506
2025-10-18 16:51:15,957: t15.2023.12.10 val PER: 0.0552
2025-10-18 16:51:15,958: t15.2023.12.17 val PER: 0.0925
2025-10-18 16:51:15,958: t15.2023.12.29 val PER: 0.0858
2025-10-18 16:51:15,959: t15.2024.02.25 val PER: 0.0857
2025-10-18 16:51:15,960: t15.2024.03.08 val PER: 0.1863
2025-10-18 16:51:15,961: t15.2024.03.15 val PER: 0.1720
2025-10-18 16:51:15,961: t15.2024.03.17 val PER: 0.0900
2025-10-18 16:51:15,962: t15.2024.05.10 val PER: 0.1322
2025-10-18 16:51:15,963: t15.2024.06.14 val PER: 0.1120
2025-10-18 16:51:15,964: t15.2024.07.19 val PER: 0.1780
2025-10-18 16:51:15,964: t15.2024.07.21 val PER: 0.0607
2025-10-18 16:51:15,965: t15.2024.07.28 val PER: 0.1081
2025-10-18 16:51:15,966: t15.2025.01.10 val PER: 0.2521
2025-10-18 16:51:15,967: t15.2025.01.12 val PER: 0.1139
2025-10-18 16:51:15,968: t15.2025.03.14 val PER: 0.2959
2025-10-18 16:51:15,968: t15.2025.03.16 val PER: 0.1610
2025-10-18 16:51:15,970: t15.2025.03.30 val PER: 0.2333
2025-10-18 16:51:15,970: t15.2025.04.13 val PER: 0.1840
2025-10-18 16:51:15,971: New best test PER 0.1156 --> 0.1149
2025-10-18 16:51:15,972: Checkpointing model
2025-10-18 16:51:17,447: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 16:54:38,781: Train batch 22200: loss: 0.15 grad norm: 5.08 time: 0.810
2025-10-18 16:57:58,744: Train batch 22400: loss: 0.33 grad norm: 10.60 time: 1.158
2025-10-18 17:01:18,410: Train batch 22600: loss: 0.13 grad norm: 4.88 time: 0.972
2025-10-18 17:04:33,443: Train batch 22800: loss: 0.46 grad norm: 13.78 time: 0.901
2025-10-18 17:07:49,302: Train batch 23000: loss: 0.31 grad norm: 8.76 time: 1.262
2025-10-18 17:11:07,633: Train batch 23200: loss: 0.32 grad norm: 7.81 time: 1.028
2025-10-18 17:14:27,464: Train batch 23400: loss: 0.19 grad norm: 4.70 time: 0.924
2025-10-18 17:17:49,467: Train batch 23600: loss: 0.27 grad norm: 6.28 time: 1.115
2025-10-18 17:21:06,567: Train batch 23800: loss: 0.33 grad norm: 8.17 time: 0.930
2025-10-18 17:24:28,177: Train batch 24000: loss: 0.12 grad norm: 4.92 time: 1.045
2025-10-18 17:24:28,178: Running test after training batch: 24000
2025-10-18 17:25:09,360: Val batch 24000: PER (avg): 0.1111 CTC Loss (avg): 20.4402 time: 41.182
2025-10-18 17:25:09,361: t15.2023.08.13 val PER: 0.0759
2025-10-18 17:25:09,362: t15.2023.08.18 val PER: 0.0712
2025-10-18 17:25:09,363: t15.2023.08.20 val PER: 0.0516
2025-10-18 17:25:09,364: t15.2023.08.25 val PER: 0.0783
2025-10-18 17:25:09,365: t15.2023.08.27 val PER: 0.1656
2025-10-18 17:25:09,366: t15.2023.09.01 val PER: 0.0471
2025-10-18 17:25:09,367: t15.2023.09.03 val PER: 0.1259
2025-10-18 17:25:09,368: t15.2023.09.24 val PER: 0.0813
2025-10-18 17:25:09,368: t15.2023.09.29 val PER: 0.1002
2025-10-18 17:25:09,369: t15.2023.10.01 val PER: 0.1347
2025-10-18 17:25:09,370: t15.2023.10.06 val PER: 0.0786
2025-10-18 17:25:09,371: t15.2023.10.08 val PER: 0.1813
2025-10-18 17:25:09,372: t15.2023.10.13 val PER: 0.1528
2025-10-18 17:25:09,373: t15.2023.10.15 val PER: 0.1252
2025-10-18 17:25:09,374: t15.2023.10.20 val PER: 0.1544
2025-10-18 17:25:09,375: t15.2023.10.22 val PER: 0.1013
2025-10-18 17:25:09,375: t15.2023.11.03 val PER: 0.1465
2025-10-18 17:25:09,377: t15.2023.11.04 val PER: 0.0375
2025-10-18 17:25:09,377: t15.2023.11.17 val PER: 0.0171
2025-10-18 17:25:09,378: t15.2023.11.19 val PER: 0.0160
2025-10-18 17:25:09,379: t15.2023.11.26 val PER: 0.0587
2025-10-18 17:25:09,380: t15.2023.12.03 val PER: 0.0588
2025-10-18 17:25:09,381: t15.2023.12.08 val PER: 0.0519
2025-10-18 17:25:09,381: t15.2023.12.10 val PER: 0.0460
2025-10-18 17:25:09,382: t15.2023.12.17 val PER: 0.0852
2025-10-18 17:25:09,383: t15.2023.12.29 val PER: 0.0728
2025-10-18 17:25:09,384: t15.2024.02.25 val PER: 0.0674
2025-10-18 17:25:09,384: t15.2024.03.08 val PER: 0.1764
2025-10-18 17:25:09,385: t15.2024.03.15 val PER: 0.1720
2025-10-18 17:25:09,386: t15.2024.03.17 val PER: 0.0921
2025-10-18 17:25:09,387: t15.2024.05.10 val PER: 0.1322
2025-10-18 17:25:09,388: t15.2024.06.14 val PER: 0.1215
2025-10-18 17:25:09,388: t15.2024.07.19 val PER: 0.1674
2025-10-18 17:25:09,389: t15.2024.07.21 val PER: 0.0586
2025-10-18 17:25:09,390: t15.2024.07.28 val PER: 0.0985
2025-10-18 17:25:09,391: t15.2025.01.10 val PER: 0.2590
2025-10-18 17:25:09,392: t15.2025.01.12 val PER: 0.1147
2025-10-18 17:25:09,393: t15.2025.03.14 val PER: 0.2988
2025-10-18 17:25:09,394: t15.2025.03.16 val PER: 0.1584
2025-10-18 17:25:09,394: t15.2025.03.30 val PER: 0.2322
2025-10-18 17:25:09,395: t15.2025.04.13 val PER: 0.1883
2025-10-18 17:25:09,395: New best test PER 0.1149 --> 0.1111
2025-10-18 17:25:09,396: Checkpointing model
2025-10-18 17:25:10,860: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 17:28:28,623: Train batch 24200: loss: 0.38 grad norm: 8.23 time: 1.217
2025-10-18 17:31:48,126: Train batch 24400: loss: 0.25 grad norm: 6.26 time: 1.259
2025-10-18 17:35:11,569: Train batch 24600: loss: 0.28 grad norm: 6.91 time: 1.313
2025-10-18 17:38:29,753: Train batch 24800: loss: 0.24 grad norm: 6.93 time: 0.788
2025-10-18 17:41:46,912: Train batch 25000: loss: 0.21 grad norm: 6.56 time: 1.066
2025-10-18 17:44:58,911: Train batch 25200: loss: 0.09 grad norm: 6.83 time: 1.017
2025-10-18 17:48:18,865: Train batch 25400: loss: 0.10 grad norm: 4.57 time: 0.957
2025-10-18 17:51:35,666: Train batch 25600: loss: 0.18 grad norm: 6.53 time: 0.917
2025-10-18 17:54:52,150: Train batch 25800: loss: 0.15 grad norm: 5.68 time: 1.131
2025-10-18 17:58:11,492: Train batch 26000: loss: 0.40 grad norm: 10.92 time: 0.816
2025-10-18 17:58:11,493: Running test after training batch: 26000
2025-10-18 17:58:51,417: Val batch 26000: PER (avg): 0.1109 CTC Loss (avg): 20.9790 time: 39.924
2025-10-18 17:58:51,418: t15.2023.08.13 val PER: 0.0800
2025-10-18 17:58:51,419: t15.2023.08.18 val PER: 0.0738
2025-10-18 17:58:51,420: t15.2023.08.20 val PER: 0.0532
2025-10-18 17:58:51,420: t15.2023.08.25 val PER: 0.0828
2025-10-18 17:58:51,421: t15.2023.08.27 val PER: 0.1463
2025-10-18 17:58:51,422: t15.2023.09.01 val PER: 0.0471
2025-10-18 17:58:51,423: t15.2023.09.03 val PER: 0.1306
2025-10-18 17:58:51,424: t15.2023.09.24 val PER: 0.0850
2025-10-18 17:58:51,425: t15.2023.09.29 val PER: 0.0989
2025-10-18 17:58:51,425: t15.2023.10.01 val PER: 0.1380
2025-10-18 17:58:51,426: t15.2023.10.06 val PER: 0.0818
2025-10-18 17:58:51,427: t15.2023.10.08 val PER: 0.1854
2025-10-18 17:58:51,428: t15.2023.10.13 val PER: 0.1567
2025-10-18 17:58:51,428: t15.2023.10.15 val PER: 0.1187
2025-10-18 17:58:51,429: t15.2023.10.20 val PER: 0.1678
2025-10-18 17:58:51,430: t15.2023.10.22 val PER: 0.1069
2025-10-18 17:58:51,430: t15.2023.11.03 val PER: 0.1452
2025-10-18 17:58:51,431: t15.2023.11.04 val PER: 0.0273
2025-10-18 17:58:51,431: t15.2023.11.17 val PER: 0.0156
2025-10-18 17:58:51,432: t15.2023.11.19 val PER: 0.0100
2025-10-18 17:58:51,433: t15.2023.11.26 val PER: 0.0609
2025-10-18 17:58:51,434: t15.2023.12.03 val PER: 0.0599
2025-10-18 17:58:51,434: t15.2023.12.08 val PER: 0.0486
2025-10-18 17:58:51,435: t15.2023.12.10 val PER: 0.0460
2025-10-18 17:58:51,436: t15.2023.12.17 val PER: 0.0936
2025-10-18 17:58:51,437: t15.2023.12.29 val PER: 0.0755
2025-10-18 17:58:51,438: t15.2024.02.25 val PER: 0.0702
2025-10-18 17:58:51,438: t15.2024.03.08 val PER: 0.1892
2025-10-18 17:58:51,440: t15.2024.03.15 val PER: 0.1714
2025-10-18 17:58:51,440: t15.2024.03.17 val PER: 0.0886
2025-10-18 17:58:51,441: t15.2024.05.10 val PER: 0.1248
2025-10-18 17:58:51,442: t15.2024.06.14 val PER: 0.1104
2025-10-18 17:58:51,443: t15.2024.07.19 val PER: 0.1674
2025-10-18 17:58:51,444: t15.2024.07.21 val PER: 0.0621
2025-10-18 17:58:51,445: t15.2024.07.28 val PER: 0.0919
2025-10-18 17:58:51,446: t15.2025.01.10 val PER: 0.2562
2025-10-18 17:58:51,446: t15.2025.01.12 val PER: 0.1147
2025-10-18 17:58:51,447: t15.2025.03.14 val PER: 0.2988
2025-10-18 17:58:51,448: t15.2025.03.16 val PER: 0.1610
2025-10-18 17:58:51,448: t15.2025.03.30 val PER: 0.2184
2025-10-18 17:58:51,449: t15.2025.04.13 val PER: 0.1826
2025-10-18 17:58:51,450: New best test PER 0.1111 --> 0.1109
2025-10-18 17:58:51,450: Checkpointing model
2025-10-18 17:58:52,934: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 18:02:10,816: Train batch 26200: loss: 0.53 grad norm: 14.35 time: 0.972
2025-10-18 18:05:31,776: Train batch 26400: loss: 0.20 grad norm: 9.17 time: 1.296
2025-10-18 18:08:46,588: Train batch 26600: loss: 0.11 grad norm: 5.71 time: 0.836
2025-10-18 18:12:06,877: Train batch 26800: loss: 0.10 grad norm: 7.59 time: 0.743
2025-10-18 18:15:27,365: Train batch 27000: loss: 0.19 grad norm: 6.23 time: 0.887
2025-10-18 18:18:44,288: Train batch 27200: loss: 0.23 grad norm: 7.54 time: 0.914
2025-10-18 18:22:03,196: Train batch 27400: loss: 0.38 grad norm: 12.69 time: 0.922
2025-10-18 18:25:19,042: Train batch 27600: loss: 0.11 grad norm: 7.46 time: 1.113
2025-10-18 18:28:36,207: Train batch 27800: loss: 0.30 grad norm: 8.68 time: 0.893
2025-10-18 18:31:56,516: Train batch 28000: loss: 0.40 grad norm: 10.13 time: 0.968
2025-10-18 18:31:56,517: Running test after training batch: 28000
2025-10-18 18:32:36,990: Val batch 28000: PER (avg): 0.1100 CTC Loss (avg): 20.8612 time: 40.472
2025-10-18 18:32:36,991: t15.2023.08.13 val PER: 0.0738
2025-10-18 18:32:36,992: t15.2023.08.18 val PER: 0.0763
2025-10-18 18:32:36,993: t15.2023.08.20 val PER: 0.0540
2025-10-18 18:32:36,993: t15.2023.08.25 val PER: 0.0828
2025-10-18 18:32:36,994: t15.2023.08.27 val PER: 0.1527
2025-10-18 18:32:36,995: t15.2023.09.01 val PER: 0.0463
2025-10-18 18:32:36,996: t15.2023.09.03 val PER: 0.1306
2025-10-18 18:32:36,997: t15.2023.09.24 val PER: 0.0837
2025-10-18 18:32:36,997: t15.2023.09.29 val PER: 0.1021
2025-10-18 18:32:36,998: t15.2023.10.01 val PER: 0.1347
2025-10-18 18:32:36,999: t15.2023.10.06 val PER: 0.0775
2025-10-18 18:32:37,000: t15.2023.10.08 val PER: 0.1827
2025-10-18 18:32:37,000: t15.2023.10.13 val PER: 0.1458
2025-10-18 18:32:37,001: t15.2023.10.15 val PER: 0.1127
2025-10-18 18:32:37,002: t15.2023.10.20 val PER: 0.1544
2025-10-18 18:32:37,003: t15.2023.10.22 val PER: 0.1013
2025-10-18 18:32:37,004: t15.2023.11.03 val PER: 0.1472
2025-10-18 18:32:37,005: t15.2023.11.04 val PER: 0.0307
2025-10-18 18:32:37,005: t15.2023.11.17 val PER: 0.0156
2025-10-18 18:32:37,006: t15.2023.11.19 val PER: 0.0100
2025-10-18 18:32:37,007: t15.2023.11.26 val PER: 0.0630
2025-10-18 18:32:37,007: t15.2023.12.03 val PER: 0.0588
2025-10-18 18:32:37,009: t15.2023.12.08 val PER: 0.0499
2025-10-18 18:32:37,009: t15.2023.12.10 val PER: 0.0420
2025-10-18 18:32:37,010: t15.2023.12.17 val PER: 0.0790
2025-10-18 18:32:37,011: t15.2023.12.29 val PER: 0.0824
2025-10-18 18:32:37,011: t15.2024.02.25 val PER: 0.0730
2025-10-18 18:32:37,012: t15.2024.03.08 val PER: 0.1707
2025-10-18 18:32:37,013: t15.2024.03.15 val PER: 0.1682
2025-10-18 18:32:37,014: t15.2024.03.17 val PER: 0.0865
2025-10-18 18:32:37,015: t15.2024.05.10 val PER: 0.1278
2025-10-18 18:32:37,016: t15.2024.06.14 val PER: 0.1278
2025-10-18 18:32:37,016: t15.2024.07.19 val PER: 0.1707
2025-10-18 18:32:37,017: t15.2024.07.21 val PER: 0.0566
2025-10-18 18:32:37,018: t15.2024.07.28 val PER: 0.0963
2025-10-18 18:32:37,019: t15.2025.01.10 val PER: 0.2534
2025-10-18 18:32:37,020: t15.2025.01.12 val PER: 0.1101
2025-10-18 18:32:37,021: t15.2025.03.14 val PER: 0.3077
2025-10-18 18:32:37,021: t15.2025.03.16 val PER: 0.1623
2025-10-18 18:32:37,022: t15.2025.03.30 val PER: 0.2195
2025-10-18 18:32:37,023: t15.2025.04.13 val PER: 0.1854
2025-10-18 18:32:37,024: New best test PER 0.1109 --> 0.1100
2025-10-18 18:32:37,025: Checkpointing model
2025-10-18 18:32:38,515: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 18:35:56,223: Train batch 28200: loss: 0.16 grad norm: 7.39 time: 0.806
2025-10-18 18:39:16,233: Train batch 28400: loss: 0.15 grad norm: 5.17 time: 0.928
2025-10-18 18:42:34,625: Train batch 28600: loss: 0.26 grad norm: 9.18 time: 1.250
2025-10-18 18:45:55,057: Train batch 28800: loss: 0.25 grad norm: 14.23 time: 1.047
2025-10-18 18:49:12,128: Train batch 29000: loss: 0.10 grad norm: 3.70 time: 0.877
2025-10-18 18:52:31,564: Train batch 29200: loss: 0.17 grad norm: 5.76 time: 0.933
2025-10-18 18:55:57,090: Train batch 29400: loss: 0.16 grad norm: 5.97 time: 1.304
2025-10-18 18:59:16,478: Train batch 29600: loss: 0.18 grad norm: 6.22 time: 1.370
2025-10-18 19:02:35,882: Train batch 29800: loss: 0.08 grad norm: 7.25 time: 0.960
2025-10-18 19:05:51,725: Train batch 30000: loss: 0.22 grad norm: 11.14 time: 0.987
2025-10-18 19:05:51,726: Running test after training batch: 30000
2025-10-18 19:06:31,767: Val batch 30000: PER (avg): 0.1085 CTC Loss (avg): 20.9147 time: 40.040
2025-10-18 19:06:31,768: t15.2023.08.13 val PER: 0.0738
2025-10-18 19:06:31,769: t15.2023.08.18 val PER: 0.0704
2025-10-18 19:06:31,770: t15.2023.08.20 val PER: 0.0477
2025-10-18 19:06:31,770: t15.2023.08.25 val PER: 0.0904
2025-10-18 19:06:31,771: t15.2023.08.27 val PER: 0.1543
2025-10-18 19:06:31,772: t15.2023.09.01 val PER: 0.0471
2025-10-18 19:06:31,773: t15.2023.09.03 val PER: 0.1295
2025-10-18 19:06:31,774: t15.2023.09.24 val PER: 0.0765
2025-10-18 19:06:31,775: t15.2023.09.29 val PER: 0.1059
2025-10-18 19:06:31,775: t15.2023.10.01 val PER: 0.1361
2025-10-18 19:06:31,776: t15.2023.10.06 val PER: 0.0753
2025-10-18 19:06:31,777: t15.2023.10.08 val PER: 0.1786
2025-10-18 19:06:31,777: t15.2023.10.13 val PER: 0.1466
2025-10-18 19:06:31,778: t15.2023.10.15 val PER: 0.1200
2025-10-18 19:06:31,778: t15.2023.10.20 val PER: 0.1544
2025-10-18 19:06:31,779: t15.2023.10.22 val PER: 0.1036
2025-10-18 19:06:31,780: t15.2023.11.03 val PER: 0.1445
2025-10-18 19:06:31,780: t15.2023.11.04 val PER: 0.0341
2025-10-18 19:06:31,781: t15.2023.11.17 val PER: 0.0187
2025-10-18 19:06:31,782: t15.2023.11.19 val PER: 0.0140
2025-10-18 19:06:31,783: t15.2023.11.26 val PER: 0.0594
2025-10-18 19:06:31,784: t15.2023.12.03 val PER: 0.0525
2025-10-18 19:06:31,784: t15.2023.12.08 val PER: 0.0439
2025-10-18 19:06:31,785: t15.2023.12.10 val PER: 0.0434
2025-10-18 19:06:31,786: t15.2023.12.17 val PER: 0.0811
2025-10-18 19:06:31,789: t15.2023.12.29 val PER: 0.0803
2025-10-18 19:06:31,789: t15.2024.02.25 val PER: 0.0716
2025-10-18 19:06:31,790: t15.2024.03.08 val PER: 0.1792
2025-10-18 19:06:31,791: t15.2024.03.15 val PER: 0.1720
2025-10-18 19:06:31,792: t15.2024.03.17 val PER: 0.0886
2025-10-18 19:06:31,793: t15.2024.05.10 val PER: 0.1278
2025-10-18 19:06:31,794: t15.2024.06.14 val PER: 0.1167
2025-10-18 19:06:31,794: t15.2024.07.19 val PER: 0.1602
2025-10-18 19:06:31,795: t15.2024.07.21 val PER: 0.0559
2025-10-18 19:06:31,797: t15.2024.07.28 val PER: 0.0904
2025-10-18 19:06:31,797: t15.2025.01.10 val PER: 0.2328
2025-10-18 19:06:31,798: t15.2025.01.12 val PER: 0.1101
2025-10-18 19:06:31,799: t15.2025.03.14 val PER: 0.2885
2025-10-18 19:06:31,800: t15.2025.03.16 val PER: 0.1571
2025-10-18 19:06:31,801: t15.2025.03.30 val PER: 0.2322
2025-10-18 19:06:31,801: t15.2025.04.13 val PER: 0.1740
2025-10-18 19:06:31,802: New best test PER 0.1100 --> 0.1085
2025-10-18 19:06:31,803: Checkpointing model
2025-10-18 19:06:33,280: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 19:09:50,408: Train batch 30200: loss: 0.13 grad norm: 4.08 time: 0.968
2025-10-18 19:13:10,149: Train batch 30400: loss: 0.17 grad norm: 7.54 time: 1.169
2025-10-18 19:16:30,631: Train batch 30600: loss: 0.19 grad norm: 25.40 time: 0.698
2025-10-18 19:19:51,071: Train batch 30800: loss: 0.10 grad norm: 4.03 time: 0.924
2025-10-18 19:23:11,996: Train batch 31000: loss: 0.04 grad norm: 4.49 time: 0.869
2025-10-18 19:26:35,150: Train batch 31200: loss: 0.16 grad norm: 6.57 time: 1.369
2025-10-18 19:29:52,647: Train batch 31400: loss: 0.12 grad norm: 4.01 time: 0.909
2025-10-18 19:33:09,695: Train batch 31600: loss: 0.21 grad norm: 7.66 time: 0.973
2025-10-18 19:36:26,026: Train batch 31800: loss: 0.06 grad norm: 2.94 time: 0.801
2025-10-18 19:39:40,685: Train batch 32000: loss: 0.16 grad norm: 6.16 time: 1.207
2025-10-18 19:39:40,687: Running test after training batch: 32000
2025-10-18 19:40:20,111: Val batch 32000: PER (avg): 0.1088 CTC Loss (avg): 20.9108 time: 39.423
2025-10-18 19:40:20,112: t15.2023.08.13 val PER: 0.0780
2025-10-18 19:40:20,112: t15.2023.08.18 val PER: 0.0687
2025-10-18 19:40:20,113: t15.2023.08.20 val PER: 0.0500
2025-10-18 19:40:20,114: t15.2023.08.25 val PER: 0.0753
2025-10-18 19:40:20,114: t15.2023.08.27 val PER: 0.1543
2025-10-18 19:40:20,116: t15.2023.09.01 val PER: 0.0430
2025-10-18 19:40:20,116: t15.2023.09.03 val PER: 0.1211
2025-10-18 19:40:20,117: t15.2023.09.24 val PER: 0.0740
2025-10-18 19:40:20,118: t15.2023.09.29 val PER: 0.1040
2025-10-18 19:40:20,119: t15.2023.10.01 val PER: 0.1374
2025-10-18 19:40:20,119: t15.2023.10.06 val PER: 0.0775
2025-10-18 19:40:20,120: t15.2023.10.08 val PER: 0.1827
2025-10-18 19:40:20,121: t15.2023.10.13 val PER: 0.1427
2025-10-18 19:40:20,121: t15.2023.10.15 val PER: 0.1173
2025-10-18 19:40:20,122: t15.2023.10.20 val PER: 0.1611
2025-10-18 19:40:20,123: t15.2023.10.22 val PER: 0.1036
2025-10-18 19:40:20,123: t15.2023.11.03 val PER: 0.1493
2025-10-18 19:40:20,124: t15.2023.11.04 val PER: 0.0375
2025-10-18 19:40:20,125: t15.2023.11.17 val PER: 0.0171
2025-10-18 19:40:20,125: t15.2023.11.19 val PER: 0.0160
2025-10-18 19:40:20,126: t15.2023.11.26 val PER: 0.0580
2025-10-18 19:40:20,127: t15.2023.12.03 val PER: 0.0536
2025-10-18 19:40:20,127: t15.2023.12.08 val PER: 0.0479
2025-10-18 19:40:20,128: t15.2023.12.10 val PER: 0.0434
2025-10-18 19:40:20,129: t15.2023.12.17 val PER: 0.0842
2025-10-18 19:40:20,130: t15.2023.12.29 val PER: 0.0776
2025-10-18 19:40:20,130: t15.2024.02.25 val PER: 0.0702
2025-10-18 19:40:20,131: t15.2024.03.08 val PER: 0.1807
2025-10-18 19:40:20,133: t15.2024.03.15 val PER: 0.1739
2025-10-18 19:40:20,133: t15.2024.03.17 val PER: 0.0886
2025-10-18 19:40:20,134: t15.2024.05.10 val PER: 0.1322
2025-10-18 19:40:20,135: t15.2024.06.14 val PER: 0.1199
2025-10-18 19:40:20,136: t15.2024.07.19 val PER: 0.1635
2025-10-18 19:40:20,137: t15.2024.07.21 val PER: 0.0552
2025-10-18 19:40:20,137: t15.2024.07.28 val PER: 0.0890
2025-10-18 19:40:20,138: t15.2025.01.10 val PER: 0.2342
2025-10-18 19:40:20,139: t15.2025.01.12 val PER: 0.1232
2025-10-18 19:40:20,140: t15.2025.03.14 val PER: 0.2885
2025-10-18 19:40:20,140: t15.2025.03.16 val PER: 0.1636
2025-10-18 19:40:20,142: t15.2025.03.30 val PER: 0.2172
2025-10-18 19:40:20,143: t15.2025.04.13 val PER: 0.1783
2025-10-18 19:43:41,513: Train batch 32200: loss: 0.13 grad norm: 5.18 time: 0.827
2025-10-18 19:46:59,746: Train batch 32400: loss: 0.10 grad norm: 4.12 time: 0.900
2025-10-18 19:50:19,628: Train batch 32600: loss: 0.17 grad norm: 6.68 time: 1.040
2025-10-18 19:53:36,961: Train batch 32800: loss: 0.09 grad norm: 3.21 time: 1.025
2025-10-18 19:56:53,012: Train batch 33000: loss: 0.08 grad norm: 3.46 time: 0.905
2025-10-18 20:00:11,905: Train batch 33200: loss: 0.78 grad norm: 10.63 time: 1.050
2025-10-18 20:03:36,401: Train batch 33400: loss: 0.11 grad norm: 4.37 time: 1.289
2025-10-18 20:06:56,376: Train batch 33600: loss: 0.13 grad norm: 4.54 time: 0.864
2025-10-18 20:10:13,555: Train batch 33800: loss: 0.05 grad norm: 1.74 time: 0.977
2025-10-18 20:13:30,741: Train batch 34000: loss: 0.06 grad norm: 4.35 time: 0.938
2025-10-18 20:13:30,742: Running test after training batch: 34000
2025-10-18 20:14:10,383: Val batch 34000: PER (avg): 0.1080 CTC Loss (avg): 21.0009 time: 39.640
2025-10-18 20:14:10,384: t15.2023.08.13 val PER: 0.0728
2025-10-18 20:14:10,385: t15.2023.08.18 val PER: 0.0712
2025-10-18 20:14:10,386: t15.2023.08.20 val PER: 0.0500
2025-10-18 20:14:10,386: t15.2023.08.25 val PER: 0.0813
2025-10-18 20:14:10,387: t15.2023.08.27 val PER: 0.1495
2025-10-18 20:14:10,388: t15.2023.09.01 val PER: 0.0446
2025-10-18 20:14:10,389: t15.2023.09.03 val PER: 0.1306
2025-10-18 20:14:10,389: t15.2023.09.24 val PER: 0.0716
2025-10-18 20:14:10,390: t15.2023.09.29 val PER: 0.1059
2025-10-18 20:14:10,391: t15.2023.10.01 val PER: 0.1380
2025-10-18 20:14:10,391: t15.2023.10.06 val PER: 0.0775
2025-10-18 20:14:10,392: t15.2023.10.08 val PER: 0.1759
2025-10-18 20:14:10,393: t15.2023.10.13 val PER: 0.1482
2025-10-18 20:14:10,394: t15.2023.10.15 val PER: 0.1173
2025-10-18 20:14:10,396: t15.2023.10.20 val PER: 0.1577
2025-10-18 20:14:10,396: t15.2023.10.22 val PER: 0.0958
2025-10-18 20:14:10,397: t15.2023.11.03 val PER: 0.1472
2025-10-18 20:14:10,398: t15.2023.11.04 val PER: 0.0273
2025-10-18 20:14:10,398: t15.2023.11.17 val PER: 0.0156
2025-10-18 20:14:10,399: t15.2023.11.19 val PER: 0.0140
2025-10-18 20:14:10,400: t15.2023.11.26 val PER: 0.0587
2025-10-18 20:14:10,401: t15.2023.12.03 val PER: 0.0515
2025-10-18 20:14:10,401: t15.2023.12.08 val PER: 0.0473
2025-10-18 20:14:10,402: t15.2023.12.10 val PER: 0.0460
2025-10-18 20:14:10,403: t15.2023.12.17 val PER: 0.0800
2025-10-18 20:14:10,403: t15.2023.12.29 val PER: 0.0755
2025-10-18 20:14:10,405: t15.2024.02.25 val PER: 0.0702
2025-10-18 20:14:10,405: t15.2024.03.08 val PER: 0.1778
2025-10-18 20:14:10,406: t15.2024.03.15 val PER: 0.1776
2025-10-18 20:14:10,407: t15.2024.03.17 val PER: 0.0830
2025-10-18 20:14:10,408: t15.2024.05.10 val PER: 0.1263
2025-10-18 20:14:10,408: t15.2024.06.14 val PER: 0.1215
2025-10-18 20:14:10,409: t15.2024.07.19 val PER: 0.1648
2025-10-18 20:14:10,411: t15.2024.07.21 val PER: 0.0524
2025-10-18 20:14:10,411: t15.2024.07.28 val PER: 0.0890
2025-10-18 20:14:10,412: t15.2025.01.10 val PER: 0.2424
2025-10-18 20:14:10,412: t15.2025.01.12 val PER: 0.1124
2025-10-18 20:14:10,413: t15.2025.03.14 val PER: 0.2929
2025-10-18 20:14:10,414: t15.2025.03.16 val PER: 0.1597
2025-10-18 20:14:10,416: t15.2025.03.30 val PER: 0.2115
2025-10-18 20:14:10,416: t15.2025.04.13 val PER: 0.1797
2025-10-18 20:14:10,417: New best test PER 0.1085 --> 0.1080
2025-10-18 20:14:10,418: Checkpointing model
2025-10-18 20:14:11,900: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 20:17:36,624: Train batch 34200: loss: 0.13 grad norm: 6.92 time: 0.901
2025-10-18 20:20:56,755: Train batch 34400: loss: 0.09 grad norm: 16.23 time: 1.077
2025-10-18 20:24:21,305: Train batch 34600: loss: 0.20 grad norm: 14.45 time: 0.850
2025-10-18 20:27:41,709: Train batch 34800: loss: 0.07 grad norm: 2.84 time: 0.740
2025-10-18 20:31:00,571: Train batch 35000: loss: 0.13 grad norm: 5.02 time: 0.896
2025-10-18 20:34:15,097: Train batch 35200: loss: 0.10 grad norm: 4.57 time: 0.979
2025-10-18 20:37:30,132: Train batch 35400: loss: 0.09 grad norm: 3.73 time: 0.705
2025-10-18 20:40:46,326: Train batch 35600: loss: 0.19 grad norm: 14.96 time: 0.869
2025-10-18 20:44:03,158: Train batch 35800: loss: 0.01 grad norm: 0.37 time: 1.211
2025-10-18 20:47:20,825: Train batch 36000: loss: 0.14 grad norm: 5.19 time: 0.823
2025-10-18 20:47:20,826: Running test after training batch: 36000
2025-10-18 20:48:00,850: Val batch 36000: PER (avg): 0.1092 CTC Loss (avg): 21.0018 time: 40.023
2025-10-18 20:48:00,851: t15.2023.08.13 val PER: 0.0759
2025-10-18 20:48:00,852: t15.2023.08.18 val PER: 0.0721
2025-10-18 20:48:00,852: t15.2023.08.20 val PER: 0.0508
2025-10-18 20:48:00,853: t15.2023.08.25 val PER: 0.0858
2025-10-18 20:48:00,854: t15.2023.08.27 val PER: 0.1495
2025-10-18 20:48:00,854: t15.2023.09.01 val PER: 0.0446
2025-10-18 20:48:00,855: t15.2023.09.03 val PER: 0.1318
2025-10-18 20:48:00,856: t15.2023.09.24 val PER: 0.0716
2025-10-18 20:48:00,856: t15.2023.09.29 val PER: 0.1027
2025-10-18 20:48:00,857: t15.2023.10.01 val PER: 0.1387
2025-10-18 20:48:00,858: t15.2023.10.06 val PER: 0.0753
2025-10-18 20:48:00,860: t15.2023.10.08 val PER: 0.1719
2025-10-18 20:48:00,860: t15.2023.10.13 val PER: 0.1490
2025-10-18 20:48:00,861: t15.2023.10.15 val PER: 0.1220
2025-10-18 20:48:00,862: t15.2023.10.20 val PER: 0.1611
2025-10-18 20:48:00,862: t15.2023.10.22 val PER: 0.1058
2025-10-18 20:48:00,863: t15.2023.11.03 val PER: 0.1486
2025-10-18 20:48:00,864: t15.2023.11.04 val PER: 0.0273
2025-10-18 20:48:00,865: t15.2023.11.17 val PER: 0.0156
2025-10-18 20:48:00,865: t15.2023.11.19 val PER: 0.0160
2025-10-18 20:48:00,866: t15.2023.11.26 val PER: 0.0580
2025-10-18 20:48:00,867: t15.2023.12.03 val PER: 0.0536
2025-10-18 20:48:00,868: t15.2023.12.08 val PER: 0.0466
2025-10-18 20:48:00,868: t15.2023.12.10 val PER: 0.0526
2025-10-18 20:48:00,869: t15.2023.12.17 val PER: 0.0800
2025-10-18 20:48:00,870: t15.2023.12.29 val PER: 0.0769
2025-10-18 20:48:00,870: t15.2024.02.25 val PER: 0.0660
2025-10-18 20:48:00,871: t15.2024.03.08 val PER: 0.1835
2025-10-18 20:48:00,872: t15.2024.03.15 val PER: 0.1714
2025-10-18 20:48:00,872: t15.2024.03.17 val PER: 0.0858
2025-10-18 20:48:00,874: t15.2024.05.10 val PER: 0.1337
2025-10-18 20:48:00,875: t15.2024.06.14 val PER: 0.1230
2025-10-18 20:48:00,876: t15.2024.07.19 val PER: 0.1674
2025-10-18 20:48:00,877: t15.2024.07.21 val PER: 0.0524
2025-10-18 20:48:00,877: t15.2024.07.28 val PER: 0.0897
2025-10-18 20:48:00,878: t15.2025.01.10 val PER: 0.2424
2025-10-18 20:48:00,879: t15.2025.01.12 val PER: 0.1116
2025-10-18 20:48:00,880: t15.2025.03.14 val PER: 0.2973
2025-10-18 20:48:00,881: t15.2025.03.16 val PER: 0.1636
2025-10-18 20:48:00,882: t15.2025.03.30 val PER: 0.2218
2025-10-18 20:48:00,882: t15.2025.04.13 val PER: 0.1783
2025-10-18 20:51:23,000: Train batch 36200: loss: 0.07 grad norm: 2.98 time: 0.789
2025-10-18 20:54:47,686: Train batch 36400: loss: 0.05 grad norm: 2.87 time: 1.093
2025-10-18 20:58:09,201: Train batch 36600: loss: 0.22 grad norm: 8.57 time: 1.279
2025-10-18 21:01:26,594: Train batch 36800: loss: 0.09 grad norm: 2.39 time: 0.906
2025-10-18 21:04:42,634: Train batch 37000: loss: 0.06 grad norm: 2.08 time: 0.925
2025-10-18 21:08:00,652: Train batch 37200: loss: 0.14 grad norm: 19.29 time: 0.993
2025-10-18 21:11:20,048: Train batch 37400: loss: 0.08 grad norm: 4.09 time: 1.051
2025-10-18 21:14:37,024: Train batch 37600: loss: 0.09 grad norm: 4.32 time: 0.706
2025-10-18 21:17:55,620: Train batch 37800: loss: 0.13 grad norm: 5.33 time: 0.815
2025-10-18 21:21:16,102: Train batch 38000: loss: 0.17 grad norm: 6.35 time: 1.203
2025-10-18 21:21:16,103: Running test after training batch: 38000
2025-10-18 21:21:55,970: Val batch 38000: PER (avg): 0.1079 CTC Loss (avg): 20.8020 time: 39.866
2025-10-18 21:21:55,971: t15.2023.08.13 val PER: 0.0748
2025-10-18 21:21:55,972: t15.2023.08.18 val PER: 0.0687
2025-10-18 21:21:55,973: t15.2023.08.20 val PER: 0.0524
2025-10-18 21:21:55,973: t15.2023.08.25 val PER: 0.0843
2025-10-18 21:21:55,974: t15.2023.08.27 val PER: 0.1479
2025-10-18 21:21:55,975: t15.2023.09.01 val PER: 0.0438
2025-10-18 21:21:55,976: t15.2023.09.03 val PER: 0.1247
2025-10-18 21:21:55,977: t15.2023.09.24 val PER: 0.0728
2025-10-18 21:21:55,978: t15.2023.09.29 val PER: 0.1002
2025-10-18 21:21:55,978: t15.2023.10.01 val PER: 0.1387
2025-10-18 21:21:55,979: t15.2023.10.06 val PER: 0.0753
2025-10-18 21:21:55,980: t15.2023.10.08 val PER: 0.1732
2025-10-18 21:21:55,981: t15.2023.10.13 val PER: 0.1474
2025-10-18 21:21:55,982: t15.2023.10.15 val PER: 0.1206
2025-10-18 21:21:55,983: t15.2023.10.20 val PER: 0.1644
2025-10-18 21:21:55,984: t15.2023.10.22 val PER: 0.1058
2025-10-18 21:21:55,985: t15.2023.11.03 val PER: 0.1452
2025-10-18 21:21:55,985: t15.2023.11.04 val PER: 0.0273
2025-10-18 21:21:55,986: t15.2023.11.17 val PER: 0.0156
2025-10-18 21:21:55,987: t15.2023.11.19 val PER: 0.0140
2025-10-18 21:21:55,988: t15.2023.11.26 val PER: 0.0594
2025-10-18 21:21:55,989: t15.2023.12.03 val PER: 0.0504
2025-10-18 21:21:55,990: t15.2023.12.08 val PER: 0.0419
2025-10-18 21:21:55,990: t15.2023.12.10 val PER: 0.0473
2025-10-18 21:21:55,991: t15.2023.12.17 val PER: 0.0821
2025-10-18 21:21:55,992: t15.2023.12.29 val PER: 0.0748
2025-10-18 21:21:55,992: t15.2024.02.25 val PER: 0.0688
2025-10-18 21:21:55,994: t15.2024.03.08 val PER: 0.1835
2025-10-18 21:21:55,994: t15.2024.03.15 val PER: 0.1707
2025-10-18 21:21:55,995: t15.2024.03.17 val PER: 0.0858
2025-10-18 21:21:55,996: t15.2024.05.10 val PER: 0.1218
2025-10-18 21:21:55,996: t15.2024.06.14 val PER: 0.1246
2025-10-18 21:21:55,997: t15.2024.07.19 val PER: 0.1668
2025-10-18 21:21:55,998: t15.2024.07.21 val PER: 0.0545
2025-10-18 21:21:55,999: t15.2024.07.28 val PER: 0.0853
2025-10-18 21:21:56,000: t15.2025.01.10 val PER: 0.2452
2025-10-18 21:21:56,001: t15.2025.01.12 val PER: 0.1116
2025-10-18 21:21:56,002: t15.2025.03.14 val PER: 0.2959
2025-10-18 21:21:56,003: t15.2025.03.16 val PER: 0.1597
2025-10-18 21:21:56,003: t15.2025.03.30 val PER: 0.2207
2025-10-18 21:21:56,005: t15.2025.04.13 val PER: 0.1755
2025-10-18 21:21:56,005: New best test PER 0.1080 --> 0.1079
2025-10-18 21:21:56,006: Checkpointing model
2025-10-18 21:21:57,495: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 21:25:16,945: Train batch 38200: loss: 0.24 grad norm: 7.99 time: 1.053
2025-10-18 21:28:32,888: Train batch 38400: loss: 0.17 grad norm: 6.69 time: 0.848
2025-10-18 21:31:50,952: Train batch 38600: loss: 0.15 grad norm: 8.13 time: 1.095
2025-10-18 21:35:07,598: Train batch 38800: loss: 0.06 grad norm: 2.97 time: 0.958
2025-10-18 21:38:27,506: Train batch 39000: loss: 0.07 grad norm: 3.67 time: 1.224
2025-10-18 21:41:46,232: Train batch 39200: loss: 0.11 grad norm: 4.78 time: 0.780
2025-10-18 21:45:03,216: Train batch 39400: loss: 0.21 grad norm: 7.74 time: 0.891
2025-10-18 21:48:19,134: Train batch 39600: loss: 0.13 grad norm: 4.95 time: 1.027
2025-10-18 21:51:39,363: Train batch 39800: loss: 0.07 grad norm: 4.10 time: 0.872
2025-10-18 21:54:56,715: Running test after training batch: 39999
2025-10-18 21:55:35,910: Val batch 39999: PER (avg): 0.1070 CTC Loss (avg): 20.8463 time: 39.195
2025-10-18 21:55:35,911: t15.2023.08.13 val PER: 0.0707
2025-10-18 21:55:35,912: t15.2023.08.18 val PER: 0.0696
2025-10-18 21:55:35,913: t15.2023.08.20 val PER: 0.0524
2025-10-18 21:55:35,914: t15.2023.08.25 val PER: 0.0828
2025-10-18 21:55:35,914: t15.2023.08.27 val PER: 0.1463
2025-10-18 21:55:35,915: t15.2023.09.01 val PER: 0.0422
2025-10-18 21:55:35,916: t15.2023.09.03 val PER: 0.1259
2025-10-18 21:55:35,916: t15.2023.09.24 val PER: 0.0728
2025-10-18 21:55:35,917: t15.2023.09.29 val PER: 0.0989
2025-10-18 21:55:35,918: t15.2023.10.01 val PER: 0.1347
2025-10-18 21:55:35,919: t15.2023.10.06 val PER: 0.0797
2025-10-18 21:55:35,920: t15.2023.10.08 val PER: 0.1719
2025-10-18 21:55:35,920: t15.2023.10.13 val PER: 0.1451
2025-10-18 21:55:35,921: t15.2023.10.15 val PER: 0.1226
2025-10-18 21:55:35,922: t15.2023.10.20 val PER: 0.1577
2025-10-18 21:55:35,923: t15.2023.10.22 val PER: 0.1036
2025-10-18 21:55:35,923: t15.2023.11.03 val PER: 0.1425
2025-10-18 21:55:35,924: t15.2023.11.04 val PER: 0.0307
2025-10-18 21:55:35,925: t15.2023.11.17 val PER: 0.0171
2025-10-18 21:55:35,925: t15.2023.11.19 val PER: 0.0140
2025-10-18 21:55:35,926: t15.2023.11.26 val PER: 0.0580
2025-10-18 21:55:35,927: t15.2023.12.03 val PER: 0.0525
2025-10-18 21:55:35,927: t15.2023.12.08 val PER: 0.0419
2025-10-18 21:55:35,929: t15.2023.12.10 val PER: 0.0473
2025-10-18 21:55:35,930: t15.2023.12.17 val PER: 0.0800
2025-10-18 21:55:35,930: t15.2023.12.29 val PER: 0.0728
2025-10-18 21:55:35,931: t15.2024.02.25 val PER: 0.0660
2025-10-18 21:55:35,932: t15.2024.03.08 val PER: 0.1778
2025-10-18 21:55:35,933: t15.2024.03.15 val PER: 0.1707
2025-10-18 21:55:35,934: t15.2024.03.17 val PER: 0.0844
2025-10-18 21:55:35,934: t15.2024.05.10 val PER: 0.1263
2025-10-18 21:55:35,935: t15.2024.06.14 val PER: 0.1215
2025-10-18 21:55:35,936: t15.2024.07.19 val PER: 0.1641
2025-10-18 21:55:35,937: t15.2024.07.21 val PER: 0.0531
2025-10-18 21:55:35,938: t15.2024.07.28 val PER: 0.0868
2025-10-18 21:55:35,938: t15.2025.01.10 val PER: 0.2410
2025-10-18 21:55:35,939: t15.2025.01.12 val PER: 0.1116
2025-10-18 21:55:35,940: t15.2025.03.14 val PER: 0.2944
2025-10-18 21:55:35,940: t15.2025.03.16 val PER: 0.1610
2025-10-18 21:55:35,941: t15.2025.03.30 val PER: 0.2184
2025-10-18 21:55:35,941: t15.2025.04.13 val PER: 0.1755
2025-10-18 21:55:35,942: New best test PER 0.1079 --> 0.1070
2025-10-18 21:55:35,943: Checkpointing model
2025-10-18 21:55:37,412: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 21:55:37,687: Best avg val PER achieved: 0.10703
2025-10-18 21:55:37,688: Total training time: 677.59 minutes
