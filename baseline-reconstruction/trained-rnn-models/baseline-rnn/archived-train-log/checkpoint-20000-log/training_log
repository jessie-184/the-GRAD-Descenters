2025-10-18 10:36:59,823: Requested GPU 1 not available. Using GPU 0 instead.
2025-10-18 10:37:00,003: Using device: cuda:0
2025-10-18 10:37:01,042: Using torch.compile
2025-10-18 10:37:03,020: Initialized RNN decoding model
2025-10-18 10:37:03,021: OptimizedModule(
  (_orig_mod): GRUDecoder(
    (day_layer_activation): Softsign()
    (day_weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 512x512]
        (1): Parameter containing: [torch.float32 of size 512x512]
        (2): Parameter containing: [torch.float32 of size 512x512]
        (3): Parameter containing: [torch.float32 of size 512x512]
        (4): Parameter containing: [torch.float32 of size 512x512]
        (5): Parameter containing: [torch.float32 of size 512x512]
        (6): Parameter containing: [torch.float32 of size 512x512]
        (7): Parameter containing: [torch.float32 of size 512x512]
        (8): Parameter containing: [torch.float32 of size 512x512]
        (9): Parameter containing: [torch.float32 of size 512x512]
        (10): Parameter containing: [torch.float32 of size 512x512]
        (11): Parameter containing: [torch.float32 of size 512x512]
        (12): Parameter containing: [torch.float32 of size 512x512]
        (13): Parameter containing: [torch.float32 of size 512x512]
        (14): Parameter containing: [torch.float32 of size 512x512]
        (15): Parameter containing: [torch.float32 of size 512x512]
        (16): Parameter containing: [torch.float32 of size 512x512]
        (17): Parameter containing: [torch.float32 of size 512x512]
        (18): Parameter containing: [torch.float32 of size 512x512]
        (19): Parameter containing: [torch.float32 of size 512x512]
        (20): Parameter containing: [torch.float32 of size 512x512]
        (21): Parameter containing: [torch.float32 of size 512x512]
        (22): Parameter containing: [torch.float32 of size 512x512]
        (23): Parameter containing: [torch.float32 of size 512x512]
        (24): Parameter containing: [torch.float32 of size 512x512]
        (25): Parameter containing: [torch.float32 of size 512x512]
        (26): Parameter containing: [torch.float32 of size 512x512]
        (27): Parameter containing: [torch.float32 of size 512x512]
        (28): Parameter containing: [torch.float32 of size 512x512]
        (29): Parameter containing: [torch.float32 of size 512x512]
        (30): Parameter containing: [torch.float32 of size 512x512]
        (31): Parameter containing: [torch.float32 of size 512x512]
        (32): Parameter containing: [torch.float32 of size 512x512]
        (33): Parameter containing: [torch.float32 of size 512x512]
        (34): Parameter containing: [torch.float32 of size 512x512]
        (35): Parameter containing: [torch.float32 of size 512x512]
        (36): Parameter containing: [torch.float32 of size 512x512]
        (37): Parameter containing: [torch.float32 of size 512x512]
        (38): Parameter containing: [torch.float32 of size 512x512]
        (39): Parameter containing: [torch.float32 of size 512x512]
        (40): Parameter containing: [torch.float32 of size 512x512]
        (41): Parameter containing: [torch.float32 of size 512x512]
        (42): Parameter containing: [torch.float32 of size 512x512]
        (43): Parameter containing: [torch.float32 of size 512x512]
        (44): Parameter containing: [torch.float32 of size 512x512]
    )
    (day_biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1x512]
        (1): Parameter containing: [torch.float32 of size 1x512]
        (2): Parameter containing: [torch.float32 of size 1x512]
        (3): Parameter containing: [torch.float32 of size 1x512]
        (4): Parameter containing: [torch.float32 of size 1x512]
        (5): Parameter containing: [torch.float32 of size 1x512]
        (6): Parameter containing: [torch.float32 of size 1x512]
        (7): Parameter containing: [torch.float32 of size 1x512]
        (8): Parameter containing: [torch.float32 of size 1x512]
        (9): Parameter containing: [torch.float32 of size 1x512]
        (10): Parameter containing: [torch.float32 of size 1x512]
        (11): Parameter containing: [torch.float32 of size 1x512]
        (12): Parameter containing: [torch.float32 of size 1x512]
        (13): Parameter containing: [torch.float32 of size 1x512]
        (14): Parameter containing: [torch.float32 of size 1x512]
        (15): Parameter containing: [torch.float32 of size 1x512]
        (16): Parameter containing: [torch.float32 of size 1x512]
        (17): Parameter containing: [torch.float32 of size 1x512]
        (18): Parameter containing: [torch.float32 of size 1x512]
        (19): Parameter containing: [torch.float32 of size 1x512]
        (20): Parameter containing: [torch.float32 of size 1x512]
        (21): Parameter containing: [torch.float32 of size 1x512]
        (22): Parameter containing: [torch.float32 of size 1x512]
        (23): Parameter containing: [torch.float32 of size 1x512]
        (24): Parameter containing: [torch.float32 of size 1x512]
        (25): Parameter containing: [torch.float32 of size 1x512]
        (26): Parameter containing: [torch.float32 of size 1x512]
        (27): Parameter containing: [torch.float32 of size 1x512]
        (28): Parameter containing: [torch.float32 of size 1x512]
        (29): Parameter containing: [torch.float32 of size 1x512]
        (30): Parameter containing: [torch.float32 of size 1x512]
        (31): Parameter containing: [torch.float32 of size 1x512]
        (32): Parameter containing: [torch.float32 of size 1x512]
        (33): Parameter containing: [torch.float32 of size 1x512]
        (34): Parameter containing: [torch.float32 of size 1x512]
        (35): Parameter containing: [torch.float32 of size 1x512]
        (36): Parameter containing: [torch.float32 of size 1x512]
        (37): Parameter containing: [torch.float32 of size 1x512]
        (38): Parameter containing: [torch.float32 of size 1x512]
        (39): Parameter containing: [torch.float32 of size 1x512]
        (40): Parameter containing: [torch.float32 of size 1x512]
        (41): Parameter containing: [torch.float32 of size 1x512]
        (42): Parameter containing: [torch.float32 of size 1x512]
        (43): Parameter containing: [torch.float32 of size 1x512]
        (44): Parameter containing: [torch.float32 of size 1x512]
    )
    (day_layer_dropout): Dropout(p=0.2, inplace=False)
    (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.4)
    (out): Linear(in_features=768, out_features=41, bias=True)
  )
)
2025-10-18 10:37:03,024: Model has 44,315,177 parameters
2025-10-18 10:37:03,025: Model has 11,819,520 day-specific parameters | 26.67% of total parameters
2025-10-18 10:37:31,933: Successfully initialized datasets
2025-10-18 10:38:14,466: Train batch 0: loss: 753.82 grad norm: 302.81 time: 7.967
2025-10-18 10:38:14,467: Running test after training batch: 0
2025-10-18 10:39:03,134: Val batch 0: PER (avg): 1.2154 CTC Loss (avg): 714.9868 time: 48.666
2025-10-18 10:39:03,135: t15.2023.08.13 val PER: 1.1247
2025-10-18 10:39:03,136: t15.2023.08.18 val PER: 1.1459
2025-10-18 10:39:03,137: t15.2023.08.20 val PER: 1.1541
2025-10-18 10:39:03,138: t15.2023.08.25 val PER: 1.1611
2025-10-18 10:39:03,138: t15.2023.08.27 val PER: 1.0659
2025-10-18 10:39:03,139: t15.2023.09.01 val PER: 1.2248
2025-10-18 10:39:03,141: t15.2023.09.03 val PER: 1.1176
2025-10-18 10:39:03,142: t15.2023.09.24 val PER: 1.3337
2025-10-18 10:39:03,143: t15.2023.09.29 val PER: 1.2406
2025-10-18 10:39:03,143: t15.2023.10.01 val PER: 1.0542
2025-10-18 10:39:03,144: t15.2023.10.06 val PER: 1.2465
2025-10-18 10:39:03,145: t15.2023.10.08 val PER: 1.0406
2025-10-18 10:39:03,146: t15.2023.10.13 val PER: 1.1621
2025-10-18 10:39:03,146: t15.2023.10.15 val PER: 1.2037
2025-10-18 10:39:03,147: t15.2023.10.20 val PER: 1.3557
2025-10-18 10:39:03,148: t15.2023.10.22 val PER: 1.2996
2025-10-18 10:39:03,149: t15.2023.11.03 val PER: 1.2972
2025-10-18 10:39:03,150: t15.2023.11.04 val PER: 1.3891
2025-10-18 10:39:03,151: t15.2023.11.17 val PER: 1.6454
2025-10-18 10:39:03,152: t15.2023.11.19 val PER: 1.4351
2025-10-18 10:39:03,152: t15.2023.11.26 val PER: 1.2529
2025-10-18 10:39:03,154: t15.2023.12.03 val PER: 1.1838
2025-10-18 10:39:03,154: t15.2023.12.08 val PER: 1.2557
2025-10-18 10:39:03,155: t15.2023.12.10 val PER: 1.3219
2025-10-18 10:39:03,156: t15.2023.12.17 val PER: 1.0696
2025-10-18 10:39:03,157: t15.2023.12.29 val PER: 1.1640
2025-10-18 10:39:03,157: t15.2024.02.25 val PER: 1.1320
2025-10-18 10:39:03,158: t15.2024.03.08 val PER: 1.1550
2025-10-18 10:39:03,159: t15.2024.03.15 val PER: 1.1138
2025-10-18 10:39:03,160: t15.2024.03.17 val PER: 1.1722
2025-10-18 10:39:03,160: t15.2024.05.10 val PER: 1.2065
2025-10-18 10:39:03,161: t15.2024.06.14 val PER: 1.4132
2025-10-18 10:39:03,162: t15.2024.07.19 val PER: 0.9881
2025-10-18 10:39:03,163: t15.2024.07.21 val PER: 1.4193
2025-10-18 10:39:03,163: t15.2024.07.28 val PER: 1.4654
2025-10-18 10:39:03,164: t15.2025.01.10 val PER: 0.9518
2025-10-18 10:39:03,165: t15.2025.01.12 val PER: 1.4126
2025-10-18 10:39:03,165: t15.2025.03.14 val PER: 1.0178
2025-10-18 10:39:03,166: t15.2025.03.16 val PER: 1.4202
2025-10-18 10:39:03,167: t15.2025.03.30 val PER: 1.0805
2025-10-18 10:39:03,167: t15.2025.04.13 val PER: 1.3024
2025-10-18 10:39:03,168: New best test PER inf --> 1.2154
2025-10-18 10:39:03,169: Checkpointing model
2025-10-18 10:39:03,949: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 10:42:25,563: Train batch 200: loss: 93.76 grad norm: 18.19 time: 0.903
2025-10-18 10:45:43,904: Train batch 400: loss: 74.97 grad norm: 50.62 time: 1.072
2025-10-18 10:48:59,948: Train batch 600: loss: 58.92 grad norm: 43.93 time: 0.705
2025-10-18 10:52:17,997: Train batch 800: loss: 51.95 grad norm: 45.32 time: 1.050
2025-10-18 10:55:38,151: Train batch 1000: loss: 45.50 grad norm: 48.73 time: 0.908
2025-10-18 10:58:53,145: Train batch 1200: loss: 28.15 grad norm: 31.10 time: 1.107
2025-10-18 11:02:10,438: Train batch 1400: loss: 16.88 grad norm: 29.33 time: 0.887
2025-10-18 11:05:27,481: Train batch 1600: loss: 28.82 grad norm: 40.23 time: 0.942
2025-10-18 11:08:46,091: Train batch 1800: loss: 26.20 grad norm: 51.03 time: 0.721
2025-10-18 11:12:05,687: Train batch 2000: loss: 12.81 grad norm: 33.08 time: 0.905
2025-10-18 11:12:05,689: Running test after training batch: 2000
2025-10-18 11:12:45,716: Val batch 2000: PER (avg): 0.2252 CTC Loss (avg): 22.8470 time: 40.026
2025-10-18 11:12:45,717: t15.2023.08.13 val PER: 0.1861
2025-10-18 11:12:45,718: t15.2023.08.18 val PER: 0.1844
2025-10-18 11:12:45,719: t15.2023.08.20 val PER: 0.1620
2025-10-18 11:12:45,720: t15.2023.08.25 val PER: 0.1717
2025-10-18 11:12:45,721: t15.2023.08.27 val PER: 0.2765
2025-10-18 11:12:45,722: t15.2023.09.01 val PER: 0.1502
2025-10-18 11:12:45,722: t15.2023.09.03 val PER: 0.2197
2025-10-18 11:12:45,723: t15.2023.09.24 val PER: 0.1711
2025-10-18 11:12:45,723: t15.2023.09.29 val PER: 0.1927
2025-10-18 11:12:45,724: t15.2023.10.01 val PER: 0.2398
2025-10-18 11:12:45,725: t15.2023.10.06 val PER: 0.1679
2025-10-18 11:12:45,725: t15.2023.10.08 val PER: 0.3221
2025-10-18 11:12:45,726: t15.2023.10.13 val PER: 0.2832
2025-10-18 11:12:45,728: t15.2023.10.15 val PER: 0.2274
2025-10-18 11:12:45,728: t15.2023.10.20 val PER: 0.2416
2025-10-18 11:12:45,729: t15.2023.10.22 val PER: 0.1938
2025-10-18 11:12:45,730: t15.2023.11.03 val PER: 0.2307
2025-10-18 11:12:45,730: t15.2023.11.04 val PER: 0.0410
2025-10-18 11:12:45,731: t15.2023.11.17 val PER: 0.1135
2025-10-18 11:12:45,731: t15.2023.11.19 val PER: 0.1018
2025-10-18 11:12:45,733: t15.2023.11.26 val PER: 0.2377
2025-10-18 11:12:45,734: t15.2023.12.03 val PER: 0.2059
2025-10-18 11:12:45,735: t15.2023.12.08 val PER: 0.1997
2025-10-18 11:12:45,735: t15.2023.12.10 val PER: 0.1577
2025-10-18 11:12:45,736: t15.2023.12.17 val PER: 0.2193
2025-10-18 11:12:45,737: t15.2023.12.29 val PER: 0.2107
2025-10-18 11:12:45,738: t15.2024.02.25 val PER: 0.1728
2025-10-18 11:12:45,738: t15.2024.03.08 val PER: 0.2959
2025-10-18 11:12:45,739: t15.2024.03.15 val PER: 0.2908
2025-10-18 11:12:45,741: t15.2024.03.17 val PER: 0.2204
2025-10-18 11:12:45,742: t15.2024.05.10 val PER: 0.2377
2025-10-18 11:12:45,742: t15.2024.06.14 val PER: 0.2303
2025-10-18 11:12:45,743: t15.2024.07.19 val PER: 0.3052
2025-10-18 11:12:45,743: t15.2024.07.21 val PER: 0.1621
2025-10-18 11:12:45,744: t15.2024.07.28 val PER: 0.2103
2025-10-18 11:12:45,745: t15.2025.01.10 val PER: 0.3774
2025-10-18 11:12:45,746: t15.2025.01.12 val PER: 0.2348
2025-10-18 11:12:45,746: t15.2025.03.14 val PER: 0.4053
2025-10-18 11:12:45,748: t15.2025.03.16 val PER: 0.2709
2025-10-18 11:12:45,748: t15.2025.03.30 val PER: 0.3575
2025-10-18 11:12:45,749: t15.2025.04.13 val PER: 0.2782
2025-10-18 11:12:45,749: New best test PER 1.2154 --> 0.2252
2025-10-18 11:12:45,750: Checkpointing model
2025-10-18 11:12:47,148: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 11:16:06,190: Train batch 2200: loss: 17.68 grad norm: 38.49 time: 0.976
2025-10-18 11:19:23,986: Train batch 2400: loss: 15.61 grad norm: 34.61 time: 0.944
2025-10-18 11:22:43,571: Train batch 2600: loss: 9.55 grad norm: 23.17 time: 0.719
2025-10-18 11:26:03,941: Train batch 2800: loss: 10.45 grad norm: 31.81 time: 1.158
2025-10-18 11:29:21,440: Train batch 3000: loss: 13.14 grad norm: 31.36 time: 0.921
2025-10-18 11:32:37,690: Train batch 3200: loss: 11.72 grad norm: 32.83 time: 0.773
2025-10-18 11:35:53,873: Train batch 3400: loss: 7.00 grad norm: 25.08 time: 0.924
2025-10-18 11:39:14,061: Train batch 3600: loss: 6.86 grad norm: 22.93 time: 1.017
2025-10-18 11:42:32,991: Train batch 3800: loss: 6.67 grad norm: 27.59 time: 0.844
2025-10-18 11:45:53,193: Train batch 4000: loss: 4.08 grad norm: 18.64 time: 0.873
2025-10-18 11:45:53,194: Running test after training batch: 4000
2025-10-18 11:46:35,104: Val batch 4000: PER (avg): 0.1577 CTC Loss (avg): 17.4698 time: 41.909
2025-10-18 11:46:35,105: t15.2023.08.13 val PER: 0.1175
2025-10-18 11:46:35,106: t15.2023.08.18 val PER: 0.1215
2025-10-18 11:46:35,106: t15.2023.08.20 val PER: 0.1009
2025-10-18 11:46:35,107: t15.2023.08.25 val PER: 0.1009
2025-10-18 11:46:35,108: t15.2023.08.27 val PER: 0.1833
2025-10-18 11:46:35,109: t15.2023.09.01 val PER: 0.0771
2025-10-18 11:46:35,109: t15.2023.09.03 val PER: 0.1603
2025-10-18 11:46:35,110: t15.2023.09.24 val PER: 0.1214
2025-10-18 11:46:35,111: t15.2023.09.29 val PER: 0.1436
2025-10-18 11:46:35,111: t15.2023.10.01 val PER: 0.1783
2025-10-18 11:46:35,112: t15.2023.10.06 val PER: 0.1173
2025-10-18 11:46:35,113: t15.2023.10.08 val PER: 0.2436
2025-10-18 11:46:35,113: t15.2023.10.13 val PER: 0.2188
2025-10-18 11:46:35,114: t15.2023.10.15 val PER: 0.1615
2025-10-18 11:46:35,115: t15.2023.10.20 val PER: 0.1879
2025-10-18 11:46:35,115: t15.2023.10.22 val PER: 0.1214
2025-10-18 11:46:35,118: t15.2023.11.03 val PER: 0.1818
2025-10-18 11:46:35,118: t15.2023.11.04 val PER: 0.0273
2025-10-18 11:46:35,119: t15.2023.11.17 val PER: 0.0373
2025-10-18 11:46:35,120: t15.2023.11.19 val PER: 0.0599
2025-10-18 11:46:35,120: t15.2023.11.26 val PER: 0.1406
2025-10-18 11:46:35,121: t15.2023.12.03 val PER: 0.1071
2025-10-18 11:46:35,122: t15.2023.12.08 val PER: 0.1132
2025-10-18 11:46:35,122: t15.2023.12.10 val PER: 0.1078
2025-10-18 11:46:35,123: t15.2023.12.17 val PER: 0.1507
2025-10-18 11:46:35,124: t15.2023.12.29 val PER: 0.1373
2025-10-18 11:46:35,125: t15.2024.02.25 val PER: 0.1250
2025-10-18 11:46:35,126: t15.2024.03.08 val PER: 0.2347
2025-10-18 11:46:35,126: t15.2024.03.15 val PER: 0.2045
2025-10-18 11:46:35,127: t15.2024.03.17 val PER: 0.1423
2025-10-18 11:46:35,128: t15.2024.05.10 val PER: 0.1516
2025-10-18 11:46:35,128: t15.2024.06.14 val PER: 0.1593
2025-10-18 11:46:35,130: t15.2024.07.19 val PER: 0.2340
2025-10-18 11:46:35,130: t15.2024.07.21 val PER: 0.0986
2025-10-18 11:46:35,131: t15.2024.07.28 val PER: 0.1581
2025-10-18 11:46:35,131: t15.2025.01.10 val PER: 0.3058
2025-10-18 11:46:35,133: t15.2025.01.12 val PER: 0.1647
2025-10-18 11:46:35,133: t15.2025.03.14 val PER: 0.3639
2025-10-18 11:46:35,134: t15.2025.03.16 val PER: 0.1924
2025-10-18 11:46:35,135: t15.2025.03.30 val PER: 0.2966
2025-10-18 11:46:35,136: t15.2025.04.13 val PER: 0.2068
2025-10-18 11:46:35,136: New best test PER 0.2252 --> 0.1577
2025-10-18 11:46:35,137: Checkpointing model
2025-10-18 11:46:36,580: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 11:49:54,254: Train batch 4200: loss: 5.35 grad norm: 26.12 time: 1.080
2025-10-18 11:53:17,369: Train batch 4400: loss: 7.38 grad norm: 42.27 time: 0.993
2025-10-18 11:56:36,594: Train batch 4600: loss: 4.08 grad norm: 26.55 time: 0.939
2025-10-18 11:59:58,706: Train batch 4800: loss: 7.20 grad norm: 28.44 time: 1.135
2025-10-18 12:03:19,359: Train batch 5000: loss: 4.96 grad norm: 23.47 time: 0.894
2025-10-18 12:06:38,623: Train batch 5200: loss: 7.72 grad norm: 33.87 time: 0.913
2025-10-18 12:09:56,701: Train batch 5400: loss: 2.32 grad norm: 19.99 time: 1.077
2025-10-18 12:13:12,149: Train batch 5600: loss: 4.99 grad norm: 23.93 time: 1.053
2025-10-18 12:16:31,123: Train batch 5800: loss: 3.28 grad norm: 20.84 time: 0.990
2025-10-18 12:19:52,007: Train batch 6000: loss: 4.80 grad norm: 28.62 time: 1.014
2025-10-18 12:19:52,008: Running test after training batch: 6000
2025-10-18 12:20:31,753: Val batch 6000: PER (avg): 0.1375 CTC Loss (avg): 17.0313 time: 39.744
2025-10-18 12:20:31,754: t15.2023.08.13 val PER: 0.0967
2025-10-18 12:20:31,755: t15.2023.08.18 val PER: 0.1031
2025-10-18 12:20:31,755: t15.2023.08.20 val PER: 0.0778
2025-10-18 12:20:31,756: t15.2023.08.25 val PER: 0.1024
2025-10-18 12:20:31,757: t15.2023.08.27 val PER: 0.1688
2025-10-18 12:20:31,758: t15.2023.09.01 val PER: 0.0601
2025-10-18 12:20:31,759: t15.2023.09.03 val PER: 0.1437
2025-10-18 12:20:31,760: t15.2023.09.24 val PER: 0.1068
2025-10-18 12:20:31,761: t15.2023.09.29 val PER: 0.1200
2025-10-18 12:20:31,762: t15.2023.10.01 val PER: 0.1651
2025-10-18 12:20:31,763: t15.2023.10.06 val PER: 0.1012
2025-10-18 12:20:31,764: t15.2023.10.08 val PER: 0.2287
2025-10-18 12:20:31,764: t15.2023.10.13 val PER: 0.1831
2025-10-18 12:20:31,765: t15.2023.10.15 val PER: 0.1470
2025-10-18 12:20:31,766: t15.2023.10.20 val PER: 0.1779
2025-10-18 12:20:31,766: t15.2023.10.22 val PER: 0.1069
2025-10-18 12:20:31,767: t15.2023.11.03 val PER: 0.1581
2025-10-18 12:20:31,768: t15.2023.11.04 val PER: 0.0239
2025-10-18 12:20:31,769: t15.2023.11.17 val PER: 0.0311
2025-10-18 12:20:31,770: t15.2023.11.19 val PER: 0.0279
2025-10-18 12:20:31,770: t15.2023.11.26 val PER: 0.0971
2025-10-18 12:20:31,771: t15.2023.12.03 val PER: 0.0819
2025-10-18 12:20:31,772: t15.2023.12.08 val PER: 0.0925
2025-10-18 12:20:31,773: t15.2023.12.10 val PER: 0.0920
2025-10-18 12:20:31,774: t15.2023.12.17 val PER: 0.1185
2025-10-18 12:20:31,774: t15.2023.12.29 val PER: 0.1078
2025-10-18 12:20:31,775: t15.2024.02.25 val PER: 0.0983
2025-10-18 12:20:31,776: t15.2024.03.08 val PER: 0.2134
2025-10-18 12:20:31,776: t15.2024.03.15 val PER: 0.2020
2025-10-18 12:20:31,777: t15.2024.03.17 val PER: 0.1248
2025-10-18 12:20:31,779: t15.2024.05.10 val PER: 0.1575
2025-10-18 12:20:31,779: t15.2024.06.14 val PER: 0.1372
2025-10-18 12:20:31,780: t15.2024.07.19 val PER: 0.2011
2025-10-18 12:20:31,781: t15.2024.07.21 val PER: 0.0814
2025-10-18 12:20:31,781: t15.2024.07.28 val PER: 0.1301
2025-10-18 12:20:31,782: t15.2025.01.10 val PER: 0.2645
2025-10-18 12:20:31,783: t15.2025.01.12 val PER: 0.1447
2025-10-18 12:20:31,784: t15.2025.03.14 val PER: 0.3254
2025-10-18 12:20:31,784: t15.2025.03.16 val PER: 0.1950
2025-10-18 12:20:31,785: t15.2025.03.30 val PER: 0.2598
2025-10-18 12:20:31,786: t15.2025.04.13 val PER: 0.2254
2025-10-18 12:20:31,787: New best test PER 0.1577 --> 0.1375
2025-10-18 12:20:31,787: Checkpointing model
2025-10-18 12:20:33,204: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 12:23:54,272: Train batch 6200: loss: 5.49 grad norm: 28.87 time: 1.273
2025-10-18 12:27:18,316: Train batch 6400: loss: 6.60 grad norm: 27.33 time: 1.116
2025-10-18 12:30:39,203: Train batch 6600: loss: 5.97 grad norm: 36.91 time: 1.049
2025-10-18 12:33:54,053: Train batch 6800: loss: 3.17 grad norm: 26.26 time: 0.740
2025-10-18 12:37:11,732: Train batch 7000: loss: 4.78 grad norm: 28.92 time: 0.986
2025-10-18 12:40:31,366: Train batch 7200: loss: 2.41 grad norm: 21.25 time: 1.166
2025-10-18 12:43:51,734: Train batch 7400: loss: 4.23 grad norm: 27.97 time: 1.363
2025-10-18 12:47:10,532: Train batch 7600: loss: 2.62 grad norm: 23.92 time: 1.036
2025-10-18 12:50:26,254: Train batch 7800: loss: 3.37 grad norm: 30.40 time: 1.102
2025-10-18 12:53:44,391: Train batch 8000: loss: 2.25 grad norm: 19.08 time: 0.764
2025-10-18 12:53:44,392: Running test after training batch: 8000
2025-10-18 12:54:26,291: Val batch 8000: PER (avg): 0.1310 CTC Loss (avg): 17.2807 time: 41.899
2025-10-18 12:54:26,292: t15.2023.08.13 val PER: 0.0873
2025-10-18 12:54:26,293: t15.2023.08.18 val PER: 0.0947
2025-10-18 12:54:26,294: t15.2023.08.20 val PER: 0.0715
2025-10-18 12:54:26,295: t15.2023.08.25 val PER: 0.0828
2025-10-18 12:54:26,296: t15.2023.08.27 val PER: 0.1688
2025-10-18 12:54:26,296: t15.2023.09.01 val PER: 0.0593
2025-10-18 12:54:26,297: t15.2023.09.03 val PER: 0.1485
2025-10-18 12:54:26,298: t15.2023.09.24 val PER: 0.0971
2025-10-18 12:54:26,299: t15.2023.09.29 val PER: 0.1232
2025-10-18 12:54:26,300: t15.2023.10.01 val PER: 0.1671
2025-10-18 12:54:26,300: t15.2023.10.06 val PER: 0.0904
2025-10-18 12:54:26,301: t15.2023.10.08 val PER: 0.2111
2025-10-18 12:54:26,302: t15.2023.10.13 val PER: 0.1800
2025-10-18 12:54:26,303: t15.2023.10.15 val PER: 0.1384
2025-10-18 12:54:26,303: t15.2023.10.20 val PER: 0.1812
2025-10-18 12:54:26,304: t15.2023.10.22 val PER: 0.0969
2025-10-18 12:54:26,307: t15.2023.11.03 val PER: 0.1513
2025-10-18 12:54:26,308: t15.2023.11.04 val PER: 0.0273
2025-10-18 12:54:26,309: t15.2023.11.17 val PER: 0.0404
2025-10-18 12:54:26,309: t15.2023.11.19 val PER: 0.0319
2025-10-18 12:54:26,310: t15.2023.11.26 val PER: 0.0855
2025-10-18 12:54:26,311: t15.2023.12.03 val PER: 0.0735
2025-10-18 12:54:26,312: t15.2023.12.08 val PER: 0.0739
2025-10-18 12:54:26,312: t15.2023.12.10 val PER: 0.0670
2025-10-18 12:54:26,313: t15.2023.12.17 val PER: 0.1154
2025-10-18 12:54:26,314: t15.2023.12.29 val PER: 0.1009
2025-10-18 12:54:26,315: t15.2024.02.25 val PER: 0.0969
2025-10-18 12:54:26,316: t15.2024.03.08 val PER: 0.1807
2025-10-18 12:54:26,316: t15.2024.03.15 val PER: 0.1957
2025-10-18 12:54:26,317: t15.2024.03.17 val PER: 0.1192
2025-10-18 12:54:26,318: t15.2024.05.10 val PER: 0.1471
2025-10-18 12:54:26,319: t15.2024.06.14 val PER: 0.1404
2025-10-18 12:54:26,319: t15.2024.07.19 val PER: 0.1879
2025-10-18 12:54:26,321: t15.2024.07.21 val PER: 0.0731
2025-10-18 12:54:26,322: t15.2024.07.28 val PER: 0.1110
2025-10-18 12:54:26,322: t15.2025.01.10 val PER: 0.2645
2025-10-18 12:54:26,323: t15.2025.01.12 val PER: 0.1440
2025-10-18 12:54:26,324: t15.2025.03.14 val PER: 0.3432
2025-10-18 12:54:26,324: t15.2025.03.16 val PER: 0.1846
2025-10-18 12:54:26,325: t15.2025.03.30 val PER: 0.2713
2025-10-18 12:54:26,326: t15.2025.04.13 val PER: 0.2140
2025-10-18 12:54:26,327: New best test PER 0.1375 --> 0.1310
2025-10-18 12:54:26,327: Checkpointing model
2025-10-18 12:54:27,755: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 12:57:46,827: Train batch 8200: loss: 3.11 grad norm: 31.57 time: 1.115
2025-10-18 13:01:02,725: Train batch 8400: loss: 4.32 grad norm: 25.30 time: 1.072
2025-10-18 13:04:26,449: Train batch 8600: loss: 2.08 grad norm: 18.93 time: 0.816
2025-10-18 13:07:43,057: Train batch 8800: loss: 3.24 grad norm: 27.26 time: 0.766
2025-10-18 13:11:03,190: Train batch 9000: loss: 2.53 grad norm: 26.20 time: 0.759
2025-10-18 13:14:25,566: Train batch 9200: loss: 2.16 grad norm: 18.25 time: 0.834
2025-10-18 13:17:44,437: Train batch 9400: loss: 1.61 grad norm: 16.79 time: 1.102
2025-10-18 13:21:04,421: Train batch 9600: loss: 1.91 grad norm: 22.42 time: 0.834
2025-10-18 13:24:18,109: Train batch 9800: loss: 1.36 grad norm: 13.49 time: 0.898
2025-10-18 13:27:38,169: Train batch 10000: loss: 4.35 grad norm: 34.13 time: 1.031
2025-10-18 13:27:38,170: Running test after training batch: 10000
2025-10-18 13:28:21,844: Val batch 10000: PER (avg): 0.1245 CTC Loss (avg): 18.0572 time: 43.673
2025-10-18 13:28:21,845: t15.2023.08.13 val PER: 0.0759
2025-10-18 13:28:21,846: t15.2023.08.18 val PER: 0.0964
2025-10-18 13:28:21,846: t15.2023.08.20 val PER: 0.0643
2025-10-18 13:28:21,847: t15.2023.08.25 val PER: 0.0904
2025-10-18 13:28:21,848: t15.2023.08.27 val PER: 0.1801
2025-10-18 13:28:21,849: t15.2023.09.01 val PER: 0.0625
2025-10-18 13:28:21,849: t15.2023.09.03 val PER: 0.1342
2025-10-18 13:28:21,850: t15.2023.09.24 val PER: 0.0971
2025-10-18 13:28:21,851: t15.2023.09.29 val PER: 0.1136
2025-10-18 13:28:21,851: t15.2023.10.01 val PER: 0.1546
2025-10-18 13:28:21,852: t15.2023.10.06 val PER: 0.0904
2025-10-18 13:28:21,853: t15.2023.10.08 val PER: 0.1922
2025-10-18 13:28:21,854: t15.2023.10.13 val PER: 0.1707
2025-10-18 13:28:21,854: t15.2023.10.15 val PER: 0.1332
2025-10-18 13:28:21,855: t15.2023.10.20 val PER: 0.1779
2025-10-18 13:28:21,856: t15.2023.10.22 val PER: 0.1125
2025-10-18 13:28:21,856: t15.2023.11.03 val PER: 0.1682
2025-10-18 13:28:21,857: t15.2023.11.04 val PER: 0.0273
2025-10-18 13:28:21,858: t15.2023.11.17 val PER: 0.0249
2025-10-18 13:28:21,858: t15.2023.11.19 val PER: 0.0279
2025-10-18 13:28:21,859: t15.2023.11.26 val PER: 0.0768
2025-10-18 13:28:21,861: t15.2023.12.03 val PER: 0.0683
2025-10-18 13:28:21,861: t15.2023.12.08 val PER: 0.0619
2025-10-18 13:28:21,862: t15.2023.12.10 val PER: 0.0539
2025-10-18 13:28:21,862: t15.2023.12.17 val PER: 0.1102
2025-10-18 13:28:21,863: t15.2023.12.29 val PER: 0.0933
2025-10-18 13:28:21,864: t15.2024.02.25 val PER: 0.0927
2025-10-18 13:28:21,864: t15.2024.03.08 val PER: 0.2034
2025-10-18 13:28:21,865: t15.2024.03.15 val PER: 0.1757
2025-10-18 13:28:21,866: t15.2024.03.17 val PER: 0.1123
2025-10-18 13:28:21,867: t15.2024.05.10 val PER: 0.1189
2025-10-18 13:28:21,868: t15.2024.06.14 val PER: 0.1278
2025-10-18 13:28:21,868: t15.2024.07.19 val PER: 0.1846
2025-10-18 13:28:21,869: t15.2024.07.21 val PER: 0.0731
2025-10-18 13:28:21,870: t15.2024.07.28 val PER: 0.0941
2025-10-18 13:28:21,870: t15.2025.01.10 val PER: 0.2590
2025-10-18 13:28:21,871: t15.2025.01.12 val PER: 0.1263
2025-10-18 13:28:21,873: t15.2025.03.14 val PER: 0.3210
2025-10-18 13:28:21,873: t15.2025.03.16 val PER: 0.1872
2025-10-18 13:28:21,874: t15.2025.03.30 val PER: 0.2483
2025-10-18 13:28:21,874: t15.2025.04.13 val PER: 0.2011
2025-10-18 13:28:21,875: New best test PER 0.1310 --> 0.1245
2025-10-18 13:28:21,876: Checkpointing model
2025-10-18 13:28:23,290: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 13:31:38,373: Train batch 10200: loss: 1.10 grad norm: 14.84 time: 1.035
2025-10-18 13:34:58,996: Train batch 10400: loss: 1.71 grad norm: 30.30 time: 1.051
2025-10-18 13:38:16,034: Train batch 10600: loss: 1.82 grad norm: 20.03 time: 0.885
2025-10-18 13:41:33,290: Train batch 10800: loss: 1.69 grad norm: 16.83 time: 0.970
2025-10-18 13:44:57,795: Train batch 11000: loss: 1.37 grad norm: 19.53 time: 1.161
2025-10-18 13:48:15,410: Train batch 11200: loss: 2.94 grad norm: 27.63 time: 0.867
2025-10-18 13:51:31,325: Train batch 11400: loss: 1.40 grad norm: 16.40 time: 0.719
2025-10-18 13:54:50,552: Train batch 11600: loss: 1.97 grad norm: 23.93 time: 0.745
2025-10-18 13:58:06,869: Train batch 11800: loss: 0.95 grad norm: 17.51 time: 1.117
2025-10-18 14:01:20,637: Train batch 12000: loss: 1.12 grad norm: 28.74 time: 1.116
2025-10-18 14:01:20,638: Running test after training batch: 12000
2025-10-18 14:02:02,859: Val batch 12000: PER (avg): 0.1255 CTC Loss (avg): 19.2307 time: 42.220
2025-10-18 14:02:02,860: t15.2023.08.13 val PER: 0.0863
2025-10-18 14:02:02,861: t15.2023.08.18 val PER: 0.0863
2025-10-18 14:02:02,861: t15.2023.08.20 val PER: 0.0715
2025-10-18 14:02:02,862: t15.2023.08.25 val PER: 0.0979
2025-10-18 14:02:02,863: t15.2023.08.27 val PER: 0.1817
2025-10-18 14:02:02,864: t15.2023.09.01 val PER: 0.0584
2025-10-18 14:02:02,865: t15.2023.09.03 val PER: 0.1235
2025-10-18 14:02:02,866: t15.2023.09.24 val PER: 0.0983
2025-10-18 14:02:02,867: t15.2023.09.29 val PER: 0.1321
2025-10-18 14:02:02,868: t15.2023.10.01 val PER: 0.1559
2025-10-18 14:02:02,868: t15.2023.10.06 val PER: 0.0904
2025-10-18 14:02:02,869: t15.2023.10.08 val PER: 0.2003
2025-10-18 14:02:02,870: t15.2023.10.13 val PER: 0.1746
2025-10-18 14:02:02,871: t15.2023.10.15 val PER: 0.1397
2025-10-18 14:02:02,871: t15.2023.10.20 val PER: 0.1678
2025-10-18 14:02:02,872: t15.2023.10.22 val PER: 0.1058
2025-10-18 14:02:02,873: t15.2023.11.03 val PER: 0.1459
2025-10-18 14:02:02,874: t15.2023.11.04 val PER: 0.0239
2025-10-18 14:02:02,875: t15.2023.11.17 val PER: 0.0202
2025-10-18 14:02:02,876: t15.2023.11.19 val PER: 0.0200
2025-10-18 14:02:02,877: t15.2023.11.26 val PER: 0.0710
2025-10-18 14:02:02,877: t15.2023.12.03 val PER: 0.0788
2025-10-18 14:02:02,878: t15.2023.12.08 val PER: 0.0593
2025-10-18 14:02:02,879: t15.2023.12.10 val PER: 0.0670
2025-10-18 14:02:02,879: t15.2023.12.17 val PER: 0.0988
2025-10-18 14:02:02,880: t15.2023.12.29 val PER: 0.1002
2025-10-18 14:02:02,882: t15.2024.02.25 val PER: 0.0857
2025-10-18 14:02:02,882: t15.2024.03.08 val PER: 0.2077
2025-10-18 14:02:02,883: t15.2024.03.15 val PER: 0.1907
2025-10-18 14:02:02,884: t15.2024.03.17 val PER: 0.1088
2025-10-18 14:02:02,885: t15.2024.05.10 val PER: 0.1426
2025-10-18 14:02:02,886: t15.2024.06.14 val PER: 0.1372
2025-10-18 14:02:02,887: t15.2024.07.19 val PER: 0.1721
2025-10-18 14:02:02,887: t15.2024.07.21 val PER: 0.0786
2025-10-18 14:02:02,888: t15.2024.07.28 val PER: 0.1132
2025-10-18 14:02:02,889: t15.2025.01.10 val PER: 0.2713
2025-10-18 14:02:02,890: t15.2025.01.12 val PER: 0.1216
2025-10-18 14:02:02,891: t15.2025.03.14 val PER: 0.3003
2025-10-18 14:02:02,892: t15.2025.03.16 val PER: 0.1806
2025-10-18 14:02:02,893: t15.2025.03.30 val PER: 0.2437
2025-10-18 14:02:02,894: t15.2025.04.13 val PER: 0.2011
2025-10-18 14:05:18,041: Train batch 12200: loss: 1.78 grad norm: 47.89 time: 1.054
2025-10-18 14:08:34,559: Train batch 12400: loss: 0.69 grad norm: 20.26 time: 1.347
2025-10-18 14:11:53,819: Train batch 12600: loss: 1.44 grad norm: 23.74 time: 0.963
2025-10-18 14:15:11,077: Train batch 12800: loss: 3.87 grad norm: 25.50 time: 1.166
2025-10-18 14:18:30,379: Train batch 13000: loss: 1.67 grad norm: 21.09 time: 1.263
2025-10-18 14:21:46,996: Train batch 13200: loss: 0.83 grad norm: 15.50 time: 0.866
2025-10-18 14:25:05,040: Train batch 13400: loss: 0.70 grad norm: 13.94 time: 0.850
2025-10-18 14:28:24,940: Train batch 13600: loss: 1.14 grad norm: 14.89 time: 1.151
2025-10-18 14:31:45,843: Train batch 13800: loss: 0.77 grad norm: 13.48 time: 0.905
2025-10-18 14:35:08,261: Train batch 14000: loss: 0.78 grad norm: 16.20 time: 0.905
2025-10-18 14:35:08,262: Running test after training batch: 14000
2025-10-18 14:35:51,627: Val batch 14000: PER (avg): 0.1202 CTC Loss (avg): 19.5162 time: 43.365
2025-10-18 14:35:51,628: t15.2023.08.13 val PER: 0.0811
2025-10-18 14:35:51,629: t15.2023.08.18 val PER: 0.0813
2025-10-18 14:35:51,630: t15.2023.08.20 val PER: 0.0508
2025-10-18 14:35:51,631: t15.2023.08.25 val PER: 0.0813
2025-10-18 14:35:51,631: t15.2023.08.27 val PER: 0.1559
2025-10-18 14:35:51,632: t15.2023.09.01 val PER: 0.0552
2025-10-18 14:35:51,634: t15.2023.09.03 val PER: 0.1330
2025-10-18 14:35:51,635: t15.2023.09.24 val PER: 0.0886
2025-10-18 14:35:51,635: t15.2023.09.29 val PER: 0.1149
2025-10-18 14:35:51,636: t15.2023.10.01 val PER: 0.1493
2025-10-18 14:35:51,637: t15.2023.10.06 val PER: 0.0732
2025-10-18 14:35:51,638: t15.2023.10.08 val PER: 0.1881
2025-10-18 14:35:51,639: t15.2023.10.13 val PER: 0.1606
2025-10-18 14:35:51,640: t15.2023.10.15 val PER: 0.1318
2025-10-18 14:35:51,641: t15.2023.10.20 val PER: 0.1510
2025-10-18 14:35:51,641: t15.2023.10.22 val PER: 0.1069
2025-10-18 14:35:51,642: t15.2023.11.03 val PER: 0.1526
2025-10-18 14:35:51,643: t15.2023.11.04 val PER: 0.0171
2025-10-18 14:35:51,643: t15.2023.11.17 val PER: 0.0358
2025-10-18 14:35:51,644: t15.2023.11.19 val PER: 0.0299
2025-10-18 14:35:51,645: t15.2023.11.26 val PER: 0.0768
2025-10-18 14:35:51,645: t15.2023.12.03 val PER: 0.0672
2025-10-18 14:35:51,647: t15.2023.12.08 val PER: 0.0546
2025-10-18 14:35:51,648: t15.2023.12.10 val PER: 0.0565
2025-10-18 14:35:51,649: t15.2023.12.17 val PER: 0.1071
2025-10-18 14:35:51,650: t15.2023.12.29 val PER: 0.0837
2025-10-18 14:35:51,650: t15.2024.02.25 val PER: 0.0787
2025-10-18 14:35:51,651: t15.2024.03.08 val PER: 0.2105
2025-10-18 14:35:51,652: t15.2024.03.15 val PER: 0.1832
2025-10-18 14:35:51,653: t15.2024.03.17 val PER: 0.1046
2025-10-18 14:35:51,653: t15.2024.05.10 val PER: 0.1263
2025-10-18 14:35:51,654: t15.2024.06.14 val PER: 0.1293
2025-10-18 14:35:51,655: t15.2024.07.19 val PER: 0.1819
2025-10-18 14:35:51,656: t15.2024.07.21 val PER: 0.0703
2025-10-18 14:35:51,656: t15.2024.07.28 val PER: 0.1059
2025-10-18 14:35:51,658: t15.2025.01.10 val PER: 0.2521
2025-10-18 14:35:51,658: t15.2025.01.12 val PER: 0.1316
2025-10-18 14:35:51,659: t15.2025.03.14 val PER: 0.3254
2025-10-18 14:35:51,660: t15.2025.03.16 val PER: 0.1623
2025-10-18 14:35:51,661: t15.2025.03.30 val PER: 0.2402
2025-10-18 14:35:51,661: t15.2025.04.13 val PER: 0.1983
2025-10-18 14:35:51,662: New best test PER 0.1245 --> 0.1202
2025-10-18 14:35:51,662: Checkpointing model
2025-10-18 14:35:53,092: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 14:39:08,958: Train batch 14200: loss: 0.74 grad norm: 17.02 time: 0.838
2025-10-18 14:42:27,412: Train batch 14400: loss: 0.18 grad norm: 5.85 time: 0.842
2025-10-18 14:45:47,435: Train batch 14600: loss: 0.93 grad norm: 20.83 time: 0.960
2025-10-18 14:49:02,508: Train batch 14800: loss: 0.97 grad norm: 17.89 time: 0.945
2025-10-18 14:52:20,102: Train batch 15000: loss: 0.86 grad norm: 11.16 time: 1.294
2025-10-18 14:55:37,620: Train batch 15200: loss: 1.03 grad norm: 43.11 time: 0.968
2025-10-18 14:58:53,853: Train batch 15400: loss: 0.66 grad norm: 20.36 time: 1.136
2025-10-18 15:02:15,758: Train batch 15600: loss: 1.41 grad norm: 13.31 time: 0.892
2025-10-18 15:05:33,886: Train batch 15800: loss: 0.71 grad norm: 11.13 time: 0.687
2025-10-18 15:08:50,614: Train batch 16000: loss: 0.96 grad norm: 15.05 time: 0.933
2025-10-18 15:08:50,616: Running test after training batch: 16000
2025-10-18 15:09:36,738: Val batch 16000: PER (avg): 0.1181 CTC Loss (avg): 19.6140 time: 46.122
2025-10-18 15:09:36,739: t15.2023.08.13 val PER: 0.0832
2025-10-18 15:09:36,740: t15.2023.08.18 val PER: 0.0746
2025-10-18 15:09:36,741: t15.2023.08.20 val PER: 0.0612
2025-10-18 15:09:36,742: t15.2023.08.25 val PER: 0.0783
2025-10-18 15:09:36,742: t15.2023.08.27 val PER: 0.1624
2025-10-18 15:09:36,743: t15.2023.09.01 val PER: 0.0536
2025-10-18 15:09:36,744: t15.2023.09.03 val PER: 0.1473
2025-10-18 15:09:36,745: t15.2023.09.24 val PER: 0.0910
2025-10-18 15:09:36,745: t15.2023.09.29 val PER: 0.1117
2025-10-18 15:09:36,747: t15.2023.10.01 val PER: 0.1446
2025-10-18 15:09:36,748: t15.2023.10.06 val PER: 0.0840
2025-10-18 15:09:36,748: t15.2023.10.08 val PER: 0.1949
2025-10-18 15:09:36,749: t15.2023.10.13 val PER: 0.1606
2025-10-18 15:09:36,750: t15.2023.10.15 val PER: 0.1272
2025-10-18 15:09:36,751: t15.2023.10.20 val PER: 0.1812
2025-10-18 15:09:36,752: t15.2023.10.22 val PER: 0.0980
2025-10-18 15:09:36,753: t15.2023.11.03 val PER: 0.1459
2025-10-18 15:09:36,754: t15.2023.11.04 val PER: 0.0307
2025-10-18 15:09:36,755: t15.2023.11.17 val PER: 0.0187
2025-10-18 15:09:36,755: t15.2023.11.19 val PER: 0.0240
2025-10-18 15:09:36,756: t15.2023.11.26 val PER: 0.0754
2025-10-18 15:09:36,757: t15.2023.12.03 val PER: 0.0609
2025-10-18 15:09:36,758: t15.2023.12.08 val PER: 0.0599
2025-10-18 15:09:36,759: t15.2023.12.10 val PER: 0.0342
2025-10-18 15:09:36,760: t15.2023.12.17 val PER: 0.0998
2025-10-18 15:09:36,761: t15.2023.12.29 val PER: 0.0858
2025-10-18 15:09:36,762: t15.2024.02.25 val PER: 0.0801
2025-10-18 15:09:36,762: t15.2024.03.08 val PER: 0.1935
2025-10-18 15:09:36,763: t15.2024.03.15 val PER: 0.1764
2025-10-18 15:09:36,764: t15.2024.03.17 val PER: 0.1053
2025-10-18 15:09:36,765: t15.2024.05.10 val PER: 0.1337
2025-10-18 15:09:36,766: t15.2024.06.14 val PER: 0.1309
2025-10-18 15:09:36,766: t15.2024.07.19 val PER: 0.1879
2025-10-18 15:09:36,767: t15.2024.07.21 val PER: 0.0641
2025-10-18 15:09:36,768: t15.2024.07.28 val PER: 0.1007
2025-10-18 15:09:36,768: t15.2025.01.10 val PER: 0.2548
2025-10-18 15:09:36,770: t15.2025.01.12 val PER: 0.1155
2025-10-18 15:09:36,770: t15.2025.03.14 val PER: 0.2944
2025-10-18 15:09:36,771: t15.2025.03.16 val PER: 0.1715
2025-10-18 15:09:36,772: t15.2025.03.30 val PER: 0.2471
2025-10-18 15:09:36,772: t15.2025.04.13 val PER: 0.1812
2025-10-18 15:09:36,773: New best test PER 0.1202 --> 0.1181
2025-10-18 15:09:36,773: Checkpointing model
2025-10-18 15:09:38,221: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 15:12:55,517: Train batch 16200: loss: 0.60 grad norm: 12.87 time: 0.840
2025-10-18 15:16:13,636: Train batch 16400: loss: 1.54 grad norm: 21.60 time: 0.900
2025-10-18 15:19:34,539: Train batch 16600: loss: 0.26 grad norm: 16.00 time: 0.894
2025-10-18 15:22:55,903: Train batch 16800: loss: 0.79 grad norm: 24.08 time: 1.204
2025-10-18 15:26:15,918: Train batch 17000: loss: 0.66 grad norm: 12.79 time: 0.946
2025-10-18 15:29:33,029: Train batch 17200: loss: 1.16 grad norm: 13.21 time: 0.855
2025-10-18 15:32:52,361: Train batch 17400: loss: 0.42 grad norm: 8.79 time: 1.214
2025-10-18 15:36:10,063: Train batch 17600: loss: 1.00 grad norm: 27.72 time: 0.846
2025-10-18 15:39:30,647: Train batch 17800: loss: 0.95 grad norm: 21.97 time: 1.364
2025-10-18 15:42:49,083: Train batch 18000: loss: 0.84 grad norm: 18.50 time: 1.207
2025-10-18 15:42:49,084: Running test after training batch: 18000
2025-10-18 15:43:29,370: Val batch 18000: PER (avg): 0.1181 CTC Loss (avg): 20.7056 time: 40.285
2025-10-18 15:43:29,371: t15.2023.08.13 val PER: 0.0832
2025-10-18 15:43:29,372: t15.2023.08.18 val PER: 0.0780
2025-10-18 15:43:29,372: t15.2023.08.20 val PER: 0.0556
2025-10-18 15:43:29,374: t15.2023.08.25 val PER: 0.0904
2025-10-18 15:43:29,374: t15.2023.08.27 val PER: 0.1656
2025-10-18 15:43:29,376: t15.2023.09.01 val PER: 0.0471
2025-10-18 15:43:29,376: t15.2023.09.03 val PER: 0.1318
2025-10-18 15:43:29,377: t15.2023.09.24 val PER: 0.0801
2025-10-18 15:43:29,378: t15.2023.09.29 val PER: 0.1123
2025-10-18 15:43:29,378: t15.2023.10.01 val PER: 0.1460
2025-10-18 15:43:29,379: t15.2023.10.06 val PER: 0.0807
2025-10-18 15:43:29,380: t15.2023.10.08 val PER: 0.1962
2025-10-18 15:43:29,381: t15.2023.10.13 val PER: 0.1645
2025-10-18 15:43:29,382: t15.2023.10.15 val PER: 0.1378
2025-10-18 15:43:29,383: t15.2023.10.20 val PER: 0.1611
2025-10-18 15:43:29,384: t15.2023.10.22 val PER: 0.1036
2025-10-18 15:43:29,385: t15.2023.11.03 val PER: 0.1554
2025-10-18 15:43:29,386: t15.2023.11.04 val PER: 0.0307
2025-10-18 15:43:29,387: t15.2023.11.17 val PER: 0.0140
2025-10-18 15:43:29,387: t15.2023.11.19 val PER: 0.0240
2025-10-18 15:43:29,388: t15.2023.11.26 val PER: 0.0652
2025-10-18 15:43:29,389: t15.2023.12.03 val PER: 0.0630
2025-10-18 15:43:29,389: t15.2023.12.08 val PER: 0.0519
2025-10-18 15:43:29,391: t15.2023.12.10 val PER: 0.0512
2025-10-18 15:43:29,391: t15.2023.12.17 val PER: 0.1060
2025-10-18 15:43:29,392: t15.2023.12.29 val PER: 0.0933
2025-10-18 15:43:29,393: t15.2024.02.25 val PER: 0.0801
2025-10-18 15:43:29,393: t15.2024.03.08 val PER: 0.1849
2025-10-18 15:43:29,395: t15.2024.03.15 val PER: 0.1920
2025-10-18 15:43:29,395: t15.2024.03.17 val PER: 0.0934
2025-10-18 15:43:29,396: t15.2024.05.10 val PER: 0.1322
2025-10-18 15:43:29,397: t15.2024.06.14 val PER: 0.1199
2025-10-18 15:43:29,397: t15.2024.07.19 val PER: 0.1767
2025-10-18 15:43:29,398: t15.2024.07.21 val PER: 0.0690
2025-10-18 15:43:29,399: t15.2024.07.28 val PER: 0.0897
2025-10-18 15:43:29,399: t15.2025.01.10 val PER: 0.2438
2025-10-18 15:43:29,400: t15.2025.01.12 val PER: 0.1201
2025-10-18 15:43:29,401: t15.2025.03.14 val PER: 0.3210
2025-10-18 15:43:29,402: t15.2025.03.16 val PER: 0.1702
2025-10-18 15:43:29,403: t15.2025.03.30 val PER: 0.2425
2025-10-18 15:43:29,404: t15.2025.04.13 val PER: 0.1840
2025-10-18 15:43:29,404: New best test PER 0.1181 --> 0.1181
2025-10-18 15:43:29,405: Checkpointing model
2025-10-18 15:43:30,835: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 15:46:49,437: Train batch 18200: loss: 2.03 grad norm: 31.91 time: 1.151
2025-10-18 15:50:09,607: Train batch 18400: loss: 0.35 grad norm: 6.85 time: 1.095
2025-10-18 15:53:29,322: Train batch 18600: loss: 0.67 grad norm: 15.29 time: 1.035
2025-10-18 15:56:50,438: Train batch 18800: loss: 0.57 grad norm: 13.07 time: 0.969
2025-10-18 16:00:13,155: Train batch 19000: loss: 0.38 grad norm: 8.58 time: 1.076
2025-10-18 16:03:32,395: Train batch 19200: loss: 0.20 grad norm: 5.71 time: 1.051
2025-10-18 16:06:52,341: Train batch 19400: loss: 0.32 grad norm: 16.99 time: 1.077
2025-10-18 16:10:14,110: Train batch 19600: loss: 0.32 grad norm: 8.26 time: 0.969
2025-10-18 16:13:36,044: Train batch 19800: loss: 0.20 grad norm: 7.63 time: 0.787
2025-10-18 16:16:58,479: Train batch 20000: loss: 0.44 grad norm: 10.90 time: 0.872
2025-10-18 16:16:58,481: Running test after training batch: 20000
2025-10-18 16:17:41,953: Val batch 20000: PER (avg): 0.1156 CTC Loss (avg): 20.4240 time: 43.471
2025-10-18 16:17:41,954: t15.2023.08.13 val PER: 0.0780
2025-10-18 16:17:41,955: t15.2023.08.18 val PER: 0.0763
2025-10-18 16:17:41,956: t15.2023.08.20 val PER: 0.0643
2025-10-18 16:17:41,957: t15.2023.08.25 val PER: 0.0873
2025-10-18 16:17:41,958: t15.2023.08.27 val PER: 0.1527
2025-10-18 16:17:41,959: t15.2023.09.01 val PER: 0.0552
2025-10-18 16:17:41,959: t15.2023.09.03 val PER: 0.1354
2025-10-18 16:17:41,960: t15.2023.09.24 val PER: 0.0886
2025-10-18 16:17:41,961: t15.2023.09.29 val PER: 0.1008
2025-10-18 16:17:41,962: t15.2023.10.01 val PER: 0.1460
2025-10-18 16:17:41,963: t15.2023.10.06 val PER: 0.0926
2025-10-18 16:17:41,964: t15.2023.10.08 val PER: 0.1976
2025-10-18 16:17:41,965: t15.2023.10.13 val PER: 0.1629
2025-10-18 16:17:41,965: t15.2023.10.15 val PER: 0.1180
2025-10-18 16:17:41,966: t15.2023.10.20 val PER: 0.1577
2025-10-18 16:17:41,967: t15.2023.10.22 val PER: 0.0969
2025-10-18 16:17:41,968: t15.2023.11.03 val PER: 0.1459
2025-10-18 16:17:41,968: t15.2023.11.04 val PER: 0.0273
2025-10-18 16:17:41,969: t15.2023.11.17 val PER: 0.0140
2025-10-18 16:17:41,971: t15.2023.11.19 val PER: 0.0259
2025-10-18 16:17:41,971: t15.2023.11.26 val PER: 0.0681
2025-10-18 16:17:41,972: t15.2023.12.03 val PER: 0.0693
2025-10-18 16:17:41,973: t15.2023.12.08 val PER: 0.0439
2025-10-18 16:17:41,973: t15.2023.12.10 val PER: 0.0486
2025-10-18 16:17:41,975: t15.2023.12.17 val PER: 0.0977
2025-10-18 16:17:41,975: t15.2023.12.29 val PER: 0.0851
2025-10-18 16:17:41,976: t15.2024.02.25 val PER: 0.0758
2025-10-18 16:17:41,977: t15.2024.03.08 val PER: 0.1679
2025-10-18 16:17:41,978: t15.2024.03.15 val PER: 0.1864
2025-10-18 16:17:41,979: t15.2024.03.17 val PER: 0.0934
2025-10-18 16:17:41,980: t15.2024.05.10 val PER: 0.1337
2025-10-18 16:17:41,980: t15.2024.06.14 val PER: 0.1183
2025-10-18 16:17:41,981: t15.2024.07.19 val PER: 0.1773
2025-10-18 16:17:41,982: t15.2024.07.21 val PER: 0.0586
2025-10-18 16:17:41,982: t15.2024.07.28 val PER: 0.1007
2025-10-18 16:17:41,984: t15.2025.01.10 val PER: 0.2438
2025-10-18 16:17:41,985: t15.2025.01.12 val PER: 0.1201
2025-10-18 16:17:41,985: t15.2025.03.14 val PER: 0.3033
2025-10-18 16:17:41,986: t15.2025.03.16 val PER: 0.1584
2025-10-18 16:17:41,987: t15.2025.03.30 val PER: 0.2460
2025-10-18 16:17:41,988: t15.2025.04.13 val PER: 0.1926
2025-10-18 16:17:41,988: New best test PER 0.1181 --> 0.1156
2025-10-18 16:17:41,989: Checkpointing model
2025-10-18 16:17:43,462: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 16:20:59,761: Train batch 20200: loss: 0.36 grad norm: 7.22 time: 0.921
2025-10-18 16:24:18,139: Train batch 20400: loss: 0.09 grad norm: 3.37 time: 1.369
2025-10-18 16:27:34,218: Train batch 20600: loss: 0.50 grad norm: 10.92 time: 1.170
2025-10-18 16:30:51,759: Train batch 20800: loss: 0.60 grad norm: 72.21 time: 1.057
2025-10-18 16:34:09,089: Train batch 21000: loss: 0.14 grad norm: 5.64 time: 0.872
2025-10-18 16:37:29,228: Train batch 21200: loss: 0.29 grad norm: 10.74 time: 0.892
2025-10-18 16:40:44,107: Train batch 21400: loss: 0.36 grad norm: 9.28 time: 0.782
2025-10-18 16:43:58,609: Train batch 21600: loss: 0.31 grad norm: 7.09 time: 0.914
2025-10-18 16:47:17,817: Train batch 21800: loss: 0.25 grad norm: 7.89 time: 0.802
