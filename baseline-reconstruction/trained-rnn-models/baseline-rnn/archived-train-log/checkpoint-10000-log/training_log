2025-10-18 10:36:59,823: Requested GPU 1 not available. Using GPU 0 instead.
2025-10-18 10:37:00,003: Using device: cuda:0
2025-10-18 10:37:01,042: Using torch.compile
2025-10-18 10:37:03,020: Initialized RNN decoding model
2025-10-18 10:37:03,021: OptimizedModule(
  (_orig_mod): GRUDecoder(
    (day_layer_activation): Softsign()
    (day_weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 512x512]
        (1): Parameter containing: [torch.float32 of size 512x512]
        (2): Parameter containing: [torch.float32 of size 512x512]
        (3): Parameter containing: [torch.float32 of size 512x512]
        (4): Parameter containing: [torch.float32 of size 512x512]
        (5): Parameter containing: [torch.float32 of size 512x512]
        (6): Parameter containing: [torch.float32 of size 512x512]
        (7): Parameter containing: [torch.float32 of size 512x512]
        (8): Parameter containing: [torch.float32 of size 512x512]
        (9): Parameter containing: [torch.float32 of size 512x512]
        (10): Parameter containing: [torch.float32 of size 512x512]
        (11): Parameter containing: [torch.float32 of size 512x512]
        (12): Parameter containing: [torch.float32 of size 512x512]
        (13): Parameter containing: [torch.float32 of size 512x512]
        (14): Parameter containing: [torch.float32 of size 512x512]
        (15): Parameter containing: [torch.float32 of size 512x512]
        (16): Parameter containing: [torch.float32 of size 512x512]
        (17): Parameter containing: [torch.float32 of size 512x512]
        (18): Parameter containing: [torch.float32 of size 512x512]
        (19): Parameter containing: [torch.float32 of size 512x512]
        (20): Parameter containing: [torch.float32 of size 512x512]
        (21): Parameter containing: [torch.float32 of size 512x512]
        (22): Parameter containing: [torch.float32 of size 512x512]
        (23): Parameter containing: [torch.float32 of size 512x512]
        (24): Parameter containing: [torch.float32 of size 512x512]
        (25): Parameter containing: [torch.float32 of size 512x512]
        (26): Parameter containing: [torch.float32 of size 512x512]
        (27): Parameter containing: [torch.float32 of size 512x512]
        (28): Parameter containing: [torch.float32 of size 512x512]
        (29): Parameter containing: [torch.float32 of size 512x512]
        (30): Parameter containing: [torch.float32 of size 512x512]
        (31): Parameter containing: [torch.float32 of size 512x512]
        (32): Parameter containing: [torch.float32 of size 512x512]
        (33): Parameter containing: [torch.float32 of size 512x512]
        (34): Parameter containing: [torch.float32 of size 512x512]
        (35): Parameter containing: [torch.float32 of size 512x512]
        (36): Parameter containing: [torch.float32 of size 512x512]
        (37): Parameter containing: [torch.float32 of size 512x512]
        (38): Parameter containing: [torch.float32 of size 512x512]
        (39): Parameter containing: [torch.float32 of size 512x512]
        (40): Parameter containing: [torch.float32 of size 512x512]
        (41): Parameter containing: [torch.float32 of size 512x512]
        (42): Parameter containing: [torch.float32 of size 512x512]
        (43): Parameter containing: [torch.float32 of size 512x512]
        (44): Parameter containing: [torch.float32 of size 512x512]
    )
    (day_biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1x512]
        (1): Parameter containing: [torch.float32 of size 1x512]
        (2): Parameter containing: [torch.float32 of size 1x512]
        (3): Parameter containing: [torch.float32 of size 1x512]
        (4): Parameter containing: [torch.float32 of size 1x512]
        (5): Parameter containing: [torch.float32 of size 1x512]
        (6): Parameter containing: [torch.float32 of size 1x512]
        (7): Parameter containing: [torch.float32 of size 1x512]
        (8): Parameter containing: [torch.float32 of size 1x512]
        (9): Parameter containing: [torch.float32 of size 1x512]
        (10): Parameter containing: [torch.float32 of size 1x512]
        (11): Parameter containing: [torch.float32 of size 1x512]
        (12): Parameter containing: [torch.float32 of size 1x512]
        (13): Parameter containing: [torch.float32 of size 1x512]
        (14): Parameter containing: [torch.float32 of size 1x512]
        (15): Parameter containing: [torch.float32 of size 1x512]
        (16): Parameter containing: [torch.float32 of size 1x512]
        (17): Parameter containing: [torch.float32 of size 1x512]
        (18): Parameter containing: [torch.float32 of size 1x512]
        (19): Parameter containing: [torch.float32 of size 1x512]
        (20): Parameter containing: [torch.float32 of size 1x512]
        (21): Parameter containing: [torch.float32 of size 1x512]
        (22): Parameter containing: [torch.float32 of size 1x512]
        (23): Parameter containing: [torch.float32 of size 1x512]
        (24): Parameter containing: [torch.float32 of size 1x512]
        (25): Parameter containing: [torch.float32 of size 1x512]
        (26): Parameter containing: [torch.float32 of size 1x512]
        (27): Parameter containing: [torch.float32 of size 1x512]
        (28): Parameter containing: [torch.float32 of size 1x512]
        (29): Parameter containing: [torch.float32 of size 1x512]
        (30): Parameter containing: [torch.float32 of size 1x512]
        (31): Parameter containing: [torch.float32 of size 1x512]
        (32): Parameter containing: [torch.float32 of size 1x512]
        (33): Parameter containing: [torch.float32 of size 1x512]
        (34): Parameter containing: [torch.float32 of size 1x512]
        (35): Parameter containing: [torch.float32 of size 1x512]
        (36): Parameter containing: [torch.float32 of size 1x512]
        (37): Parameter containing: [torch.float32 of size 1x512]
        (38): Parameter containing: [torch.float32 of size 1x512]
        (39): Parameter containing: [torch.float32 of size 1x512]
        (40): Parameter containing: [torch.float32 of size 1x512]
        (41): Parameter containing: [torch.float32 of size 1x512]
        (42): Parameter containing: [torch.float32 of size 1x512]
        (43): Parameter containing: [torch.float32 of size 1x512]
        (44): Parameter containing: [torch.float32 of size 1x512]
    )
    (day_layer_dropout): Dropout(p=0.2, inplace=False)
    (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.4)
    (out): Linear(in_features=768, out_features=41, bias=True)
  )
)
2025-10-18 10:37:03,024: Model has 44,315,177 parameters
2025-10-18 10:37:03,025: Model has 11,819,520 day-specific parameters | 26.67% of total parameters
2025-10-18 10:37:31,933: Successfully initialized datasets
2025-10-18 10:38:14,466: Train batch 0: loss: 753.82 grad norm: 302.81 time: 7.967
2025-10-18 10:38:14,467: Running test after training batch: 0
2025-10-18 10:39:03,134: Val batch 0: PER (avg): 1.2154 CTC Loss (avg): 714.9868 time: 48.666
2025-10-18 10:39:03,135: t15.2023.08.13 val PER: 1.1247
2025-10-18 10:39:03,136: t15.2023.08.18 val PER: 1.1459
2025-10-18 10:39:03,137: t15.2023.08.20 val PER: 1.1541
2025-10-18 10:39:03,138: t15.2023.08.25 val PER: 1.1611
2025-10-18 10:39:03,138: t15.2023.08.27 val PER: 1.0659
2025-10-18 10:39:03,139: t15.2023.09.01 val PER: 1.2248
2025-10-18 10:39:03,141: t15.2023.09.03 val PER: 1.1176
2025-10-18 10:39:03,142: t15.2023.09.24 val PER: 1.3337
2025-10-18 10:39:03,143: t15.2023.09.29 val PER: 1.2406
2025-10-18 10:39:03,143: t15.2023.10.01 val PER: 1.0542
2025-10-18 10:39:03,144: t15.2023.10.06 val PER: 1.2465
2025-10-18 10:39:03,145: t15.2023.10.08 val PER: 1.0406
2025-10-18 10:39:03,146: t15.2023.10.13 val PER: 1.1621
2025-10-18 10:39:03,146: t15.2023.10.15 val PER: 1.2037
2025-10-18 10:39:03,147: t15.2023.10.20 val PER: 1.3557
2025-10-18 10:39:03,148: t15.2023.10.22 val PER: 1.2996
2025-10-18 10:39:03,149: t15.2023.11.03 val PER: 1.2972
2025-10-18 10:39:03,150: t15.2023.11.04 val PER: 1.3891
2025-10-18 10:39:03,151: t15.2023.11.17 val PER: 1.6454
2025-10-18 10:39:03,152: t15.2023.11.19 val PER: 1.4351
2025-10-18 10:39:03,152: t15.2023.11.26 val PER: 1.2529
2025-10-18 10:39:03,154: t15.2023.12.03 val PER: 1.1838
2025-10-18 10:39:03,154: t15.2023.12.08 val PER: 1.2557
2025-10-18 10:39:03,155: t15.2023.12.10 val PER: 1.3219
2025-10-18 10:39:03,156: t15.2023.12.17 val PER: 1.0696
2025-10-18 10:39:03,157: t15.2023.12.29 val PER: 1.1640
2025-10-18 10:39:03,157: t15.2024.02.25 val PER: 1.1320
2025-10-18 10:39:03,158: t15.2024.03.08 val PER: 1.1550
2025-10-18 10:39:03,159: t15.2024.03.15 val PER: 1.1138
2025-10-18 10:39:03,160: t15.2024.03.17 val PER: 1.1722
2025-10-18 10:39:03,160: t15.2024.05.10 val PER: 1.2065
2025-10-18 10:39:03,161: t15.2024.06.14 val PER: 1.4132
2025-10-18 10:39:03,162: t15.2024.07.19 val PER: 0.9881
2025-10-18 10:39:03,163: t15.2024.07.21 val PER: 1.4193
2025-10-18 10:39:03,163: t15.2024.07.28 val PER: 1.4654
2025-10-18 10:39:03,164: t15.2025.01.10 val PER: 0.9518
2025-10-18 10:39:03,165: t15.2025.01.12 val PER: 1.4126
2025-10-18 10:39:03,165: t15.2025.03.14 val PER: 1.0178
2025-10-18 10:39:03,166: t15.2025.03.16 val PER: 1.4202
2025-10-18 10:39:03,167: t15.2025.03.30 val PER: 1.0805
2025-10-18 10:39:03,167: t15.2025.04.13 val PER: 1.3024
2025-10-18 10:39:03,168: New best test PER inf --> 1.2154
2025-10-18 10:39:03,169: Checkpointing model
2025-10-18 10:39:03,949: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 10:42:25,563: Train batch 200: loss: 93.76 grad norm: 18.19 time: 0.903
2025-10-18 10:45:43,904: Train batch 400: loss: 74.97 grad norm: 50.62 time: 1.072
2025-10-18 10:48:59,948: Train batch 600: loss: 58.92 grad norm: 43.93 time: 0.705
2025-10-18 10:52:17,997: Train batch 800: loss: 51.95 grad norm: 45.32 time: 1.050
2025-10-18 10:55:38,151: Train batch 1000: loss: 45.50 grad norm: 48.73 time: 0.908
2025-10-18 10:58:53,145: Train batch 1200: loss: 28.15 grad norm: 31.10 time: 1.107
2025-10-18 11:02:10,438: Train batch 1400: loss: 16.88 grad norm: 29.33 time: 0.887
2025-10-18 11:05:27,481: Train batch 1600: loss: 28.82 grad norm: 40.23 time: 0.942
2025-10-18 11:08:46,091: Train batch 1800: loss: 26.20 grad norm: 51.03 time: 0.721
2025-10-18 11:12:05,687: Train batch 2000: loss: 12.81 grad norm: 33.08 time: 0.905
2025-10-18 11:12:05,689: Running test after training batch: 2000
2025-10-18 11:12:45,716: Val batch 2000: PER (avg): 0.2252 CTC Loss (avg): 22.8470 time: 40.026
2025-10-18 11:12:45,717: t15.2023.08.13 val PER: 0.1861
2025-10-18 11:12:45,718: t15.2023.08.18 val PER: 0.1844
2025-10-18 11:12:45,719: t15.2023.08.20 val PER: 0.1620
2025-10-18 11:12:45,720: t15.2023.08.25 val PER: 0.1717
2025-10-18 11:12:45,721: t15.2023.08.27 val PER: 0.2765
2025-10-18 11:12:45,722: t15.2023.09.01 val PER: 0.1502
2025-10-18 11:12:45,722: t15.2023.09.03 val PER: 0.2197
2025-10-18 11:12:45,723: t15.2023.09.24 val PER: 0.1711
2025-10-18 11:12:45,723: t15.2023.09.29 val PER: 0.1927
2025-10-18 11:12:45,724: t15.2023.10.01 val PER: 0.2398
2025-10-18 11:12:45,725: t15.2023.10.06 val PER: 0.1679
2025-10-18 11:12:45,725: t15.2023.10.08 val PER: 0.3221
2025-10-18 11:12:45,726: t15.2023.10.13 val PER: 0.2832
2025-10-18 11:12:45,728: t15.2023.10.15 val PER: 0.2274
2025-10-18 11:12:45,728: t15.2023.10.20 val PER: 0.2416
2025-10-18 11:12:45,729: t15.2023.10.22 val PER: 0.1938
2025-10-18 11:12:45,730: t15.2023.11.03 val PER: 0.2307
2025-10-18 11:12:45,730: t15.2023.11.04 val PER: 0.0410
2025-10-18 11:12:45,731: t15.2023.11.17 val PER: 0.1135
2025-10-18 11:12:45,731: t15.2023.11.19 val PER: 0.1018
2025-10-18 11:12:45,733: t15.2023.11.26 val PER: 0.2377
2025-10-18 11:12:45,734: t15.2023.12.03 val PER: 0.2059
2025-10-18 11:12:45,735: t15.2023.12.08 val PER: 0.1997
2025-10-18 11:12:45,735: t15.2023.12.10 val PER: 0.1577
2025-10-18 11:12:45,736: t15.2023.12.17 val PER: 0.2193
2025-10-18 11:12:45,737: t15.2023.12.29 val PER: 0.2107
2025-10-18 11:12:45,738: t15.2024.02.25 val PER: 0.1728
2025-10-18 11:12:45,738: t15.2024.03.08 val PER: 0.2959
2025-10-18 11:12:45,739: t15.2024.03.15 val PER: 0.2908
2025-10-18 11:12:45,741: t15.2024.03.17 val PER: 0.2204
2025-10-18 11:12:45,742: t15.2024.05.10 val PER: 0.2377
2025-10-18 11:12:45,742: t15.2024.06.14 val PER: 0.2303
2025-10-18 11:12:45,743: t15.2024.07.19 val PER: 0.3052
2025-10-18 11:12:45,743: t15.2024.07.21 val PER: 0.1621
2025-10-18 11:12:45,744: t15.2024.07.28 val PER: 0.2103
2025-10-18 11:12:45,745: t15.2025.01.10 val PER: 0.3774
2025-10-18 11:12:45,746: t15.2025.01.12 val PER: 0.2348
2025-10-18 11:12:45,746: t15.2025.03.14 val PER: 0.4053
2025-10-18 11:12:45,748: t15.2025.03.16 val PER: 0.2709
2025-10-18 11:12:45,748: t15.2025.03.30 val PER: 0.3575
2025-10-18 11:12:45,749: t15.2025.04.13 val PER: 0.2782
2025-10-18 11:12:45,749: New best test PER 1.2154 --> 0.2252
2025-10-18 11:12:45,750: Checkpointing model
2025-10-18 11:12:47,148: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 11:16:06,190: Train batch 2200: loss: 17.68 grad norm: 38.49 time: 0.976
2025-10-18 11:19:23,986: Train batch 2400: loss: 15.61 grad norm: 34.61 time: 0.944
2025-10-18 11:22:43,571: Train batch 2600: loss: 9.55 grad norm: 23.17 time: 0.719
2025-10-18 11:26:03,941: Train batch 2800: loss: 10.45 grad norm: 31.81 time: 1.158
2025-10-18 11:29:21,440: Train batch 3000: loss: 13.14 grad norm: 31.36 time: 0.921
2025-10-18 11:32:37,690: Train batch 3200: loss: 11.72 grad norm: 32.83 time: 0.773
2025-10-18 11:35:53,873: Train batch 3400: loss: 7.00 grad norm: 25.08 time: 0.924
2025-10-18 11:39:14,061: Train batch 3600: loss: 6.86 grad norm: 22.93 time: 1.017
2025-10-18 11:42:32,991: Train batch 3800: loss: 6.67 grad norm: 27.59 time: 0.844
2025-10-18 11:45:53,193: Train batch 4000: loss: 4.08 grad norm: 18.64 time: 0.873
2025-10-18 11:45:53,194: Running test after training batch: 4000
2025-10-18 11:46:35,104: Val batch 4000: PER (avg): 0.1577 CTC Loss (avg): 17.4698 time: 41.909
2025-10-18 11:46:35,105: t15.2023.08.13 val PER: 0.1175
2025-10-18 11:46:35,106: t15.2023.08.18 val PER: 0.1215
2025-10-18 11:46:35,106: t15.2023.08.20 val PER: 0.1009
2025-10-18 11:46:35,107: t15.2023.08.25 val PER: 0.1009
2025-10-18 11:46:35,108: t15.2023.08.27 val PER: 0.1833
2025-10-18 11:46:35,109: t15.2023.09.01 val PER: 0.0771
2025-10-18 11:46:35,109: t15.2023.09.03 val PER: 0.1603
2025-10-18 11:46:35,110: t15.2023.09.24 val PER: 0.1214
2025-10-18 11:46:35,111: t15.2023.09.29 val PER: 0.1436
2025-10-18 11:46:35,111: t15.2023.10.01 val PER: 0.1783
2025-10-18 11:46:35,112: t15.2023.10.06 val PER: 0.1173
2025-10-18 11:46:35,113: t15.2023.10.08 val PER: 0.2436
2025-10-18 11:46:35,113: t15.2023.10.13 val PER: 0.2188
2025-10-18 11:46:35,114: t15.2023.10.15 val PER: 0.1615
2025-10-18 11:46:35,115: t15.2023.10.20 val PER: 0.1879
2025-10-18 11:46:35,115: t15.2023.10.22 val PER: 0.1214
2025-10-18 11:46:35,118: t15.2023.11.03 val PER: 0.1818
2025-10-18 11:46:35,118: t15.2023.11.04 val PER: 0.0273
2025-10-18 11:46:35,119: t15.2023.11.17 val PER: 0.0373
2025-10-18 11:46:35,120: t15.2023.11.19 val PER: 0.0599
2025-10-18 11:46:35,120: t15.2023.11.26 val PER: 0.1406
2025-10-18 11:46:35,121: t15.2023.12.03 val PER: 0.1071
2025-10-18 11:46:35,122: t15.2023.12.08 val PER: 0.1132
2025-10-18 11:46:35,122: t15.2023.12.10 val PER: 0.1078
2025-10-18 11:46:35,123: t15.2023.12.17 val PER: 0.1507
2025-10-18 11:46:35,124: t15.2023.12.29 val PER: 0.1373
2025-10-18 11:46:35,125: t15.2024.02.25 val PER: 0.1250
2025-10-18 11:46:35,126: t15.2024.03.08 val PER: 0.2347
2025-10-18 11:46:35,126: t15.2024.03.15 val PER: 0.2045
2025-10-18 11:46:35,127: t15.2024.03.17 val PER: 0.1423
2025-10-18 11:46:35,128: t15.2024.05.10 val PER: 0.1516
2025-10-18 11:46:35,128: t15.2024.06.14 val PER: 0.1593
2025-10-18 11:46:35,130: t15.2024.07.19 val PER: 0.2340
2025-10-18 11:46:35,130: t15.2024.07.21 val PER: 0.0986
2025-10-18 11:46:35,131: t15.2024.07.28 val PER: 0.1581
2025-10-18 11:46:35,131: t15.2025.01.10 val PER: 0.3058
2025-10-18 11:46:35,133: t15.2025.01.12 val PER: 0.1647
2025-10-18 11:46:35,133: t15.2025.03.14 val PER: 0.3639
2025-10-18 11:46:35,134: t15.2025.03.16 val PER: 0.1924
2025-10-18 11:46:35,135: t15.2025.03.30 val PER: 0.2966
2025-10-18 11:46:35,136: t15.2025.04.13 val PER: 0.2068
2025-10-18 11:46:35,136: New best test PER 0.2252 --> 0.1577
2025-10-18 11:46:35,137: Checkpointing model
2025-10-18 11:46:36,580: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 11:49:54,254: Train batch 4200: loss: 5.35 grad norm: 26.12 time: 1.080
2025-10-18 11:53:17,369: Train batch 4400: loss: 7.38 grad norm: 42.27 time: 0.993
2025-10-18 11:56:36,594: Train batch 4600: loss: 4.08 grad norm: 26.55 time: 0.939
2025-10-18 11:59:58,706: Train batch 4800: loss: 7.20 grad norm: 28.44 time: 1.135
2025-10-18 12:03:19,359: Train batch 5000: loss: 4.96 grad norm: 23.47 time: 0.894
2025-10-18 12:06:38,623: Train batch 5200: loss: 7.72 grad norm: 33.87 time: 0.913
2025-10-18 12:09:56,701: Train batch 5400: loss: 2.32 grad norm: 19.99 time: 1.077
2025-10-18 12:13:12,149: Train batch 5600: loss: 4.99 grad norm: 23.93 time: 1.053
2025-10-18 12:16:31,123: Train batch 5800: loss: 3.28 grad norm: 20.84 time: 0.990
2025-10-18 12:19:52,007: Train batch 6000: loss: 4.80 grad norm: 28.62 time: 1.014
2025-10-18 12:19:52,008: Running test after training batch: 6000
2025-10-18 12:20:31,753: Val batch 6000: PER (avg): 0.1375 CTC Loss (avg): 17.0313 time: 39.744
2025-10-18 12:20:31,754: t15.2023.08.13 val PER: 0.0967
2025-10-18 12:20:31,755: t15.2023.08.18 val PER: 0.1031
2025-10-18 12:20:31,755: t15.2023.08.20 val PER: 0.0778
2025-10-18 12:20:31,756: t15.2023.08.25 val PER: 0.1024
2025-10-18 12:20:31,757: t15.2023.08.27 val PER: 0.1688
2025-10-18 12:20:31,758: t15.2023.09.01 val PER: 0.0601
2025-10-18 12:20:31,759: t15.2023.09.03 val PER: 0.1437
2025-10-18 12:20:31,760: t15.2023.09.24 val PER: 0.1068
2025-10-18 12:20:31,761: t15.2023.09.29 val PER: 0.1200
2025-10-18 12:20:31,762: t15.2023.10.01 val PER: 0.1651
2025-10-18 12:20:31,763: t15.2023.10.06 val PER: 0.1012
2025-10-18 12:20:31,764: t15.2023.10.08 val PER: 0.2287
2025-10-18 12:20:31,764: t15.2023.10.13 val PER: 0.1831
2025-10-18 12:20:31,765: t15.2023.10.15 val PER: 0.1470
2025-10-18 12:20:31,766: t15.2023.10.20 val PER: 0.1779
2025-10-18 12:20:31,766: t15.2023.10.22 val PER: 0.1069
2025-10-18 12:20:31,767: t15.2023.11.03 val PER: 0.1581
2025-10-18 12:20:31,768: t15.2023.11.04 val PER: 0.0239
2025-10-18 12:20:31,769: t15.2023.11.17 val PER: 0.0311
2025-10-18 12:20:31,770: t15.2023.11.19 val PER: 0.0279
2025-10-18 12:20:31,770: t15.2023.11.26 val PER: 0.0971
2025-10-18 12:20:31,771: t15.2023.12.03 val PER: 0.0819
2025-10-18 12:20:31,772: t15.2023.12.08 val PER: 0.0925
2025-10-18 12:20:31,773: t15.2023.12.10 val PER: 0.0920
2025-10-18 12:20:31,774: t15.2023.12.17 val PER: 0.1185
2025-10-18 12:20:31,774: t15.2023.12.29 val PER: 0.1078
2025-10-18 12:20:31,775: t15.2024.02.25 val PER: 0.0983
2025-10-18 12:20:31,776: t15.2024.03.08 val PER: 0.2134
2025-10-18 12:20:31,776: t15.2024.03.15 val PER: 0.2020
2025-10-18 12:20:31,777: t15.2024.03.17 val PER: 0.1248
2025-10-18 12:20:31,779: t15.2024.05.10 val PER: 0.1575
2025-10-18 12:20:31,779: t15.2024.06.14 val PER: 0.1372
2025-10-18 12:20:31,780: t15.2024.07.19 val PER: 0.2011
2025-10-18 12:20:31,781: t15.2024.07.21 val PER: 0.0814
2025-10-18 12:20:31,781: t15.2024.07.28 val PER: 0.1301
2025-10-18 12:20:31,782: t15.2025.01.10 val PER: 0.2645
2025-10-18 12:20:31,783: t15.2025.01.12 val PER: 0.1447
2025-10-18 12:20:31,784: t15.2025.03.14 val PER: 0.3254
2025-10-18 12:20:31,784: t15.2025.03.16 val PER: 0.1950
2025-10-18 12:20:31,785: t15.2025.03.30 val PER: 0.2598
2025-10-18 12:20:31,786: t15.2025.04.13 val PER: 0.2254
2025-10-18 12:20:31,787: New best test PER 0.1577 --> 0.1375
2025-10-18 12:20:31,787: Checkpointing model
2025-10-18 12:20:33,204: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 12:23:54,272: Train batch 6200: loss: 5.49 grad norm: 28.87 time: 1.273
2025-10-18 12:27:18,316: Train batch 6400: loss: 6.60 grad norm: 27.33 time: 1.116
2025-10-18 12:30:39,203: Train batch 6600: loss: 5.97 grad norm: 36.91 time: 1.049
2025-10-18 12:33:54,053: Train batch 6800: loss: 3.17 grad norm: 26.26 time: 0.740
2025-10-18 12:37:11,732: Train batch 7000: loss: 4.78 grad norm: 28.92 time: 0.986
2025-10-18 12:40:31,366: Train batch 7200: loss: 2.41 grad norm: 21.25 time: 1.166
2025-10-18 12:43:51,734: Train batch 7400: loss: 4.23 grad norm: 27.97 time: 1.363
2025-10-18 12:47:10,532: Train batch 7600: loss: 2.62 grad norm: 23.92 time: 1.036
2025-10-18 12:50:26,254: Train batch 7800: loss: 3.37 grad norm: 30.40 time: 1.102
2025-10-18 12:53:44,391: Train batch 8000: loss: 2.25 grad norm: 19.08 time: 0.764
2025-10-18 12:53:44,392: Running test after training batch: 8000
2025-10-18 12:54:26,291: Val batch 8000: PER (avg): 0.1310 CTC Loss (avg): 17.2807 time: 41.899
2025-10-18 12:54:26,292: t15.2023.08.13 val PER: 0.0873
2025-10-18 12:54:26,293: t15.2023.08.18 val PER: 0.0947
2025-10-18 12:54:26,294: t15.2023.08.20 val PER: 0.0715
2025-10-18 12:54:26,295: t15.2023.08.25 val PER: 0.0828
2025-10-18 12:54:26,296: t15.2023.08.27 val PER: 0.1688
2025-10-18 12:54:26,296: t15.2023.09.01 val PER: 0.0593
2025-10-18 12:54:26,297: t15.2023.09.03 val PER: 0.1485
2025-10-18 12:54:26,298: t15.2023.09.24 val PER: 0.0971
2025-10-18 12:54:26,299: t15.2023.09.29 val PER: 0.1232
2025-10-18 12:54:26,300: t15.2023.10.01 val PER: 0.1671
2025-10-18 12:54:26,300: t15.2023.10.06 val PER: 0.0904
2025-10-18 12:54:26,301: t15.2023.10.08 val PER: 0.2111
2025-10-18 12:54:26,302: t15.2023.10.13 val PER: 0.1800
2025-10-18 12:54:26,303: t15.2023.10.15 val PER: 0.1384
2025-10-18 12:54:26,303: t15.2023.10.20 val PER: 0.1812
2025-10-18 12:54:26,304: t15.2023.10.22 val PER: 0.0969
2025-10-18 12:54:26,307: t15.2023.11.03 val PER: 0.1513
2025-10-18 12:54:26,308: t15.2023.11.04 val PER: 0.0273
2025-10-18 12:54:26,309: t15.2023.11.17 val PER: 0.0404
2025-10-18 12:54:26,309: t15.2023.11.19 val PER: 0.0319
2025-10-18 12:54:26,310: t15.2023.11.26 val PER: 0.0855
2025-10-18 12:54:26,311: t15.2023.12.03 val PER: 0.0735
2025-10-18 12:54:26,312: t15.2023.12.08 val PER: 0.0739
2025-10-18 12:54:26,312: t15.2023.12.10 val PER: 0.0670
2025-10-18 12:54:26,313: t15.2023.12.17 val PER: 0.1154
2025-10-18 12:54:26,314: t15.2023.12.29 val PER: 0.1009
2025-10-18 12:54:26,315: t15.2024.02.25 val PER: 0.0969
2025-10-18 12:54:26,316: t15.2024.03.08 val PER: 0.1807
2025-10-18 12:54:26,316: t15.2024.03.15 val PER: 0.1957
2025-10-18 12:54:26,317: t15.2024.03.17 val PER: 0.1192
2025-10-18 12:54:26,318: t15.2024.05.10 val PER: 0.1471
2025-10-18 12:54:26,319: t15.2024.06.14 val PER: 0.1404
2025-10-18 12:54:26,319: t15.2024.07.19 val PER: 0.1879
2025-10-18 12:54:26,321: t15.2024.07.21 val PER: 0.0731
2025-10-18 12:54:26,322: t15.2024.07.28 val PER: 0.1110
2025-10-18 12:54:26,322: t15.2025.01.10 val PER: 0.2645
2025-10-18 12:54:26,323: t15.2025.01.12 val PER: 0.1440
2025-10-18 12:54:26,324: t15.2025.03.14 val PER: 0.3432
2025-10-18 12:54:26,324: t15.2025.03.16 val PER: 0.1846
2025-10-18 12:54:26,325: t15.2025.03.30 val PER: 0.2713
2025-10-18 12:54:26,326: t15.2025.04.13 val PER: 0.2140
2025-10-18 12:54:26,327: New best test PER 0.1375 --> 0.1310
2025-10-18 12:54:26,327: Checkpointing model
2025-10-18 12:54:27,755: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-10-18 12:57:46,827: Train batch 8200: loss: 3.11 grad norm: 31.57 time: 1.115
2025-10-18 13:01:02,725: Train batch 8400: loss: 4.32 grad norm: 25.30 time: 1.072
2025-10-18 13:04:26,449: Train batch 8600: loss: 2.08 grad norm: 18.93 time: 0.816
2025-10-18 13:07:43,057: Train batch 8800: loss: 3.24 grad norm: 27.26 time: 0.766
2025-10-18 13:11:03,190: Train batch 9000: loss: 2.53 grad norm: 26.20 time: 0.759
2025-10-18 13:14:25,566: Train batch 9200: loss: 2.16 grad norm: 18.25 time: 0.834
2025-10-18 13:17:44,437: Train batch 9400: loss: 1.61 grad norm: 16.79 time: 1.102
2025-10-18 13:21:04,421: Train batch 9600: loss: 1.91 grad norm: 22.42 time: 0.834
2025-10-18 13:24:18,109: Train batch 9800: loss: 1.36 grad norm: 13.49 time: 0.898
2025-10-18 13:27:38,169: Train batch 10000: loss: 4.35 grad norm: 34.13 time: 1.031
2025-10-18 13:27:38,170: Running test after training batch: 10000
2025-10-18 13:28:21,844: Val batch 10000: PER (avg): 0.1245 CTC Loss (avg): 18.0572 time: 43.673
2025-10-18 13:28:21,845: t15.2023.08.13 val PER: 0.0759
2025-10-18 13:28:21,846: t15.2023.08.18 val PER: 0.0964
2025-10-18 13:28:21,846: t15.2023.08.20 val PER: 0.0643
2025-10-18 13:28:21,847: t15.2023.08.25 val PER: 0.0904
2025-10-18 13:28:21,848: t15.2023.08.27 val PER: 0.1801
2025-10-18 13:28:21,849: t15.2023.09.01 val PER: 0.0625
2025-10-18 13:28:21,849: t15.2023.09.03 val PER: 0.1342
2025-10-18 13:28:21,850: t15.2023.09.24 val PER: 0.0971
2025-10-18 13:28:21,851: t15.2023.09.29 val PER: 0.1136
2025-10-18 13:28:21,851: t15.2023.10.01 val PER: 0.1546
2025-10-18 13:28:21,852: t15.2023.10.06 val PER: 0.0904
2025-10-18 13:28:21,853: t15.2023.10.08 val PER: 0.1922
2025-10-18 13:28:21,854: t15.2023.10.13 val PER: 0.1707
2025-10-18 13:28:21,854: t15.2023.10.15 val PER: 0.1332
2025-10-18 13:28:21,855: t15.2023.10.20 val PER: 0.1779
2025-10-18 13:28:21,856: t15.2023.10.22 val PER: 0.1125
2025-10-18 13:28:21,856: t15.2023.11.03 val PER: 0.1682
2025-10-18 13:28:21,857: t15.2023.11.04 val PER: 0.0273
2025-10-18 13:28:21,858: t15.2023.11.17 val PER: 0.0249
2025-10-18 13:28:21,858: t15.2023.11.19 val PER: 0.0279
2025-10-18 13:28:21,859: t15.2023.11.26 val PER: 0.0768
2025-10-18 13:28:21,861: t15.2023.12.03 val PER: 0.0683
2025-10-18 13:28:21,861: t15.2023.12.08 val PER: 0.0619
2025-10-18 13:28:21,862: t15.2023.12.10 val PER: 0.0539
2025-10-18 13:28:21,862: t15.2023.12.17 val PER: 0.1102
2025-10-18 13:28:21,863: t15.2023.12.29 val PER: 0.0933
2025-10-18 13:28:21,864: t15.2024.02.25 val PER: 0.0927
2025-10-18 13:28:21,864: t15.2024.03.08 val PER: 0.2034
2025-10-18 13:28:21,865: t15.2024.03.15 val PER: 0.1757
2025-10-18 13:28:21,866: t15.2024.03.17 val PER: 0.1123
2025-10-18 13:28:21,867: t15.2024.05.10 val PER: 0.1189
2025-10-18 13:28:21,868: t15.2024.06.14 val PER: 0.1278
2025-10-18 13:28:21,868: t15.2024.07.19 val PER: 0.1846
2025-10-18 13:28:21,869: t15.2024.07.21 val PER: 0.0731
2025-10-18 13:28:21,870: t15.2024.07.28 val PER: 0.0941
2025-10-18 13:28:21,870: t15.2025.01.10 val PER: 0.2590
2025-10-18 13:28:21,871: t15.2025.01.12 val PER: 0.1263
2025-10-18 13:28:21,873: t15.2025.03.14 val PER: 0.3210
2025-10-18 13:28:21,873: t15.2025.03.16 val PER: 0.1872
2025-10-18 13:28:21,874: t15.2025.03.30 val PER: 0.2483
2025-10-18 13:28:21,874: t15.2025.04.13 val PER: 0.2011
2025-10-18 13:28:21,875: New best test PER 0.1310 --> 0.1245
2025-10-18 13:28:21,876: Checkpointing model
2025-10-18 13:28:23,290: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
