{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12491966,"sourceType":"datasetVersion","datasetId":7875034},{"sourceId":13420488,"sourceType":"datasetVersion","datasetId":8517787},{"sourceId":13423035,"sourceType":"datasetVersion","datasetId":8519528}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset \nfrom torch import nn\nfrom scipy.ndimage import gaussian_filter1d\nimport torch.nn.functional as F\nimport h5py\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence\nimport math \nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import LambdaLR\nimport random\nimport time\nimport pathlib\nimport logging\nimport sys\nimport json\nimport pickle\nfrom omegaconf import OmegaConf\nimport torchaudio.functional as F1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:35:47.497911Z","iopub.execute_input":"2025-10-18T10:35:47.498119Z","iopub.status.idle":"2025-10-18T10:35:52.568222Z","shell.execute_reply.started":"2025-10-18T10:35:47.498100Z","shell.execute_reply":"2025-10-18T10:35:52.567400Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"LOGIT_TO_PHONEME = [\n    'BLANK',\n    'AA', 'AE', 'AH', 'AO', 'AW',\n    'AY', 'B',  'CH', 'D', 'DH',\n    'EH', 'ER', 'EY', 'F', 'G',\n    'HH', 'IH', 'IY', 'JH', 'K',\n    'L', 'M', 'N', 'NG', 'OW',\n    'OY', 'P', 'R', 'S', 'SH',\n    'T', 'TH', 'UH', 'UW', 'V',\n    'W', 'Y', 'Z', 'ZH',\n    ' | ',\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:35:55.665651Z","iopub.execute_input":"2025-10-18T10:35:55.666200Z","iopub.status.idle":"2025-10-18T10:35:55.670291Z","shell.execute_reply.started":"2025-10-18T10:35:55.666175Z","shell.execute_reply":"2025-10-18T10:35:55.669564Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# LOADING DATASET\ndef load_h5py_file(file_path, b2txt_csv_df):\n    data = {\n        'neural_features': [],\n        'n_time_steps': [],\n        'seq_class_ids': [],\n        'seq_len': [],\n        'transcriptions': [],\n        'sentence_label': [],\n        'session': [],\n        'block_num': [],\n        'trial_num': [],\n        'corpus': [],\n    }\n    # Open the hdf5 file for that day\n    with h5py.File(file_path, 'r') as f:\n\n        keys = list(f.keys())\n\n        # For each trial in the selected trials in that day\n        for key in keys:\n            g = f[key]\n\n            neural_features = g['input_features'][:]\n            n_time_steps = g.attrs['n_time_steps']\n            seq_class_ids = g['seq_class_ids'][:] if 'seq_class_ids' in g else None\n            seq_len = g.attrs['seq_len'] if 'seq_len' in g.attrs else None\n            transcription = g['transcription'][:] if 'transcription' in g else None\n            sentence_label = g.attrs['sentence_label'][:] if 'sentence_label' in g.attrs else None\n            session = g.attrs['session']\n            block_num = g.attrs['block_num']\n            trial_num = g.attrs['trial_num']\n\n            # match this trial up with the csv to get the corpus name\n            year, month, day = session.split('.')[1:]\n            date = f'{year}-{month}-{day}'\n            row = b2txt_csv_df[(b2txt_csv_df['Date'] == date) & (b2txt_csv_df['Block number'] == block_num)]\n            corpus_name = row['Corpus'].values[0]\n\n            data['neural_features'].append(neural_features)\n            data['n_time_steps'].append(n_time_steps)\n            data['seq_class_ids'].append(seq_class_ids)\n            data['seq_len'].append(seq_len)\n            data['transcriptions'].append(transcription)\n            data['sentence_label'].append(sentence_label)\n            data['session'].append(session)\n            data['block_num'].append(block_num)\n            data['trial_num'].append(trial_num)\n            data['corpus'].append(corpus_name)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:35:59.117861Z","iopub.execute_input":"2025-10-18T10:35:59.118577Z","iopub.status.idle":"2025-10-18T10:35:59.126352Z","shell.execute_reply.started":"2025-10-18T10:35:59.118549Z","shell.execute_reply":"2025-10-18T10:35:59.125473Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# PREPARING TRAINING AND TESTING DATASET\nclass BrainToTextDataset(Dataset):\n    '''\n    Dataset for brain-to-text data\n    \n    Returns an entire batch of data instead of a single example\n    '''\n\n    def __init__(\n            self, \n            trial_indicies,\n            n_batches,\n            split = 'train', \n            batch_size = 64, \n            days_per_batch = 1, \n            random_seed = -1,\n            must_include_days = None,\n            feature_subset = None\n            ): \n        '''\n        trial_indicies:  (dict)      - dictionary with day numbers as keys and lists of trial indices as values\n        n_batches:       (int)       - number of random training batches to create\n        split:           (string)    - string specifying if this is a train or test dataset\n        batch_size:      (int)       - number of examples to include in batch returned from __getitem_()\n        days_per_batch:  (int)       - how many unique days can exist in a batch; this is important for making sure that updates \n                                       to individual day layers in the GRU are not excesively noisy. Validation data will always have 1 day per batch\n        random_seed:     (int)       - seed to set for randomly assigning trials to a batch. If set to -1, trial assignment will be random\n        must_include_days ([int])    - list of days that must be included in every batch\n        feature_subset  ([int])      - list of neural feature indicies that should be the only features included in the neural data \n         '''\n        \n        # Set random seed for reproducibility\n        if random_seed != -1:\n            np.random.seed(random_seed)\n            torch.manual_seed(random_seed)\n\n        self.split = split\n\n        # Ensure the split is valid\n        if self.split not in ['train', 'test']:\n            raise ValueError(f'split must be either \"train\" or \"test\". Received {self.split}')\n        \n        self.days_per_batch = days_per_batch\n\n        self.batch_size = batch_size\n\n        self.n_batches = n_batches\n\n        self.days = {}\n        self.n_trials = 0 \n        self.trial_indicies = trial_indicies\n        self.n_days = len(trial_indicies.keys())\n\n        self.feature_subset = feature_subset\n\n        # Calculate total number of trials in the dataset\n        for d in trial_indicies:\n            self.n_trials += len(trial_indicies[d]['trials'])\n\n        if must_include_days is not None and len(must_include_days) > days_per_batch:\n            raise ValueError(f'must_include_days must be less than or equal to days_per_batch. Received {must_include_days} and days_per_batch {days_per_batch}')\n        \n        if must_include_days is not None and len(must_include_days) > self.n_days and split != 'train':\n            raise ValueError(f'must_include_days is not valid for test data. Received {must_include_days} and but only {self.n_days} in the dataset')\n        \n        if must_include_days is not None:\n            # Map must_include_days to correct indicies if they are negative\n            for i, d in enumerate(must_include_days):\n                if d < 0: \n                    must_include_days[i] = self.n_days + d\n\n        self.must_include_days = must_include_days    \n\n        # Ensure that the days_per_batch is not greater than the number of days in the dataset. Raise error\n        if self.split == 'train' and self.days_per_batch > self.n_days:\n            raise ValueError(f'Requested days_per_batch: {days_per_batch} is greater than available days {self.n_days}.')\n           \n        \n        if self.split == 'train':\n            self.batch_index = self.create_batch_index_train()\n        else: \n            self.batch_index = self.create_batch_index_test()\n            self.n_batches = len(self.batch_index.keys()) # The validation data has a fixed amount of data \n    \n    def __len__(self):\n        ''' \n        How many batches are in this dataset. \n        Because training data is sampled randomly, there is no fixed dataset length, \n        however this method is required for DataLoader to work \n        '''\n        return self.n_batches\n    \n    def __getitem__(self, idx):\n        ''' \n        Gets an entire batch of data from the dataset, not just a single item\n        '''\n        batch = {\n            'input_features' : [],\n            'seq_class_ids' : [],\n            'n_time_steps' : [],\n            'phone_seq_lens' : [],\n            'day_indicies' : [],\n            'transcriptions' : [],\n            'block_nums' : [],\n            'trial_nums' : [],\n        }\n\n        index = self.batch_index[idx]\n\n        # Iterate through each day in the index\n        for d in index.keys():\n\n            # Open the hdf5 file for that day\n            with h5py.File(self.trial_indicies[d]['session_path'], 'r') as f:\n\n                # For each trial in the selected trials in that day\n                for t in index[d]:\n                    \n                    try: \n                        g = f[f'trial_{t:04d}']\n\n                        # Remove features is neccessary \n                        input_features = torch.from_numpy(g['input_features'][:]) # neural data\n                        if self.feature_subset:\n                            input_features = input_features[:,self.feature_subset]\n\n                        batch['input_features'].append(input_features)\n\n                        batch['seq_class_ids'].append(torch.from_numpy(g['seq_class_ids'][:]))  # phoneme labels\n                        batch['transcriptions'].append(torch.from_numpy(g['transcription'][:])) # character level transcriptions\n                        batch['n_time_steps'].append(g.attrs['n_time_steps']) # number of time steps in the trial - required since we are padding\n                        batch['phone_seq_lens'].append(g.attrs['seq_len']) # number of phonemes in the label - required since we are padding\n                        batch['day_indicies'].append(int(d)) # day index of each trial - required for the day specific layers \n                        batch['block_nums'].append(g.attrs['block_num'])\n                        batch['trial_nums'].append(g.attrs['trial_num'])\n                    \n                    except Exception as e:\n                        print(f'Error loading trial {t} from session {self.trial_indicies[d][\"session_path\"]}: {e}')\n                        continue\n\n        # Pad data to form a cohesive batch\n        batch['input_features'] = pad_sequence(batch['input_features'], batch_first = True, padding_value = 0)\n        batch['seq_class_ids'] = pad_sequence(batch['seq_class_ids'], batch_first = True, padding_value = 0)\n\n        batch['n_time_steps'] = torch.tensor(batch['n_time_steps']) \n        batch['phone_seq_lens'] = torch.tensor(batch['phone_seq_lens'])\n        batch['day_indicies'] = torch.tensor(batch['day_indicies'])\n        batch['transcriptions'] = torch.stack(batch['transcriptions'])\n        batch['block_nums'] = torch.tensor(batch['block_nums'])\n        batch['trial_nums'] = torch.tensor(batch['trial_nums'])\n\n        return batch\n    \n\n    def create_batch_index_train(self):\n        '''\n        Create an index that maps a batch_number to batch_size number of trials\n\n        Each batch will have days_per_batch unique days of data, with the number of trials for each day evenly split between the days \n        (or as even as possible if batch_size is not divisible by days_per_batch)\n        '''\n\n        batch_index = {}\n\n        # Precompute the days that are not in must_include_days\n        if self.must_include_days is not None:\n            non_must_include_days = [d for d in self.trial_indicies.keys() if d not in self.must_include_days]\n\n        for batch_idx in range(self.n_batches):\n            batch = {}\n\n            # Which days will be used for this batch. Picked randomly without replacement\n            # TODO: In the future we may want to consider sampling days in proportion to the number of trials in each day \n\n            # If must_include_days is not empty, we will use those days and then randomly sample the rest\n            if self.must_include_days is not None and len(self.must_include_days) > 0:\n\n                days = np.concatenate((self.must_include_days, np.random.choice(non_must_include_days, size = self.days_per_batch - len(self.must_include_days), replace = False)))\n            \n            # Otherwise we will select random days without replacement\n            else: \n                days = np.random.choice(list(self.trial_indicies.keys()), size = self.days_per_batch, replace = False)\n            \n            # How many trials will be sampled from each day\n            num_trials = math.ceil(self.batch_size / self.days_per_batch) # Use ceiling to make sure we get at least batch_size trials\n\n            for d in days:\n\n                # Trials are sampled with replacement, so if a day has less than (self.batch_size / days_per_batch trials) trials, it won't be a problem\n                trial_idxs = np.random.choice(self.trial_indicies[d]['trials'], size = num_trials, replace = True)\n                batch[d] = trial_idxs\n\n            # Remove extra trials\n            extra_trials = (num_trials * len(days)) - self.batch_size\n\n            # While we still have extra trials, remove the last trial from a random day\n            while extra_trials > 0: \n                d = np.random.choice(days)\n                batch[d] = batch[d][:-1]\n                extra_trials -= 1\n\n            batch_index[batch_idx] = batch\n\n        return batch_index\n    \n    def create_batch_index_test(self):\n        '''\n        Create an index that is all validation/testing data in batches of up to self.batch_size\n\n        If a day does not have at least self.batch_size trials, then the batch size will be less than self.batch_size\n\n        This index will ensures that every trial in the validation set is seen once and only once\n        '''\n        batch_index = {}\n        batch_idx = 0\n        \n        for d in self.trial_indicies.keys():\n\n            # Calculate how many batches we need for this day\n            num_trials = len(self.trial_indicies[d]['trials'])\n            num_batches = (num_trials + self.batch_size - 1) // self.batch_size \n            \n            # Create batches for this day\n            for i in range(num_batches):\n                start_idx = i * self.batch_size\n                end_idx = min((i + 1) * self.batch_size, num_trials)\n                \n                # Get the trial indices for this batch\n                batch_trials = self.trial_indicies[d]['trials'][start_idx:end_idx]\n                \n                # Add to batch_index\n                batch_index[batch_idx] = {d : batch_trials}\n                batch_idx += 1\n        \n        return batch_index\n        \ndef train_test_split_indicies(file_paths, test_percentage = 0.1, seed = -1, bad_trials_dict = None):\n    '''\n    Split data from file_paths into train and test splits \n    Returns two dictionaries that detail which trials in each day will be a part of that split:\n    Example: \n        {\n            0: trials[1,2,3], session_path: 'path'\n            1: trials[2,5,6], session_path: 'path'\n        }\n\n    Args:\n        file_paths (list): List of file paths to the hdf5 files containing the data\n        test_percentage (float): Percentage of trials to use for testing. 0 will use all trials for training, 1 will use all trials for testing\n        seed (int): Seed for reproducibility. If set to -1, the split will be random\n        bad_trials_dict (dict): Dictionary of trials to exclude from the dataset. Formatted as:\n            {\n                'session_name_1': {block_num_1: [trial_nums], block_num_2: [trial_nums], ...},\n                'session_name_2': {block_num_1: [trial_nums], block_num_2: [trial_nums], ...},\n                ...\n            }\n    '''\n    # Set seed for reporoducibility\n    if seed != -1:\n        np.random.seed(seed)\n\n    # Get trials in each day\n    trials_per_day = {}\n    for i, path in enumerate(file_paths):\n        session = [s for s in path.split('/') if (s.startswith('t15.20') or s.startswith('t12.20'))][0]\n\n        good_trial_indices = []\n\n        if os.path.exists(path):\n            with h5py.File(path, 'r') as f:\n                num_trials = len(list(f.keys()))\n                for t in range(num_trials):\n                    key = f'trial_{t:04d}'\n                    \n                    block_num = f[key].attrs['block_num']\n                    trial_num = f[key].attrs['trial_num']\n\n                    if (\n                        bad_trials_dict is not None\n                        and session in bad_trials_dict\n                        and str(block_num) in bad_trials_dict[session]\n                        and trial_num in bad_trials_dict[session][str(block_num)]\n                    ):\n                        # print(f'Bad trial: {session}_{block_num}_{trial_num}')\n                        continue\n\n                    good_trial_indices.append(t)\n\n        trials_per_day[i] = {'num_trials': len(good_trial_indices), 'trial_indices': good_trial_indices, 'session_path': path}\n\n    # Pick test_percentage of trials from each day for testing and (1 - test_percentage) for training\n    train_trials = {}\n    test_trials = {}\n\n    for day in trials_per_day.keys():\n\n        num_trials = trials_per_day[day]['num_trials']\n\n        # Generate all trial indices for this day (assuming 0-indexed)\n        all_trial_indices = trials_per_day[day]['trial_indices']\n\n        # If test_percentage is 0 or 1, we can just assign all trials to either train or test\n        if test_percentage == 0:\n            train_trials[day] = {'trials' : all_trial_indices, 'session_path' : trials_per_day[day]['session_path']}\n            test_trials[day] = {'trials' : [], 'session_path' : trials_per_day[day]['session_path']}\n            continue\n        \n        elif test_percentage == 1:\n            train_trials[day] = {'trials' : [], 'session_path' : trials_per_day[day]['session_path']}\n            test_trials[day] = {'trials' : all_trial_indices, 'session_path' : trials_per_day[day]['session_path']}\n            continue    \n\n        else:\n            # Calculate how many trials to use for testing\n            num_test = max(1, int(num_trials * test_percentage))\n            \n            # Randomly select indices for testing\n            test_indices = np.random.choice(all_trial_indices, size=num_test, replace=False).tolist()\n            \n            # Remaining indices go to training\n            train_indices = [idx for idx in all_trial_indices if idx not in test_indices]\n            \n            # Store the split indices\n            train_trials[day] = {'trials' : train_indices, 'session_path' : trials_per_day[day]['session_path']}\n            test_trials[day] = {'trials' : test_indices, 'session_path' : trials_per_day[day]['session_path']}\n    \n    return train_trials, test_trials","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:36:02.189229Z","iopub.execute_input":"2025-10-18T10:36:02.189679Z","iopub.status.idle":"2025-10-18T10:36:02.215555Z","shell.execute_reply.started":"2025-10-18T10:36:02.189658Z","shell.execute_reply":"2025-10-18T10:36:02.214828Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# DATA AUGMENTATION\ndef gauss_smooth(inputs, device, smooth_kernel_std=2, smooth_kernel_size=100,  padding='same'):\n    \"\"\"\n    Applies a 1D Gaussian smoothing operation with PyTorch to smooth the data along the time axis.\n    Args:\n        inputs (tensor : B x T x N): A 3D tensor with batch size B, time steps T, and number of features N.\n                                     Assumed to already be on the correct device (e.g., GPU).\n        kernelSD (float): Standard deviation of the Gaussian smoothing kernel.\n        padding (str): Padding mode, either 'same' or 'valid'.\n        device (str): Device to use for computation (e.g., 'cuda' or 'cpu').\n    Returns:\n        smoothed (tensor : B x T x N): A smoothed 3D tensor with batch size B, time steps T, and number of features N.\n    \"\"\"\n    # Get Gaussian kernel\n    inp = np.zeros(smooth_kernel_size, dtype=np.float32)\n    inp[smooth_kernel_size // 2] = 1\n    gaussKernel = gaussian_filter1d(inp, smooth_kernel_std)\n    validIdx = np.argwhere(gaussKernel > 0.01)\n    gaussKernel = gaussKernel[validIdx]\n    gaussKernel = np.squeeze(gaussKernel / np.sum(gaussKernel))\n\n    # Convert to tensor\n    gaussKernel = torch.tensor(gaussKernel, dtype=torch.float32, device=device)\n    gaussKernel = gaussKernel.view(1, 1, -1)  # [1, 1, kernel_size]\n\n    # Prepare convolution\n    B, T, C = inputs.shape\n    inputs = inputs.permute(0, 2, 1)  # [B, C, T]\n    gaussKernel = gaussKernel.repeat(C, 1, 1)  # [C, 1, kernel_size]\n\n    # Perform convolution\n    smoothed = F.conv1d(inputs, gaussKernel, padding=padding, groups=C)\n    return smoothed.permute(0, 2, 1)  # [B, T, C]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:36:09.705660Z","iopub.execute_input":"2025-10-18T10:36:09.706274Z","iopub.status.idle":"2025-10-18T10:36:09.711910Z","shell.execute_reply.started":"2025-10-18T10:36:09.706251Z","shell.execute_reply":"2025-10-18T10:36:09.711225Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# RNN MODEL\nclass GRUDecoder(nn.Module):\n    '''\n    Defines the GRU decoder\n\n    This class combines day-specific input layers, a GRU, and an output classification layer\n    '''\n    def __init__(self,\n                 neural_dim,\n                 n_units,\n                 n_days,\n                 n_classes,\n                 rnn_dropout = 0.0,\n                 input_dropout = 0.0,\n                 n_layers = 5, \n                 patch_size = 0,\n                 patch_stride = 0,\n                 ):\n        '''\n        neural_dim  (int)      - number of channels in a single timestep (e.g. 512)\n        n_units     (int)      - number of hidden units in each recurrent layer - equal to the size of the hidden state\n        n_days      (int)      - number of days in the dataset\n        n_classes   (int)      - number of classes \n        rnn_dropout    (float) - percentage of units to droupout during training\n        input_dropout (float)  - percentage of input units to dropout during training\n        n_layers    (int)      - number of recurrent layers \n        patch_size  (int)      - the number of timesteps to concat on initial input layer - a value of 0 will disable this \"input concat\" step \n        patch_stride(int)      - the number of timesteps to stride over when concatenating initial input \n        '''\n        super(GRUDecoder, self).__init__()\n        \n        self.neural_dim = neural_dim\n        self.n_units = n_units\n        self.n_classes = n_classes\n        self.n_layers = n_layers \n        self.n_days = n_days\n\n        self.rnn_dropout = rnn_dropout\n        self.input_dropout = input_dropout\n        \n        self.patch_size = patch_size\n        self.patch_stride = patch_stride\n\n        # Parameters for the day-specific input layers\n        self.day_layer_activation = nn.Softsign() # basically a shallower tanh \n\n        # Set weights for day layers to be identity matrices so the model can learn its own day-specific transformations\n        self.day_weights = nn.ParameterList(\n            [nn.Parameter(torch.eye(self.neural_dim)) for _ in range(self.n_days)]\n        )\n        self.day_biases = nn.ParameterList(\n            [nn.Parameter(torch.zeros(1, self.neural_dim)) for _ in range(self.n_days)]\n        )\n\n        self.day_layer_dropout = nn.Dropout(input_dropout)\n        \n        self.input_size = self.neural_dim\n\n        # If we are using \"strided inputs\", then the input size of the first recurrent layer will actually be in_size * patch_size\n        if self.patch_size > 0:\n            self.input_size *= self.patch_size\n\n        self.gru = nn.GRU(\n            input_size = self.input_size,\n            hidden_size = self.n_units,\n            num_layers = self.n_layers,\n            dropout = self.rnn_dropout, \n            batch_first = True, # The first dim of our input is the batch dim\n            bidirectional = False,\n        )\n\n        # Set recurrent units to have orthogonal param init and input layers to have xavier init\n        for name, param in self.gru.named_parameters():\n            if \"weight_hh\" in name:\n                nn.init.orthogonal_(param)\n            if \"weight_ih\" in name:\n                nn.init.xavier_uniform_(param)\n\n        # Prediciton head. Weight init to xavier\n        self.out = nn.Linear(self.n_units, self.n_classes)\n        nn.init.xavier_uniform_(self.out.weight)\n\n        # Learnable initial hidden states\n        self.h0 = nn.Parameter(nn.init.xavier_uniform_(torch.zeros(1, 1, self.n_units)))\n\n    def forward(self, x, day_idx, states = None, return_state = False):\n        '''\n        x        (tensor)  - batch of examples (trials) of shape: (batch_size, time_series_length, neural_dim)\n        day_idx  (tensor)  - tensor which is a list of day indexs corresponding to the day of each example in the batch x. \n        '''\n\n        # Apply day-specific layer to (hopefully) project neural data from the different days to the same latent space\n        day_weights = torch.stack([self.day_weights[i] for i in day_idx], dim=0)\n        day_biases = torch.cat([self.day_biases[i] for i in day_idx], dim=0).unsqueeze(1)\n\n        x = torch.einsum(\"btd,bdk->btk\", x, day_weights) + day_biases\n        x = self.day_layer_activation(x)\n\n        # Apply dropout to the ouput of the day specific layer\n        if self.input_dropout > 0:\n            x = self.day_layer_dropout(x)\n\n        # (Optionally) Perform input concat operation\n        if self.patch_size > 0: \n  \n            x = x.unsqueeze(1)                      # [batches, 1, timesteps, feature_dim]\n            x = x.permute(0, 3, 1, 2)               # [batches, feature_dim, 1, timesteps]\n            \n            # Extract patches using unfold (sliding window)\n            x_unfold = x.unfold(3, self.patch_size, self.patch_stride)  # [batches, feature_dim, 1, num_patches, patch_size]\n            \n            # Remove dummy height dimension and rearrange dimensions\n            x_unfold = x_unfold.squeeze(2)           # [batches, feature_dum, num_patches, patch_size]\n            x_unfold = x_unfold.permute(0, 2, 3, 1)  # [batches, num_patches, patch_size, feature_dim]\n\n            # Flatten last two dimensions (patch_size and features)\n            x = x_unfold.reshape(x.size(0), x_unfold.size(1), -1) \n        \n        # Determine initial hidden states\n        if states is None:\n            states = self.h0.expand(self.n_layers, x.shape[0], self.n_units).contiguous()\n\n        # Pass input through RNN \n        output, hidden_states = self.gru(x, states)\n\n        # Compute logits\n        logits = self.out(output)\n        \n        if return_state:\n            return logits, hidden_states\n        \n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:36:21.101595Z","iopub.execute_input":"2025-10-18T10:36:21.101866Z","iopub.status.idle":"2025-10-18T10:36:21.113834Z","shell.execute_reply.started":"2025-10-18T10:36:21.101844Z","shell.execute_reply":"2025-10-18T10:36:21.112978Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# RNN TRAINER\ntorch.set_float32_matmul_precision('high') # makes float32 matmuls faster on some GPUs\ntorch.backends.cudnn.deterministic = True # makes training more reproducible\ntorch._dynamo.config.cache_size_limit = 64\n\nclass BrainToTextDecoder_Trainer:\n    \"\"\"\n    This class will initialize and train a brain-to-text phoneme decoder\n    \n    Written by Nick Card and Zachery Fogg with reference to Stanford NPTL's decoding function\n    \"\"\"\n\n    def __init__(self, args):\n        '''\n        args : dictionary of training arguments\n        '''\n\n        # Trainer fields\n        self.args = args\n        self.logger = None \n        self.device = None\n        self.model = None\n        self.optimizer = None\n        self.learning_rate_scheduler = None\n        self.ctc_loss = None \n\n        self.best_val_PER = torch.inf # track best PER for checkpointing\n        self.best_val_loss = torch.inf # track best loss for checkpointing\n\n        self.train_dataset = None \n        self.val_dataset = None \n        self.train_loader = None \n        self.val_loader = None \n\n        self.transform_args = self.args['dataset']['data_transforms']\n\n        # Create output directory\n        if args['mode'] == 'train':\n            os.makedirs(self.args['output_dir'], exist_ok=False)\n\n        # Create checkpoint directory\n        if args['save_best_checkpoint'] or args['save_all_val_steps'] or args['save_final_model']: \n            os.makedirs(self.args['checkpoint_dir'], exist_ok=False)\n\n        # Set up logging\n        self.logger = logging.getLogger(__name__)\n        for handler in self.logger.handlers[:]:  # make a copy of the list\n            self.logger.removeHandler(handler)\n        self.logger.setLevel(logging.INFO)\n        formatter = logging.Formatter(fmt='%(asctime)s: %(message)s')        \n\n        if args['mode']=='train':\n            # During training, save logs to file in output directory\n            fh = logging.FileHandler(str(pathlib.Path(self.args['output_dir'],'training_log')))\n            fh.setFormatter(formatter)\n            self.logger.addHandler(fh)\n\n        # Always print logs to stdout\n        sh = logging.StreamHandler(sys.stdout)\n        sh.setFormatter(formatter)\n        self.logger.addHandler(sh)\n\n        # Configure device pytorch will use \n        if torch.cuda.is_available():\n            gpu_num = self.args.get('gpu_number', 0)\n            try:\n                gpu_num = int(gpu_num)\n            except ValueError:\n                self.logger.warning(f\"Invalid gpu_number value: {gpu_num}. Using 0 instead.\")\n                gpu_num = 0\n\n            max_gpu_index = torch.cuda.device_count() - 1\n            if gpu_num > max_gpu_index:\n                self.logger.warning(f\"Requested GPU {gpu_num} not available. Using GPU 0 instead.\")\n                gpu_num = 0\n\n            try:\n                self.device = torch.device(f\"cuda:{gpu_num}\")\n                test_tensor = torch.tensor([1.0]).to(self.device)\n                test_tensor = test_tensor * 2\n            except Exception as e:\n                self.logger.error(f\"Error initializing CUDA device {gpu_num}: {str(e)}\")\n                self.logger.info(\"Falling back to CPU\")\n                self.device = torch.device(\"cpu\")\n        else:\n            self.device = torch.device(\"cpu\")\n\n        self.logger.info(f'Using device: {self.device}')\n\n\n\n        # Set seed if provided \n        if self.args['seed'] != -1:\n            np.random.seed(self.args['seed'])\n            random.seed(self.args['seed'])\n            torch.manual_seed(self.args['seed'])\n\n        # Initialize the model \n        self.model = GRUDecoder(\n            neural_dim = self.args['model']['n_input_features'],\n            n_units = self.args['model']['n_units'],\n            n_days = len(self.args['dataset']['sessions']),\n            n_classes  = self.args['dataset']['n_classes'],\n            rnn_dropout = self.args['model']['rnn_dropout'], \n            input_dropout = self.args['model']['input_network']['input_layer_dropout'], \n            n_layers = self.args['model']['n_layers'],\n            patch_size = self.args['model']['patch_size'],\n            patch_stride = self.args['model']['patch_stride'],\n        )\n\n        # Call torch.compile to speed up training\n        self.logger.info(\"Using torch.compile\")\n        self.model = torch.compile(self.model)\n\n        self.logger.info(f\"Initialized RNN decoding model\")\n\n        self.logger.info(self.model)\n\n        # Log how many parameters are in the model\n        total_params = sum(p.numel() for p in self.model.parameters())\n        self.logger.info(f\"Model has {total_params:,} parameters\")\n\n        # Determine how many day-specific parameters are in the model\n        day_params = 0\n        for name, param in self.model.named_parameters():\n            if 'day' in name:\n                day_params += param.numel()\n        \n        self.logger.info(f\"Model has {day_params:,} day-specific parameters | {((day_params / total_params) * 100):.2f}% of total parameters\")\n\n        # Create datasets and dataloaders\n        train_file_paths = [os.path.join(self.args[\"dataset\"][\"dataset_dir\"],s,'data_train.hdf5') for s in self.args['dataset']['sessions']]\n        val_file_paths = [os.path.join(self.args[\"dataset\"][\"dataset_dir\"],s,'data_val.hdf5') for s in self.args['dataset']['sessions']]\n\n        # Ensure that there are no duplicate days\n        if len(set(train_file_paths)) != len(train_file_paths):\n            raise ValueError(\"There are duplicate sessions listed in the train dataset\")\n        if len(set(val_file_paths)) != len(val_file_paths):\n            raise ValueError(\"There are duplicate sessions listed in the val dataset\")\n\n        # Split trials into train and test sets\n        train_trials, _ = train_test_split_indicies(\n            file_paths = train_file_paths, \n            test_percentage = 0,\n            seed = self.args['dataset']['seed'],\n            bad_trials_dict = None,\n            )\n        _, val_trials = train_test_split_indicies(\n            file_paths = val_file_paths, \n            test_percentage = 1,\n            seed = self.args['dataset']['seed'],\n            bad_trials_dict = None,\n            )\n\n        # Save dictionaries to output directory to know which trials were train vs val \n        with open(os.path.join(self.args['output_dir'], 'train_val_trials.json'), 'w') as f: \n            json.dump({'train' : train_trials, 'val': val_trials}, f)\n\n        # Determine if a only a subset of neural features should be used\n        feature_subset = None\n        if ('feature_subset' in self.args['dataset']) and self.args['dataset']['feature_subset'] != None: \n            feature_subset = self.args['dataset']['feature_subset']\n            self.logger.info(f'Using only a subset of features: {feature_subset}')\n            \n        # train dataset and dataloader\n        self.train_dataset = BrainToTextDataset(\n            trial_indicies = train_trials,\n            split = 'train',\n            days_per_batch = self.args['dataset']['days_per_batch'],\n            n_batches = self.args['num_training_batches'],\n            batch_size = self.args['dataset']['batch_size'],\n            must_include_days = None,\n            random_seed = self.args['dataset']['seed'],\n            feature_subset = feature_subset\n            )\n        self.train_loader = DataLoader(\n            self.train_dataset,\n            batch_size = None, # Dataset.__getitem__() already returns batches\n            shuffle = self.args['dataset']['loader_shuffle'],\n            num_workers = self.args['dataset']['num_dataloader_workers'],\n            pin_memory = True \n        )\n\n        # val dataset and dataloader\n        self.val_dataset = BrainToTextDataset(\n            trial_indicies = val_trials, \n            split = 'test',\n            days_per_batch = None,\n            n_batches = None,\n            batch_size = self.args['dataset']['batch_size'],\n            must_include_days = None,\n            random_seed = self.args['dataset']['seed'],\n            feature_subset = feature_subset   \n            )\n        self.val_loader = DataLoader(\n            self.val_dataset,\n            batch_size = None, # Dataset.__getitem__() already returns batches\n            shuffle = False, \n            num_workers = 0,\n            pin_memory = True \n        )\n\n        self.logger.info(\"Successfully initialized datasets\")\n\n        # Create optimizer, learning rate scheduler, and loss\n        self.optimizer = self.create_optimizer()\n\n        if self.args['lr_scheduler_type'] == 'linear':\n            self.learning_rate_scheduler = torch.optim.lr_scheduler.LinearLR(\n                optimizer = self.optimizer,\n                start_factor = 1.0,\n                end_factor = self.args['lr_min'] / self.args['lr_max'],\n                total_iters = self.args['lr_decay_steps'],\n            )\n        elif self.args['lr_scheduler_type'] == 'cosine':\n            self.learning_rate_scheduler = self.create_cosine_lr_scheduler(self.optimizer)\n        \n        else:\n            raise ValueError(f\"Invalid learning rate scheduler type: {self.args['lr_scheduler_type']}\")\n        \n        self.ctc_loss = torch.nn.CTCLoss(blank = 0, reduction = 'none', zero_infinity = False)\n\n        # If a checkpoint is provided, then load from checkpoint\n        if self.args['init_from_checkpoint']:\n            self.load_model_checkpoint(self.args['init_checkpoint_path'])\n\n        # Set rnn and/or input layers to not trainable if specified \n        for name, param in self.model.named_parameters():\n            if not self.args['model']['rnn_trainable'] and 'gru' in name:\n                param.requires_grad = False\n\n            elif not self.args['model']['input_network']['input_trainable'] and 'day' in name:\n                param.requires_grad = False\n\n        # Send model to device \n        self.model.to(self.device)\n\n    def create_optimizer(self):\n        '''\n        Create the optimizer with special param groups \n\n        Biases and day weights should not be decayed\n\n        Day weights should have a separate learning rate\n        '''\n        bias_params = [p for name, p in self.model.named_parameters() if 'gru.bias' in name or 'out.bias' in name]\n        day_params = [p for name, p in self.model.named_parameters() if 'day_' in name]\n        other_params = [p for name, p in self.model.named_parameters() if 'day_' not in name and 'gru.bias' not in name and 'out.bias' not in name]\n\n        if len(day_params) != 0:\n            param_groups = [\n                    {'params' : bias_params, 'weight_decay' : 0, 'group_type' : 'bias'},\n                    {'params' : day_params, 'lr' : self.args['lr_max_day'], 'weight_decay' : self.args['weight_decay_day'], 'group_type' : 'day_layer'},\n                    {'params' : other_params, 'group_type' : 'other'}\n                ]\n        else: \n            param_groups = [\n                    {'params' : bias_params, 'weight_decay' : 0, 'group_type' : 'bias'},\n                    {'params' : other_params, 'group_type' : 'other'}\n                ]\n            \n        optim = torch.optim.AdamW(\n            param_groups,\n            lr = self.args['lr_max'],\n            betas = (self.args['beta0'], self.args['beta1']),\n            eps = self.args['epsilon'],\n            weight_decay = self.args['weight_decay'],\n            fused = True\n        )\n\n        return optim \n\n    def create_cosine_lr_scheduler(self, optim):\n        lr_max = self.args['lr_max']\n        lr_min = self.args['lr_min']\n        lr_decay_steps = self.args['lr_decay_steps']\n\n        lr_max_day =  self.args['lr_max_day']\n        lr_min_day = self.args['lr_min_day']\n        lr_decay_steps_day = self.args['lr_decay_steps_day']\n\n        lr_warmup_steps = self.args['lr_warmup_steps']\n        lr_warmup_steps_day = self.args['lr_warmup_steps_day']\n\n        def lr_lambda(current_step, min_lr_ratio, decay_steps, warmup_steps):\n            '''\n            Create lr lambdas for each param group that implement cosine decay\n\n            Different lr lambda decaying for day params vs rest of the model\n            '''\n            # Warmup phase\n            if current_step < warmup_steps:\n                return float(current_step) / float(max(1, warmup_steps))\n            \n            # Cosine decay phase\n            if current_step < decay_steps:\n                progress = float(current_step - warmup_steps) / float(\n                    max(1, decay_steps - warmup_steps)\n                )\n                cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n                # Scale from 1.0 to min_lr_ratio\n                return max(min_lr_ratio, min_lr_ratio + (1 - min_lr_ratio) * cosine_decay)\n            \n            # After cosine decay is complete, maintain min_lr_ratio\n            return min_lr_ratio\n\n        if len(optim.param_groups) == 3:\n            lr_lambdas = [\n                lambda step: lr_lambda(\n                    step, \n                    lr_min / lr_max, \n                    lr_decay_steps, \n                    lr_warmup_steps), # biases \n                lambda step: lr_lambda(\n                    step, \n                    lr_min_day / lr_max_day, \n                    lr_decay_steps_day,\n                    lr_warmup_steps_day, \n                    ), # day params\n                lambda step: lr_lambda(\n                    step, \n                    lr_min / lr_max, \n                    lr_decay_steps, \n                    lr_warmup_steps), # rest of model weights\n            ]\n        elif len(optim.param_groups) == 2:\n            lr_lambdas = [\n                lambda step: lr_lambda(\n                    step, \n                    lr_min / lr_max, \n                    lr_decay_steps, \n                    lr_warmup_steps), # biases \n                lambda step: lr_lambda(\n                    step, \n                    lr_min / lr_max, \n                    lr_decay_steps, \n                    lr_warmup_steps), # rest of model weights\n            ]\n        else:\n            raise ValueError(f\"Invalid number of param groups in optimizer: {len(optim.param_groups)}\")\n        \n        return LambdaLR(optim, lr_lambdas, -1)\n        \n    def load_model_checkpoint(self, load_path):\n        ''' \n        Load a training checkpoint\n        '''\n        checkpoint = torch.load(load_path, weights_only = False) # checkpoint is just a dict\n\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.learning_rate_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_val_PER = checkpoint['val_PER'] # best phoneme error rate\n        self.best_val_loss = checkpoint['val_loss'] if 'val_loss' in checkpoint.keys() else torch.inf\n\n        self.model.to(self.device)\n        \n        # Send optimizer params back to GPU\n        for state in self.optimizer.state.values():\n            for k, v in state.items():\n                if isinstance(v, torch.Tensor):\n                    state[k] = v.to(self.device)\n\n        self.logger.info(\"Loaded model from checkpoint: \" + load_path)\n\n    def save_model_checkpoint(self, save_path, PER, loss):\n        '''\n        Save a training checkpoint\n        '''\n\n        checkpoint = {\n            'model_state_dict' : self.model.state_dict(),\n            'optimizer_state_dict' : self.optimizer.state_dict(),\n            'scheduler_state_dict' : self.learning_rate_scheduler.state_dict(),\n            'val_PER' : PER,\n            'val_loss' : loss\n        }\n        \n        torch.save(checkpoint, save_path)\n        \n        self.logger.info(\"Saved model to checkpoint: \" + save_path)\n\n        # Save the args file alongside the checkpoint\n        with open(os.path.join(self.args['checkpoint_dir'], 'args.yaml'), 'w') as f:\n            OmegaConf.save(config=self.args, f=f)\n\n    def create_attention_mask(self, sequence_lengths):\n\n        max_length = torch.max(sequence_lengths).item()\n\n        batch_size = sequence_lengths.size(0)\n        \n        # Create a mask for valid key positions (columns)\n        # Shape: [batch_size, max_length]\n        key_mask = torch.arange(max_length, device=sequence_lengths.device).expand(batch_size, max_length)\n        key_mask = key_mask < sequence_lengths.unsqueeze(1)\n        \n        # Expand key_mask to [batch_size, 1, 1, max_length]\n        # This will be broadcast across all query positions\n        key_mask = key_mask.unsqueeze(1).unsqueeze(1)\n        \n        # Create the attention mask of shape [batch_size, 1, max_length, max_length]\n        # by broadcasting key_mask across all query positions\n        attention_mask = key_mask.expand(batch_size, 1, max_length, max_length)\n        \n        # Convert boolean mask to float mask:\n        # - True (valid key positions) -> 0.0 (no change to attention scores)\n        # - False (padding key positions) -> -inf (will become 0 after softmax)\n        attention_mask_float = torch.where(attention_mask, \n                                        True,\n                                        False)\n        \n        return attention_mask_float\n\n    def transform_data(self, features, n_time_steps, mode = 'train'):\n        '''\n        Apply various augmentations and smoothing to data\n        Performing augmentations is much faster on GPU than CPU\n        '''\n\n        data_shape = features.shape\n        batch_size = data_shape[0]\n        channels = data_shape[-1]\n\n        # We only apply these augmentations in training\n        if mode == 'train':\n            # add static gain noise \n            if self.transform_args['static_gain_std'] > 0:\n                warp_mat = torch.tile(torch.unsqueeze(torch.eye(channels), dim = 0), (batch_size, 1, 1))\n                warp_mat += torch.randn_like(warp_mat, device=self.device) * self.transform_args['static_gain_std']\n\n                features = torch.matmul(features, warp_mat)\n\n            # add white noise\n            if self.transform_args['white_noise_std'] > 0:\n                features += torch.randn(data_shape, device=self.device) * self.transform_args['white_noise_std']\n\n            # add constant offset noise \n            if self.transform_args['constant_offset_std'] > 0:\n                features += torch.randn((batch_size, 1, channels), device=self.device) * self.transform_args['constant_offset_std']\n\n            # add random walk noise\n            if self.transform_args['random_walk_std'] > 0:\n                features += torch.cumsum(torch.randn(data_shape, device=self.device) * self.transform_args['random_walk_std'], dim =self.transform_args['random_walk_axis'])\n\n            # randomly cutoff part of the data timecourse\n            if self.transform_args['random_cut'] > 0:\n                cut = np.random.randint(0, self.transform_args['random_cut'])\n                features = features[:, cut:, :]\n                n_time_steps = n_time_steps - cut\n\n        # Apply Gaussian smoothing to data \n        # This is done in both training and validation\n        if self.transform_args['smooth_data']:\n            features = gauss_smooth(\n                inputs = features, \n                device = self.device,\n                smooth_kernel_std = self.transform_args['smooth_kernel_std'],\n                smooth_kernel_size= self.transform_args['smooth_kernel_size'],\n                )\n            \n        \n        return features, n_time_steps\n\n    def train(self):\n        '''\n        Train the model \n        '''\n\n        # Set model to train mode (specificially to make sure dropout layers are engaged)\n        self.model.train()\n\n        # create vars to track performance\n        train_losses = []\n        val_losses = []\n        val_PERs = []\n        val_results = []\n\n        val_steps_since_improvement = 0\n\n        # training params \n        save_best_checkpoint = self.args.get('save_best_checkpoint', True)\n        early_stopping = self.args.get('early_stopping', True)\n\n        early_stopping_val_steps = self.args['early_stopping_val_steps']\n\n        train_start_time = time.time()\n\n        # train for specified number of batches\n        for i, batch in enumerate(self.train_loader):\n            \n            self.model.train()\n            self.optimizer.zero_grad()\n            \n            # Train step\n            start_time = time.time() \n\n            # Move data to device\n            features = batch['input_features'].to(self.device)\n            labels = batch['seq_class_ids'].to(self.device)\n            n_time_steps = batch['n_time_steps'].to(self.device)\n            phone_seq_lens = batch['phone_seq_lens'].to(self.device)\n            day_indicies = batch['day_indicies'].to(self.device)\n\n            # Use autocast for efficiency\n            with torch.autocast(device_type = \"cuda\", enabled = self.args['use_amp'], dtype = torch.bfloat16):\n\n                # Apply augmentations to the data\n                features, n_time_steps = self.transform_data(features, n_time_steps, 'train')\n\n                adjusted_lens = ((n_time_steps - self.args['model']['patch_size']) / self.args['model']['patch_stride'] + 1).to(torch.int32)\n\n                # Get phoneme predictions \n                logits = self.model(features, day_indicies)\n\n                # Calculate CTC Loss\n                loss = self.ctc_loss(\n                    log_probs = torch.permute(logits.log_softmax(2), [1, 0, 2]),\n                    targets = labels,\n                    input_lengths = adjusted_lens,\n                    target_lengths = phone_seq_lens\n                    )\n                    \n                loss = torch.mean(loss) # take mean loss over batches\n            \n            loss.backward()\n\n            # Clip gradient\n            if self.args['grad_norm_clip_value'] > 0: \n                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), \n                                               max_norm = self.args['grad_norm_clip_value'],\n                                               error_if_nonfinite = True,\n                                               foreach = True\n                                               )\n\n            self.optimizer.step()\n            self.learning_rate_scheduler.step()\n            \n            # Save training metrics \n            train_step_duration = time.time() - start_time\n            train_losses.append(loss.detach().item())\n\n            # Incrementally log training progress\n            if i % self.args['batches_per_train_log'] == 0:\n                self.logger.info(f'Train batch {i}: ' +\n                        f'loss: {(loss.detach().item()):.2f} ' +\n                        f'grad norm: {grad_norm:.2f} '\n                        f'time: {train_step_duration:.3f}')\n\n            # Incrementally run a test step\n            if i % self.args['batches_per_val_step'] == 0 or i == ((self.args['num_training_batches'] - 1)):\n                self.logger.info(f\"Running test after training batch: {i}\")\n                \n                # Calculate metrics on val data\n                start_time = time.time()\n                val_metrics = self.validation(loader = self.val_loader, return_logits = self.args['save_val_logits'], return_data = self.args['save_val_data'])\n                val_step_duration = time.time() - start_time\n\n\n                # Log info \n                self.logger.info(f'Val batch {i}: ' +\n                        f'PER (avg): {val_metrics[\"avg_PER\"]:.4f} ' +\n                        f'CTC Loss (avg): {val_metrics[\"avg_loss\"]:.4f} ' +\n                        f'time: {val_step_duration:.3f}')\n                \n                if self.args['log_individual_day_val_PER']:\n                    for day in val_metrics['day_PERs'].keys():\n                        self.logger.info(f\"{self.args['dataset']['sessions'][day]} val PER: {val_metrics['day_PERs'][day]['total_edit_distance'] / val_metrics['day_PERs'][day]['total_seq_length']:0.4f}\")\n\n                # Save metrics \n                val_PERs.append(val_metrics['avg_PER'])\n                val_losses.append(val_metrics['avg_loss'])\n                val_results.append(val_metrics)\n\n                # Determine if new best day. Based on if PER is lower, or in the case of a PER tie, if loss is lower\n                new_best = False\n                if val_metrics['avg_PER'] < self.best_val_PER:\n                    self.logger.info(f\"New best test PER {self.best_val_PER:.4f} --> {val_metrics['avg_PER']:.4f}\")\n                    self.best_val_PER = val_metrics['avg_PER']\n                    self.best_val_loss = val_metrics['avg_loss']\n                    new_best = True\n                elif val_metrics['avg_PER'] == self.best_val_PER and (val_metrics['avg_loss'] < self.best_val_loss): \n                    self.logger.info(f\"New best test loss {self.best_val_loss:.4f} --> {val_metrics['avg_loss']:.4f}\")\n                    self.best_val_loss = val_metrics['avg_loss']\n                    new_best = True\n\n                if new_best:\n\n                    # Checkpoint if metrics have improved \n                    if save_best_checkpoint:\n                        self.logger.info(f\"Checkpointing model\")\n                        self.save_model_checkpoint(f'{self.args[\"checkpoint_dir\"]}/best_checkpoint', self.best_val_PER, self.best_val_loss)\n\n                    # save validation metrics to pickle file\n                    if self.args['save_val_metrics']:\n                        with open(f'{self.args[\"checkpoint_dir\"]}/val_metrics.pkl', 'wb') as f:\n                            pickle.dump(val_metrics, f) \n\n                    val_steps_since_improvement = 0\n                    \n                else:\n                    val_steps_since_improvement +=1\n\n                # Optionally save this validation checkpoint, regardless of performance\n                if self.args['save_all_val_steps']:\n                    self.save_model_checkpoint(f'{self.args[\"checkpoint_dir\"]}/checkpoint_batch_{i}', val_metrics['avg_PER'])\n\n                # Early stopping \n                if early_stopping and (val_steps_since_improvement >= early_stopping_val_steps):\n                    self.logger.info(f'Overall validation PER has not improved in {early_stopping_val_steps} validation steps. Stopping training early at batch: {i}')\n                    break\n                \n        # Log final training steps \n        training_duration = time.time() - train_start_time\n\n\n        self.logger.info(f'Best avg val PER achieved: {self.best_val_PER:.5f}')\n        self.logger.info(f'Total training time: {(training_duration / 60):.2f} minutes')\n\n        # Save final model \n        if self.args['save_final_model']:\n            self.save_model_checkpoint(f'{self.args[\"checkpoint_dir\"]}/final_checkpoint_batch_{i}', val_PERs[-1])\n\n        train_stats = {}\n        train_stats['train_losses'] = train_losses\n        train_stats['val_losses'] = val_losses \n        train_stats['val_PERs'] = val_PERs\n        train_stats['val_metrics'] = val_results\n\n        return train_stats\n\n    def validation(self, loader, return_logits = False, return_data = False):\n        '''\n        Calculate metrics on the validation dataset\n        '''\n        self.model.eval()\n\n        metrics = {}\n        \n        # Record metrics\n        if return_logits: \n            metrics['logits'] = []\n            metrics['n_time_steps'] = []\n\n        if return_data: \n            metrics['input_features'] = []\n\n        metrics['decoded_seqs'] = []\n        metrics['true_seq'] = []\n        metrics['phone_seq_lens'] = []\n        metrics['transcription'] = []\n        metrics['losses'] = []\n        metrics['block_nums'] = []\n        metrics['trial_nums'] = []\n        metrics['day_indicies'] = []\n\n        total_edit_distance = 0\n        total_seq_length = 0\n\n        # Calculate PER for each specific day\n        day_per = {}\n        for d in range(len(self.args['dataset']['sessions'])):\n            if self.args['dataset']['dataset_probability_val'][d] == 1: \n                day_per[d] = {'total_edit_distance' : 0, 'total_seq_length' : 0}\n\n        for i, batch in enumerate(loader):        \n\n            features = batch['input_features'].to(self.device)\n            labels = batch['seq_class_ids'].to(self.device)\n            n_time_steps = batch['n_time_steps'].to(self.device)\n            phone_seq_lens = batch['phone_seq_lens'].to(self.device)\n            day_indicies = batch['day_indicies'].to(self.device)\n\n            # Determine if we should perform validation on this batch\n            day = day_indicies[0].item()\n            if self.args['dataset']['dataset_probability_val'][day] == 0: \n                if self.args['log_val_skip_logs']:\n                    self.logger.info(f\"Skipping validation on day {day}\")\n                continue\n            \n            with torch.no_grad():\n\n                with torch.autocast(device_type = \"cuda\", enabled = self.args['use_amp'], dtype = torch.bfloat16):\n                    features, n_time_steps = self.transform_data(features, n_time_steps, 'val')\n\n                    adjusted_lens = ((n_time_steps - self.args['model']['patch_size']) / self.args['model']['patch_stride'] + 1).to(torch.int32)\n\n                    logits = self.model(features, day_indicies)\n    \n                    loss = self.ctc_loss(\n                        torch.permute(logits.log_softmax(2), [1, 0, 2]),\n                        labels,\n                        adjusted_lens,\n                        phone_seq_lens,\n                    )\n                    loss = torch.mean(loss)\n\n                metrics['losses'].append(loss.cpu().detach().numpy())\n\n                # Calculate PER per day and also avg over entire validation set\n                batch_edit_distance = 0 \n                decoded_seqs = []\n                for iterIdx in range(logits.shape[0]):\n                    decoded_seq = torch.argmax(logits[iterIdx, 0 : adjusted_lens[iterIdx], :].clone().detach(),dim=-1)\n                    decoded_seq = torch.unique_consecutive(decoded_seq, dim=-1)\n                    decoded_seq = decoded_seq.cpu().detach().numpy()\n                    decoded_seq = np.array([i for i in decoded_seq if i != 0])\n\n                    trueSeq = np.array(\n                        labels[iterIdx][0 : phone_seq_lens[iterIdx]].cpu().detach()\n                    )\n            \n                    batch_edit_distance += F1.edit_distance(decoded_seq, trueSeq)\n\n                    decoded_seqs.append(decoded_seq)\n\n            day = batch['day_indicies'][0].item()\n                \n            day_per[day]['total_edit_distance'] += batch_edit_distance\n            day_per[day]['total_seq_length'] += torch.sum(phone_seq_lens).item()\n\n\n            total_edit_distance += batch_edit_distance\n            total_seq_length += torch.sum(phone_seq_lens)\n\n            # Record metrics\n            if return_logits: \n                metrics['logits'].append(logits.cpu().float().numpy()) # Will be in bfloat16 if AMP is enabled, so need to set back to float32\n                metrics['n_time_steps'].append(adjusted_lens.cpu().numpy())\n\n            if return_data: \n                metrics['input_features'].append(batch['input_features'].cpu().numpy()) \n\n            metrics['decoded_seqs'].append(decoded_seqs)\n            metrics['true_seq'].append(batch['seq_class_ids'].cpu().numpy())\n            metrics['phone_seq_lens'].append(batch['phone_seq_lens'].cpu().numpy())\n            metrics['transcription'].append(batch['transcriptions'].cpu().numpy())\n            metrics['losses'].append(loss.detach().item())\n            metrics['block_nums'].append(batch['block_nums'].numpy())\n            metrics['trial_nums'].append(batch['trial_nums'].numpy())\n            metrics['day_indicies'].append(batch['day_indicies'].cpu().numpy())\n\n        avg_PER = total_edit_distance / total_seq_length\n\n        metrics['day_PERs'] = day_per\n        metrics['avg_PER'] = avg_PER.item()\n        metrics['avg_loss'] = np.mean(metrics['losses'])\n\n        return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:36:25.437396Z","iopub.execute_input":"2025-10-18T10:36:25.437864Z","iopub.status.idle":"2025-10-18T10:36:28.315818Z","shell.execute_reply.started":"2025-10-18T10:36:25.437837Z","shell.execute_reply":"2025-10-18T10:36:28.315044Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"args = OmegaConf.load('/kaggle/input/baseline-25-reconstruction/rnn_args.yaml')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:36:55.620717Z","iopub.execute_input":"2025-10-18T10:36:55.621590Z","iopub.status.idle":"2025-10-18T10:36:55.652387Z","shell.execute_reply.started":"2025-10-18T10:36:55.621560Z","shell.execute_reply":"2025-10-18T10:36:55.651773Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch._dynamo\ntorch._dynamo.config.suppress_errors = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:36:57.531531Z","iopub.execute_input":"2025-10-18T10:36:57.532141Z","iopub.status.idle":"2025-10-18T10:36:57.535396Z","shell.execute_reply.started":"2025-10-18T10:36:57.532118Z","shell.execute_reply":"2025-10-18T10:36:57.534739Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"trainer = BrainToTextDecoder_Trainer(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:36:59.740186Z","iopub.execute_input":"2025-10-18T10:36:59.740775Z","iopub.status.idle":"2025-10-18T10:37:32.071594Z","shell.execute_reply.started":"2025-10-18T10:36:59.740753Z","shell.execute_reply":"2025-10-18T10:37:32.070564Z"}},"outputs":[{"name":"stdout","text":"2025-10-18 10:36:59,823: Requested GPU 1 not available. Using GPU 0 instead.\n2025-10-18 10:37:00,003: Using device: cuda:0\n2025-10-18 10:37:01,042: Using torch.compile\n2025-10-18 10:37:03,020: Initialized RNN decoding model\n2025-10-18 10:37:03,021: OptimizedModule(\n  (_orig_mod): GRUDecoder(\n    (day_layer_activation): Softsign()\n    (day_weights): ParameterList(\n        (0): Parameter containing: [torch.float32 of size 512x512]\n        (1): Parameter containing: [torch.float32 of size 512x512]\n        (2): Parameter containing: [torch.float32 of size 512x512]\n        (3): Parameter containing: [torch.float32 of size 512x512]\n        (4): Parameter containing: [torch.float32 of size 512x512]\n        (5): Parameter containing: [torch.float32 of size 512x512]\n        (6): Parameter containing: [torch.float32 of size 512x512]\n        (7): Parameter containing: [torch.float32 of size 512x512]\n        (8): Parameter containing: [torch.float32 of size 512x512]\n        (9): Parameter containing: [torch.float32 of size 512x512]\n        (10): Parameter containing: [torch.float32 of size 512x512]\n        (11): Parameter containing: [torch.float32 of size 512x512]\n        (12): Parameter containing: [torch.float32 of size 512x512]\n        (13): Parameter containing: [torch.float32 of size 512x512]\n        (14): Parameter containing: [torch.float32 of size 512x512]\n        (15): Parameter containing: [torch.float32 of size 512x512]\n        (16): Parameter containing: [torch.float32 of size 512x512]\n        (17): Parameter containing: [torch.float32 of size 512x512]\n        (18): Parameter containing: [torch.float32 of size 512x512]\n        (19): Parameter containing: [torch.float32 of size 512x512]\n        (20): Parameter containing: [torch.float32 of size 512x512]\n        (21): Parameter containing: [torch.float32 of size 512x512]\n        (22): Parameter containing: [torch.float32 of size 512x512]\n        (23): Parameter containing: [torch.float32 of size 512x512]\n        (24): Parameter containing: [torch.float32 of size 512x512]\n        (25): Parameter containing: [torch.float32 of size 512x512]\n        (26): Parameter containing: [torch.float32 of size 512x512]\n        (27): Parameter containing: [torch.float32 of size 512x512]\n        (28): Parameter containing: [torch.float32 of size 512x512]\n        (29): Parameter containing: [torch.float32 of size 512x512]\n        (30): Parameter containing: [torch.float32 of size 512x512]\n        (31): Parameter containing: [torch.float32 of size 512x512]\n        (32): Parameter containing: [torch.float32 of size 512x512]\n        (33): Parameter containing: [torch.float32 of size 512x512]\n        (34): Parameter containing: [torch.float32 of size 512x512]\n        (35): Parameter containing: [torch.float32 of size 512x512]\n        (36): Parameter containing: [torch.float32 of size 512x512]\n        (37): Parameter containing: [torch.float32 of size 512x512]\n        (38): Parameter containing: [torch.float32 of size 512x512]\n        (39): Parameter containing: [torch.float32 of size 512x512]\n        (40): Parameter containing: [torch.float32 of size 512x512]\n        (41): Parameter containing: [torch.float32 of size 512x512]\n        (42): Parameter containing: [torch.float32 of size 512x512]\n        (43): Parameter containing: [torch.float32 of size 512x512]\n        (44): Parameter containing: [torch.float32 of size 512x512]\n    )\n    (day_biases): ParameterList(\n        (0): Parameter containing: [torch.float32 of size 1x512]\n        (1): Parameter containing: [torch.float32 of size 1x512]\n        (2): Parameter containing: [torch.float32 of size 1x512]\n        (3): Parameter containing: [torch.float32 of size 1x512]\n        (4): Parameter containing: [torch.float32 of size 1x512]\n        (5): Parameter containing: [torch.float32 of size 1x512]\n        (6): Parameter containing: [torch.float32 of size 1x512]\n        (7): Parameter containing: [torch.float32 of size 1x512]\n        (8): Parameter containing: [torch.float32 of size 1x512]\n        (9): Parameter containing: [torch.float32 of size 1x512]\n        (10): Parameter containing: [torch.float32 of size 1x512]\n        (11): Parameter containing: [torch.float32 of size 1x512]\n        (12): Parameter containing: [torch.float32 of size 1x512]\n        (13): Parameter containing: [torch.float32 of size 1x512]\n        (14): Parameter containing: [torch.float32 of size 1x512]\n        (15): Parameter containing: [torch.float32 of size 1x512]\n        (16): Parameter containing: [torch.float32 of size 1x512]\n        (17): Parameter containing: [torch.float32 of size 1x512]\n        (18): Parameter containing: [torch.float32 of size 1x512]\n        (19): Parameter containing: [torch.float32 of size 1x512]\n        (20): Parameter containing: [torch.float32 of size 1x512]\n        (21): Parameter containing: [torch.float32 of size 1x512]\n        (22): Parameter containing: [torch.float32 of size 1x512]\n        (23): Parameter containing: [torch.float32 of size 1x512]\n        (24): Parameter containing: [torch.float32 of size 1x512]\n        (25): Parameter containing: [torch.float32 of size 1x512]\n        (26): Parameter containing: [torch.float32 of size 1x512]\n        (27): Parameter containing: [torch.float32 of size 1x512]\n        (28): Parameter containing: [torch.float32 of size 1x512]\n        (29): Parameter containing: [torch.float32 of size 1x512]\n        (30): Parameter containing: [torch.float32 of size 1x512]\n        (31): Parameter containing: [torch.float32 of size 1x512]\n        (32): Parameter containing: [torch.float32 of size 1x512]\n        (33): Parameter containing: [torch.float32 of size 1x512]\n        (34): Parameter containing: [torch.float32 of size 1x512]\n        (35): Parameter containing: [torch.float32 of size 1x512]\n        (36): Parameter containing: [torch.float32 of size 1x512]\n        (37): Parameter containing: [torch.float32 of size 1x512]\n        (38): Parameter containing: [torch.float32 of size 1x512]\n        (39): Parameter containing: [torch.float32 of size 1x512]\n        (40): Parameter containing: [torch.float32 of size 1x512]\n        (41): Parameter containing: [torch.float32 of size 1x512]\n        (42): Parameter containing: [torch.float32 of size 1x512]\n        (43): Parameter containing: [torch.float32 of size 1x512]\n        (44): Parameter containing: [torch.float32 of size 1x512]\n    )\n    (day_layer_dropout): Dropout(p=0.2, inplace=False)\n    (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.4)\n    (out): Linear(in_features=768, out_features=41, bias=True)\n  )\n)\n2025-10-18 10:37:03,024: Model has 44,315,177 parameters\n2025-10-18 10:37:03,025: Model has 11,819,520 day-specific parameters | 26.67% of total parameters\n2025-10-18 10:37:31,933: Successfully initialized datasets\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"metrics = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:38:02.204227Z","iopub.execute_input":"2025-10-18T10:38:02.204715Z","iopub.status.idle":"2025-10-18T21:55:37.691420Z","shell.execute_reply.started":"2025-10-18T10:38:02.204693Z","shell.execute_reply":"2025-10-18T21:55:37.690806Z"}},"outputs":[{"name":"stderr","text":"W1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] WON'T CONVERT torch_dynamo_resume_in_forward_at_93 /tmp/ipykernel_37/1313025774.py line 93 \nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] due to: \nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]              ^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return _compile(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     tracer.run()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     super().run()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     while self.step():\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]           ^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 657, in wrapper\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return handle_graph_break(self, inst, speculation.reason)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 698, in handle_graph_break\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.output.compile_subgraph(self, reason=reason)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1136, in compile_subgraph\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.compile_and_call_fx_graph(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1382, in compile_and_call_fx_graph\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn = self.call_user_compiler(gm)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1432, in call_user_compiler\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return self._call_user_compiler(gm)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1483, in _call_user_compiler\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1462, in _call_user_compiler\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 130, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_gm = compiler_fn(gm, example_inputs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2340, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1863, in compile_fx\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return aot_autograd(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/backends/common.py\", line 83, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 1155, in aot_module_simplified\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn = dispatch_and_compile()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 1131, in dispatch_and_compile\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn, _ = create_aot_dispatcher_function(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 580, in create_aot_dispatcher_function\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return _create_aot_dispatcher_function(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 830, in _create_aot_dispatcher_function\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn, fw_metadata = compiler_fn(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                                ^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 678, in aot_dispatch_autograd\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 489, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return self.compiler_fn(gm, example_inputs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1741, in fw_compiler_base\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return inner_compile(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 569, in compile_fx_inner\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/repro/after_aot.py\", line 102, in debug_wrapper\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 685, in _compile_fx_inner\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     mb_compiled_graph = fx_codegen_and_compile(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                         ^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1129, in fx_codegen_and_compile\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1044, in codegen_and_compile\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn = graph.compile_to_module().call\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/graph.py\", line 2027, in compile_to_module\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return self._compile_to_module()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/graph.py\", line 2033, in _compile_to_module\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                                                              ^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/graph.py\", line 1964, in codegen\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.scheduler = Scheduler(self.operations)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 1798, in __init__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self._init(nodes)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 1816, in _init\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 1816, in <listcomp>\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 1947, in create_scheduler_node\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return SchedulerNode(self, node)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 893, in __init__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self._compute_attrs()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 907, in _compute_attrs\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     group_fn = self.scheduler.get_backend(device).group_fn\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 3441, in get_backend\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.backends[device] = self.create_backend(device)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 3428, in create_backend\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] \nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] \nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]              ^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return _compile(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     tracer.run()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     super().run()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     while self.step():\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]           ^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 657, in wrapper\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return handle_graph_break(self, inst, speculation.reason)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 698, in handle_graph_break\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.output.compile_subgraph(self, reason=reason)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1136, in compile_subgraph\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.compile_and_call_fx_graph(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1382, in compile_and_call_fx_graph\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn = self.call_user_compiler(gm)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1432, in call_user_compiler\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return self._call_user_compiler(gm)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1483, in _call_user_compiler\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/output_graph.py\", line 1462, in _call_user_compiler\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn = compiler_fn(gm, self.example_inputs())\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 130, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_gm = compiler_fn(gm, example_inputs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2340, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return compile_fx(model_, inputs_, config_patches=self.config)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1863, in compile_fx\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return aot_autograd(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/backends/common.py\", line 83, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 1155, in aot_module_simplified\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn = dispatch_and_compile()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 1131, in dispatch_and_compile\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn, _ = create_aot_dispatcher_function(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 580, in create_aot_dispatcher_function\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return _create_aot_dispatcher_function(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 830, in _create_aot_dispatcher_function\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn, fw_metadata = compiler_fn(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                                ^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 678, in aot_dispatch_autograd\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_functorch/aot_autograd.py\", line 489, in __call__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return self.compiler_fn(gm, example_inputs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1741, in fw_compiler_base\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return inner_compile(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 569, in compile_fx_inner\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/repro/after_aot.py\", line 102, in debug_wrapper\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 685, in _compile_fx_inner\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     mb_compiled_graph = fx_codegen_and_compile(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                         ^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1129, in fx_codegen_and_compile\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py\", line 1044, in codegen_and_compile\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     compiled_fn = graph.compile_to_module().call\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/graph.py\", line 2027, in compile_to_module\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return self._compile_to_module()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/graph.py\", line 2033, in _compile_to_module\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                                                              ^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/graph.py\", line 1964, in codegen\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.scheduler = Scheduler(self.operations)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 1798, in __init__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self._init(nodes)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 1816, in _init\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 1816, in <listcomp>\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 1947, in create_scheduler_node\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     return SchedulerNode(self, node)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 893, in __init__\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self._compute_attrs()\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 907, in _compute_attrs\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     group_fn = self.scheduler.get_backend(device).group_fn\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 3441, in get_backend\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     self.backends[device] = self.create_backend(device)\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/scheduler.py\", line 3428, in create_backend\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] \nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nW1018 10:38:12.646000 37 torch/_dynamo/convert_frame.py:1233] \n","output_type":"stream"},{"name":"stdout","text":"2025-10-18 10:38:14,466: Train batch 0: loss: 753.82 grad norm: 302.81 time: 7.967\n2025-10-18 10:38:14,467: Running test after training batch: 0\n2025-10-18 10:39:03,134: Val batch 0: PER (avg): 1.2154 CTC Loss (avg): 714.9868 time: 48.666\n2025-10-18 10:39:03,135: t15.2023.08.13 val PER: 1.1247\n2025-10-18 10:39:03,136: t15.2023.08.18 val PER: 1.1459\n2025-10-18 10:39:03,137: t15.2023.08.20 val PER: 1.1541\n2025-10-18 10:39:03,138: t15.2023.08.25 val PER: 1.1611\n2025-10-18 10:39:03,138: t15.2023.08.27 val PER: 1.0659\n2025-10-18 10:39:03,139: t15.2023.09.01 val PER: 1.2248\n2025-10-18 10:39:03,141: t15.2023.09.03 val PER: 1.1176\n2025-10-18 10:39:03,142: t15.2023.09.24 val PER: 1.3337\n2025-10-18 10:39:03,143: t15.2023.09.29 val PER: 1.2406\n2025-10-18 10:39:03,143: t15.2023.10.01 val PER: 1.0542\n2025-10-18 10:39:03,144: t15.2023.10.06 val PER: 1.2465\n2025-10-18 10:39:03,145: t15.2023.10.08 val PER: 1.0406\n2025-10-18 10:39:03,146: t15.2023.10.13 val PER: 1.1621\n2025-10-18 10:39:03,146: t15.2023.10.15 val PER: 1.2037\n2025-10-18 10:39:03,147: t15.2023.10.20 val PER: 1.3557\n2025-10-18 10:39:03,148: t15.2023.10.22 val PER: 1.2996\n2025-10-18 10:39:03,149: t15.2023.11.03 val PER: 1.2972\n2025-10-18 10:39:03,150: t15.2023.11.04 val PER: 1.3891\n2025-10-18 10:39:03,151: t15.2023.11.17 val PER: 1.6454\n2025-10-18 10:39:03,152: t15.2023.11.19 val PER: 1.4351\n2025-10-18 10:39:03,152: t15.2023.11.26 val PER: 1.2529\n2025-10-18 10:39:03,154: t15.2023.12.03 val PER: 1.1838\n2025-10-18 10:39:03,154: t15.2023.12.08 val PER: 1.2557\n2025-10-18 10:39:03,155: t15.2023.12.10 val PER: 1.3219\n2025-10-18 10:39:03,156: t15.2023.12.17 val PER: 1.0696\n2025-10-18 10:39:03,157: t15.2023.12.29 val PER: 1.1640\n2025-10-18 10:39:03,157: t15.2024.02.25 val PER: 1.1320\n2025-10-18 10:39:03,158: t15.2024.03.08 val PER: 1.1550\n2025-10-18 10:39:03,159: t15.2024.03.15 val PER: 1.1138\n2025-10-18 10:39:03,160: t15.2024.03.17 val PER: 1.1722\n2025-10-18 10:39:03,160: t15.2024.05.10 val PER: 1.2065\n2025-10-18 10:39:03,161: t15.2024.06.14 val PER: 1.4132\n2025-10-18 10:39:03,162: t15.2024.07.19 val PER: 0.9881\n2025-10-18 10:39:03,163: t15.2024.07.21 val PER: 1.4193\n2025-10-18 10:39:03,163: t15.2024.07.28 val PER: 1.4654\n2025-10-18 10:39:03,164: t15.2025.01.10 val PER: 0.9518\n2025-10-18 10:39:03,165: t15.2025.01.12 val PER: 1.4126\n2025-10-18 10:39:03,165: t15.2025.03.14 val PER: 1.0178\n2025-10-18 10:39:03,166: t15.2025.03.16 val PER: 1.4202\n2025-10-18 10:39:03,167: t15.2025.03.30 val PER: 1.0805\n2025-10-18 10:39:03,167: t15.2025.04.13 val PER: 1.3024\n2025-10-18 10:39:03,168: New best test PER inf --> 1.2154\n2025-10-18 10:39:03,169: Checkpointing model\n2025-10-18 10:39:03,949: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 10:42:25,563: Train batch 200: loss: 93.76 grad norm: 18.19 time: 0.903\n2025-10-18 10:45:43,904: Train batch 400: loss: 74.97 grad norm: 50.62 time: 1.072\n2025-10-18 10:48:59,948: Train batch 600: loss: 58.92 grad norm: 43.93 time: 0.705\n2025-10-18 10:52:17,997: Train batch 800: loss: 51.95 grad norm: 45.32 time: 1.050\n2025-10-18 10:55:38,151: Train batch 1000: loss: 45.50 grad norm: 48.73 time: 0.908\n2025-10-18 10:58:53,145: Train batch 1200: loss: 28.15 grad norm: 31.10 time: 1.107\n2025-10-18 11:02:10,438: Train batch 1400: loss: 16.88 grad norm: 29.33 time: 0.887\n2025-10-18 11:05:27,481: Train batch 1600: loss: 28.82 grad norm: 40.23 time: 0.942\n2025-10-18 11:08:46,091: Train batch 1800: loss: 26.20 grad norm: 51.03 time: 0.721\n2025-10-18 11:12:05,687: Train batch 2000: loss: 12.81 grad norm: 33.08 time: 0.905\n2025-10-18 11:12:05,689: Running test after training batch: 2000\n2025-10-18 11:12:45,716: Val batch 2000: PER (avg): 0.2252 CTC Loss (avg): 22.8470 time: 40.026\n2025-10-18 11:12:45,717: t15.2023.08.13 val PER: 0.1861\n2025-10-18 11:12:45,718: t15.2023.08.18 val PER: 0.1844\n2025-10-18 11:12:45,719: t15.2023.08.20 val PER: 0.1620\n2025-10-18 11:12:45,720: t15.2023.08.25 val PER: 0.1717\n2025-10-18 11:12:45,721: t15.2023.08.27 val PER: 0.2765\n2025-10-18 11:12:45,722: t15.2023.09.01 val PER: 0.1502\n2025-10-18 11:12:45,722: t15.2023.09.03 val PER: 0.2197\n2025-10-18 11:12:45,723: t15.2023.09.24 val PER: 0.1711\n2025-10-18 11:12:45,723: t15.2023.09.29 val PER: 0.1927\n2025-10-18 11:12:45,724: t15.2023.10.01 val PER: 0.2398\n2025-10-18 11:12:45,725: t15.2023.10.06 val PER: 0.1679\n2025-10-18 11:12:45,725: t15.2023.10.08 val PER: 0.3221\n2025-10-18 11:12:45,726: t15.2023.10.13 val PER: 0.2832\n2025-10-18 11:12:45,728: t15.2023.10.15 val PER: 0.2274\n2025-10-18 11:12:45,728: t15.2023.10.20 val PER: 0.2416\n2025-10-18 11:12:45,729: t15.2023.10.22 val PER: 0.1938\n2025-10-18 11:12:45,730: t15.2023.11.03 val PER: 0.2307\n2025-10-18 11:12:45,730: t15.2023.11.04 val PER: 0.0410\n2025-10-18 11:12:45,731: t15.2023.11.17 val PER: 0.1135\n2025-10-18 11:12:45,731: t15.2023.11.19 val PER: 0.1018\n2025-10-18 11:12:45,733: t15.2023.11.26 val PER: 0.2377\n2025-10-18 11:12:45,734: t15.2023.12.03 val PER: 0.2059\n2025-10-18 11:12:45,735: t15.2023.12.08 val PER: 0.1997\n2025-10-18 11:12:45,735: t15.2023.12.10 val PER: 0.1577\n2025-10-18 11:12:45,736: t15.2023.12.17 val PER: 0.2193\n2025-10-18 11:12:45,737: t15.2023.12.29 val PER: 0.2107\n2025-10-18 11:12:45,738: t15.2024.02.25 val PER: 0.1728\n2025-10-18 11:12:45,738: t15.2024.03.08 val PER: 0.2959\n2025-10-18 11:12:45,739: t15.2024.03.15 val PER: 0.2908\n2025-10-18 11:12:45,741: t15.2024.03.17 val PER: 0.2204\n2025-10-18 11:12:45,742: t15.2024.05.10 val PER: 0.2377\n2025-10-18 11:12:45,742: t15.2024.06.14 val PER: 0.2303\n2025-10-18 11:12:45,743: t15.2024.07.19 val PER: 0.3052\n2025-10-18 11:12:45,743: t15.2024.07.21 val PER: 0.1621\n2025-10-18 11:12:45,744: t15.2024.07.28 val PER: 0.2103\n2025-10-18 11:12:45,745: t15.2025.01.10 val PER: 0.3774\n2025-10-18 11:12:45,746: t15.2025.01.12 val PER: 0.2348\n2025-10-18 11:12:45,746: t15.2025.03.14 val PER: 0.4053\n2025-10-18 11:12:45,748: t15.2025.03.16 val PER: 0.2709\n2025-10-18 11:12:45,748: t15.2025.03.30 val PER: 0.3575\n2025-10-18 11:12:45,749: t15.2025.04.13 val PER: 0.2782\n2025-10-18 11:12:45,749: New best test PER 1.2154 --> 0.2252\n2025-10-18 11:12:45,750: Checkpointing model\n2025-10-18 11:12:47,148: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 11:16:06,190: Train batch 2200: loss: 17.68 grad norm: 38.49 time: 0.976\n2025-10-18 11:19:23,986: Train batch 2400: loss: 15.61 grad norm: 34.61 time: 0.944\n2025-10-18 11:22:43,571: Train batch 2600: loss: 9.55 grad norm: 23.17 time: 0.719\n2025-10-18 11:26:03,941: Train batch 2800: loss: 10.45 grad norm: 31.81 time: 1.158\n2025-10-18 11:29:21,440: Train batch 3000: loss: 13.14 grad norm: 31.36 time: 0.921\n2025-10-18 11:32:37,690: Train batch 3200: loss: 11.72 grad norm: 32.83 time: 0.773\n2025-10-18 11:35:53,873: Train batch 3400: loss: 7.00 grad norm: 25.08 time: 0.924\n2025-10-18 11:39:14,061: Train batch 3600: loss: 6.86 grad norm: 22.93 time: 1.017\n2025-10-18 11:42:32,991: Train batch 3800: loss: 6.67 grad norm: 27.59 time: 0.844\n2025-10-18 11:45:53,193: Train batch 4000: loss: 4.08 grad norm: 18.64 time: 0.873\n2025-10-18 11:45:53,194: Running test after training batch: 4000\n2025-10-18 11:46:35,104: Val batch 4000: PER (avg): 0.1577 CTC Loss (avg): 17.4698 time: 41.909\n2025-10-18 11:46:35,105: t15.2023.08.13 val PER: 0.1175\n2025-10-18 11:46:35,106: t15.2023.08.18 val PER: 0.1215\n2025-10-18 11:46:35,106: t15.2023.08.20 val PER: 0.1009\n2025-10-18 11:46:35,107: t15.2023.08.25 val PER: 0.1009\n2025-10-18 11:46:35,108: t15.2023.08.27 val PER: 0.1833\n2025-10-18 11:46:35,109: t15.2023.09.01 val PER: 0.0771\n2025-10-18 11:46:35,109: t15.2023.09.03 val PER: 0.1603\n2025-10-18 11:46:35,110: t15.2023.09.24 val PER: 0.1214\n2025-10-18 11:46:35,111: t15.2023.09.29 val PER: 0.1436\n2025-10-18 11:46:35,111: t15.2023.10.01 val PER: 0.1783\n2025-10-18 11:46:35,112: t15.2023.10.06 val PER: 0.1173\n2025-10-18 11:46:35,113: t15.2023.10.08 val PER: 0.2436\n2025-10-18 11:46:35,113: t15.2023.10.13 val PER: 0.2188\n2025-10-18 11:46:35,114: t15.2023.10.15 val PER: 0.1615\n2025-10-18 11:46:35,115: t15.2023.10.20 val PER: 0.1879\n2025-10-18 11:46:35,115: t15.2023.10.22 val PER: 0.1214\n2025-10-18 11:46:35,118: t15.2023.11.03 val PER: 0.1818\n2025-10-18 11:46:35,118: t15.2023.11.04 val PER: 0.0273\n2025-10-18 11:46:35,119: t15.2023.11.17 val PER: 0.0373\n2025-10-18 11:46:35,120: t15.2023.11.19 val PER: 0.0599\n2025-10-18 11:46:35,120: t15.2023.11.26 val PER: 0.1406\n2025-10-18 11:46:35,121: t15.2023.12.03 val PER: 0.1071\n2025-10-18 11:46:35,122: t15.2023.12.08 val PER: 0.1132\n2025-10-18 11:46:35,122: t15.2023.12.10 val PER: 0.1078\n2025-10-18 11:46:35,123: t15.2023.12.17 val PER: 0.1507\n2025-10-18 11:46:35,124: t15.2023.12.29 val PER: 0.1373\n2025-10-18 11:46:35,125: t15.2024.02.25 val PER: 0.1250\n2025-10-18 11:46:35,126: t15.2024.03.08 val PER: 0.2347\n2025-10-18 11:46:35,126: t15.2024.03.15 val PER: 0.2045\n2025-10-18 11:46:35,127: t15.2024.03.17 val PER: 0.1423\n2025-10-18 11:46:35,128: t15.2024.05.10 val PER: 0.1516\n2025-10-18 11:46:35,128: t15.2024.06.14 val PER: 0.1593\n2025-10-18 11:46:35,130: t15.2024.07.19 val PER: 0.2340\n2025-10-18 11:46:35,130: t15.2024.07.21 val PER: 0.0986\n2025-10-18 11:46:35,131: t15.2024.07.28 val PER: 0.1581\n2025-10-18 11:46:35,131: t15.2025.01.10 val PER: 0.3058\n2025-10-18 11:46:35,133: t15.2025.01.12 val PER: 0.1647\n2025-10-18 11:46:35,133: t15.2025.03.14 val PER: 0.3639\n2025-10-18 11:46:35,134: t15.2025.03.16 val PER: 0.1924\n2025-10-18 11:46:35,135: t15.2025.03.30 val PER: 0.2966\n2025-10-18 11:46:35,136: t15.2025.04.13 val PER: 0.2068\n2025-10-18 11:46:35,136: New best test PER 0.2252 --> 0.1577\n2025-10-18 11:46:35,137: Checkpointing model\n2025-10-18 11:46:36,580: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 11:49:54,254: Train batch 4200: loss: 5.35 grad norm: 26.12 time: 1.080\n2025-10-18 11:53:17,369: Train batch 4400: loss: 7.38 grad norm: 42.27 time: 0.993\n2025-10-18 11:56:36,594: Train batch 4600: loss: 4.08 grad norm: 26.55 time: 0.939\n2025-10-18 11:59:58,706: Train batch 4800: loss: 7.20 grad norm: 28.44 time: 1.135\n2025-10-18 12:03:19,359: Train batch 5000: loss: 4.96 grad norm: 23.47 time: 0.894\n2025-10-18 12:06:38,623: Train batch 5200: loss: 7.72 grad norm: 33.87 time: 0.913\n2025-10-18 12:09:56,701: Train batch 5400: loss: 2.32 grad norm: 19.99 time: 1.077\n2025-10-18 12:13:12,149: Train batch 5600: loss: 4.99 grad norm: 23.93 time: 1.053\n2025-10-18 12:16:31,123: Train batch 5800: loss: 3.28 grad norm: 20.84 time: 0.990\n2025-10-18 12:19:52,007: Train batch 6000: loss: 4.80 grad norm: 28.62 time: 1.014\n2025-10-18 12:19:52,008: Running test after training batch: 6000\n2025-10-18 12:20:31,753: Val batch 6000: PER (avg): 0.1375 CTC Loss (avg): 17.0313 time: 39.744\n2025-10-18 12:20:31,754: t15.2023.08.13 val PER: 0.0967\n2025-10-18 12:20:31,755: t15.2023.08.18 val PER: 0.1031\n2025-10-18 12:20:31,755: t15.2023.08.20 val PER: 0.0778\n2025-10-18 12:20:31,756: t15.2023.08.25 val PER: 0.1024\n2025-10-18 12:20:31,757: t15.2023.08.27 val PER: 0.1688\n2025-10-18 12:20:31,758: t15.2023.09.01 val PER: 0.0601\n2025-10-18 12:20:31,759: t15.2023.09.03 val PER: 0.1437\n2025-10-18 12:20:31,760: t15.2023.09.24 val PER: 0.1068\n2025-10-18 12:20:31,761: t15.2023.09.29 val PER: 0.1200\n2025-10-18 12:20:31,762: t15.2023.10.01 val PER: 0.1651\n2025-10-18 12:20:31,763: t15.2023.10.06 val PER: 0.1012\n2025-10-18 12:20:31,764: t15.2023.10.08 val PER: 0.2287\n2025-10-18 12:20:31,764: t15.2023.10.13 val PER: 0.1831\n2025-10-18 12:20:31,765: t15.2023.10.15 val PER: 0.1470\n2025-10-18 12:20:31,766: t15.2023.10.20 val PER: 0.1779\n2025-10-18 12:20:31,766: t15.2023.10.22 val PER: 0.1069\n2025-10-18 12:20:31,767: t15.2023.11.03 val PER: 0.1581\n2025-10-18 12:20:31,768: t15.2023.11.04 val PER: 0.0239\n2025-10-18 12:20:31,769: t15.2023.11.17 val PER: 0.0311\n2025-10-18 12:20:31,770: t15.2023.11.19 val PER: 0.0279\n2025-10-18 12:20:31,770: t15.2023.11.26 val PER: 0.0971\n2025-10-18 12:20:31,771: t15.2023.12.03 val PER: 0.0819\n2025-10-18 12:20:31,772: t15.2023.12.08 val PER: 0.0925\n2025-10-18 12:20:31,773: t15.2023.12.10 val PER: 0.0920\n2025-10-18 12:20:31,774: t15.2023.12.17 val PER: 0.1185\n2025-10-18 12:20:31,774: t15.2023.12.29 val PER: 0.1078\n2025-10-18 12:20:31,775: t15.2024.02.25 val PER: 0.0983\n2025-10-18 12:20:31,776: t15.2024.03.08 val PER: 0.2134\n2025-10-18 12:20:31,776: t15.2024.03.15 val PER: 0.2020\n2025-10-18 12:20:31,777: t15.2024.03.17 val PER: 0.1248\n2025-10-18 12:20:31,779: t15.2024.05.10 val PER: 0.1575\n2025-10-18 12:20:31,779: t15.2024.06.14 val PER: 0.1372\n2025-10-18 12:20:31,780: t15.2024.07.19 val PER: 0.2011\n2025-10-18 12:20:31,781: t15.2024.07.21 val PER: 0.0814\n2025-10-18 12:20:31,781: t15.2024.07.28 val PER: 0.1301\n2025-10-18 12:20:31,782: t15.2025.01.10 val PER: 0.2645\n2025-10-18 12:20:31,783: t15.2025.01.12 val PER: 0.1447\n2025-10-18 12:20:31,784: t15.2025.03.14 val PER: 0.3254\n2025-10-18 12:20:31,784: t15.2025.03.16 val PER: 0.1950\n2025-10-18 12:20:31,785: t15.2025.03.30 val PER: 0.2598\n2025-10-18 12:20:31,786: t15.2025.04.13 val PER: 0.2254\n2025-10-18 12:20:31,787: New best test PER 0.1577 --> 0.1375\n2025-10-18 12:20:31,787: Checkpointing model\n2025-10-18 12:20:33,204: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 12:23:54,272: Train batch 6200: loss: 5.49 grad norm: 28.87 time: 1.273\n2025-10-18 12:27:18,316: Train batch 6400: loss: 6.60 grad norm: 27.33 time: 1.116\n2025-10-18 12:30:39,203: Train batch 6600: loss: 5.97 grad norm: 36.91 time: 1.049\n2025-10-18 12:33:54,053: Train batch 6800: loss: 3.17 grad norm: 26.26 time: 0.740\n2025-10-18 12:37:11,732: Train batch 7000: loss: 4.78 grad norm: 28.92 time: 0.986\n2025-10-18 12:40:31,366: Train batch 7200: loss: 2.41 grad norm: 21.25 time: 1.166\n2025-10-18 12:43:51,734: Train batch 7400: loss: 4.23 grad norm: 27.97 time: 1.363\n2025-10-18 12:47:10,532: Train batch 7600: loss: 2.62 grad norm: 23.92 time: 1.036\n2025-10-18 12:50:26,254: Train batch 7800: loss: 3.37 grad norm: 30.40 time: 1.102\n2025-10-18 12:53:44,391: Train batch 8000: loss: 2.25 grad norm: 19.08 time: 0.764\n2025-10-18 12:53:44,392: Running test after training batch: 8000\n2025-10-18 12:54:26,291: Val batch 8000: PER (avg): 0.1310 CTC Loss (avg): 17.2807 time: 41.899\n2025-10-18 12:54:26,292: t15.2023.08.13 val PER: 0.0873\n2025-10-18 12:54:26,293: t15.2023.08.18 val PER: 0.0947\n2025-10-18 12:54:26,294: t15.2023.08.20 val PER: 0.0715\n2025-10-18 12:54:26,295: t15.2023.08.25 val PER: 0.0828\n2025-10-18 12:54:26,296: t15.2023.08.27 val PER: 0.1688\n2025-10-18 12:54:26,296: t15.2023.09.01 val PER: 0.0593\n2025-10-18 12:54:26,297: t15.2023.09.03 val PER: 0.1485\n2025-10-18 12:54:26,298: t15.2023.09.24 val PER: 0.0971\n2025-10-18 12:54:26,299: t15.2023.09.29 val PER: 0.1232\n2025-10-18 12:54:26,300: t15.2023.10.01 val PER: 0.1671\n2025-10-18 12:54:26,300: t15.2023.10.06 val PER: 0.0904\n2025-10-18 12:54:26,301: t15.2023.10.08 val PER: 0.2111\n2025-10-18 12:54:26,302: t15.2023.10.13 val PER: 0.1800\n2025-10-18 12:54:26,303: t15.2023.10.15 val PER: 0.1384\n2025-10-18 12:54:26,303: t15.2023.10.20 val PER: 0.1812\n2025-10-18 12:54:26,304: t15.2023.10.22 val PER: 0.0969\n2025-10-18 12:54:26,307: t15.2023.11.03 val PER: 0.1513\n2025-10-18 12:54:26,308: t15.2023.11.04 val PER: 0.0273\n2025-10-18 12:54:26,309: t15.2023.11.17 val PER: 0.0404\n2025-10-18 12:54:26,309: t15.2023.11.19 val PER: 0.0319\n2025-10-18 12:54:26,310: t15.2023.11.26 val PER: 0.0855\n2025-10-18 12:54:26,311: t15.2023.12.03 val PER: 0.0735\n2025-10-18 12:54:26,312: t15.2023.12.08 val PER: 0.0739\n2025-10-18 12:54:26,312: t15.2023.12.10 val PER: 0.0670\n2025-10-18 12:54:26,313: t15.2023.12.17 val PER: 0.1154\n2025-10-18 12:54:26,314: t15.2023.12.29 val PER: 0.1009\n2025-10-18 12:54:26,315: t15.2024.02.25 val PER: 0.0969\n2025-10-18 12:54:26,316: t15.2024.03.08 val PER: 0.1807\n2025-10-18 12:54:26,316: t15.2024.03.15 val PER: 0.1957\n2025-10-18 12:54:26,317: t15.2024.03.17 val PER: 0.1192\n2025-10-18 12:54:26,318: t15.2024.05.10 val PER: 0.1471\n2025-10-18 12:54:26,319: t15.2024.06.14 val PER: 0.1404\n2025-10-18 12:54:26,319: t15.2024.07.19 val PER: 0.1879\n2025-10-18 12:54:26,321: t15.2024.07.21 val PER: 0.0731\n2025-10-18 12:54:26,322: t15.2024.07.28 val PER: 0.1110\n2025-10-18 12:54:26,322: t15.2025.01.10 val PER: 0.2645\n2025-10-18 12:54:26,323: t15.2025.01.12 val PER: 0.1440\n2025-10-18 12:54:26,324: t15.2025.03.14 val PER: 0.3432\n2025-10-18 12:54:26,324: t15.2025.03.16 val PER: 0.1846\n2025-10-18 12:54:26,325: t15.2025.03.30 val PER: 0.2713\n2025-10-18 12:54:26,326: t15.2025.04.13 val PER: 0.2140\n2025-10-18 12:54:26,327: New best test PER 0.1375 --> 0.1310\n2025-10-18 12:54:26,327: Checkpointing model\n2025-10-18 12:54:27,755: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 12:57:46,827: Train batch 8200: loss: 3.11 grad norm: 31.57 time: 1.115\n2025-10-18 13:01:02,725: Train batch 8400: loss: 4.32 grad norm: 25.30 time: 1.072\n2025-10-18 13:04:26,449: Train batch 8600: loss: 2.08 grad norm: 18.93 time: 0.816\n2025-10-18 13:07:43,057: Train batch 8800: loss: 3.24 grad norm: 27.26 time: 0.766\n2025-10-18 13:11:03,190: Train batch 9000: loss: 2.53 grad norm: 26.20 time: 0.759\n2025-10-18 13:14:25,566: Train batch 9200: loss: 2.16 grad norm: 18.25 time: 0.834\n2025-10-18 13:17:44,437: Train batch 9400: loss: 1.61 grad norm: 16.79 time: 1.102\n2025-10-18 13:21:04,421: Train batch 9600: loss: 1.91 grad norm: 22.42 time: 0.834\n2025-10-18 13:24:18,109: Train batch 9800: loss: 1.36 grad norm: 13.49 time: 0.898\n2025-10-18 13:27:38,169: Train batch 10000: loss: 4.35 grad norm: 34.13 time: 1.031\n2025-10-18 13:27:38,170: Running test after training batch: 10000\n2025-10-18 13:28:21,844: Val batch 10000: PER (avg): 0.1245 CTC Loss (avg): 18.0572 time: 43.673\n2025-10-18 13:28:21,845: t15.2023.08.13 val PER: 0.0759\n2025-10-18 13:28:21,846: t15.2023.08.18 val PER: 0.0964\n2025-10-18 13:28:21,846: t15.2023.08.20 val PER: 0.0643\n2025-10-18 13:28:21,847: t15.2023.08.25 val PER: 0.0904\n2025-10-18 13:28:21,848: t15.2023.08.27 val PER: 0.1801\n2025-10-18 13:28:21,849: t15.2023.09.01 val PER: 0.0625\n2025-10-18 13:28:21,849: t15.2023.09.03 val PER: 0.1342\n2025-10-18 13:28:21,850: t15.2023.09.24 val PER: 0.0971\n2025-10-18 13:28:21,851: t15.2023.09.29 val PER: 0.1136\n2025-10-18 13:28:21,851: t15.2023.10.01 val PER: 0.1546\n2025-10-18 13:28:21,852: t15.2023.10.06 val PER: 0.0904\n2025-10-18 13:28:21,853: t15.2023.10.08 val PER: 0.1922\n2025-10-18 13:28:21,854: t15.2023.10.13 val PER: 0.1707\n2025-10-18 13:28:21,854: t15.2023.10.15 val PER: 0.1332\n2025-10-18 13:28:21,855: t15.2023.10.20 val PER: 0.1779\n2025-10-18 13:28:21,856: t15.2023.10.22 val PER: 0.1125\n2025-10-18 13:28:21,856: t15.2023.11.03 val PER: 0.1682\n2025-10-18 13:28:21,857: t15.2023.11.04 val PER: 0.0273\n2025-10-18 13:28:21,858: t15.2023.11.17 val PER: 0.0249\n2025-10-18 13:28:21,858: t15.2023.11.19 val PER: 0.0279\n2025-10-18 13:28:21,859: t15.2023.11.26 val PER: 0.0768\n2025-10-18 13:28:21,861: t15.2023.12.03 val PER: 0.0683\n2025-10-18 13:28:21,861: t15.2023.12.08 val PER: 0.0619\n2025-10-18 13:28:21,862: t15.2023.12.10 val PER: 0.0539\n2025-10-18 13:28:21,862: t15.2023.12.17 val PER: 0.1102\n2025-10-18 13:28:21,863: t15.2023.12.29 val PER: 0.0933\n2025-10-18 13:28:21,864: t15.2024.02.25 val PER: 0.0927\n2025-10-18 13:28:21,864: t15.2024.03.08 val PER: 0.2034\n2025-10-18 13:28:21,865: t15.2024.03.15 val PER: 0.1757\n2025-10-18 13:28:21,866: t15.2024.03.17 val PER: 0.1123\n2025-10-18 13:28:21,867: t15.2024.05.10 val PER: 0.1189\n2025-10-18 13:28:21,868: t15.2024.06.14 val PER: 0.1278\n2025-10-18 13:28:21,868: t15.2024.07.19 val PER: 0.1846\n2025-10-18 13:28:21,869: t15.2024.07.21 val PER: 0.0731\n2025-10-18 13:28:21,870: t15.2024.07.28 val PER: 0.0941\n2025-10-18 13:28:21,870: t15.2025.01.10 val PER: 0.2590\n2025-10-18 13:28:21,871: t15.2025.01.12 val PER: 0.1263\n2025-10-18 13:28:21,873: t15.2025.03.14 val PER: 0.3210\n2025-10-18 13:28:21,873: t15.2025.03.16 val PER: 0.1872\n2025-10-18 13:28:21,874: t15.2025.03.30 val PER: 0.2483\n2025-10-18 13:28:21,874: t15.2025.04.13 val PER: 0.2011\n2025-10-18 13:28:21,875: New best test PER 0.1310 --> 0.1245\n2025-10-18 13:28:21,876: Checkpointing model\n2025-10-18 13:28:23,290: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 13:31:38,373: Train batch 10200: loss: 1.10 grad norm: 14.84 time: 1.035\n2025-10-18 13:34:58,996: Train batch 10400: loss: 1.71 grad norm: 30.30 time: 1.051\n2025-10-18 13:38:16,034: Train batch 10600: loss: 1.82 grad norm: 20.03 time: 0.885\n2025-10-18 13:41:33,290: Train batch 10800: loss: 1.69 grad norm: 16.83 time: 0.970\n2025-10-18 13:44:57,795: Train batch 11000: loss: 1.37 grad norm: 19.53 time: 1.161\n2025-10-18 13:48:15,410: Train batch 11200: loss: 2.94 grad norm: 27.63 time: 0.867\n2025-10-18 13:51:31,325: Train batch 11400: loss: 1.40 grad norm: 16.40 time: 0.719\n2025-10-18 13:54:50,552: Train batch 11600: loss: 1.97 grad norm: 23.93 time: 0.745\n2025-10-18 13:58:06,869: Train batch 11800: loss: 0.95 grad norm: 17.51 time: 1.117\n2025-10-18 14:01:20,637: Train batch 12000: loss: 1.12 grad norm: 28.74 time: 1.116\n2025-10-18 14:01:20,638: Running test after training batch: 12000\n2025-10-18 14:02:02,859: Val batch 12000: PER (avg): 0.1255 CTC Loss (avg): 19.2307 time: 42.220\n2025-10-18 14:02:02,860: t15.2023.08.13 val PER: 0.0863\n2025-10-18 14:02:02,861: t15.2023.08.18 val PER: 0.0863\n2025-10-18 14:02:02,861: t15.2023.08.20 val PER: 0.0715\n2025-10-18 14:02:02,862: t15.2023.08.25 val PER: 0.0979\n2025-10-18 14:02:02,863: t15.2023.08.27 val PER: 0.1817\n2025-10-18 14:02:02,864: t15.2023.09.01 val PER: 0.0584\n2025-10-18 14:02:02,865: t15.2023.09.03 val PER: 0.1235\n2025-10-18 14:02:02,866: t15.2023.09.24 val PER: 0.0983\n2025-10-18 14:02:02,867: t15.2023.09.29 val PER: 0.1321\n2025-10-18 14:02:02,868: t15.2023.10.01 val PER: 0.1559\n2025-10-18 14:02:02,868: t15.2023.10.06 val PER: 0.0904\n2025-10-18 14:02:02,869: t15.2023.10.08 val PER: 0.2003\n2025-10-18 14:02:02,870: t15.2023.10.13 val PER: 0.1746\n2025-10-18 14:02:02,871: t15.2023.10.15 val PER: 0.1397\n2025-10-18 14:02:02,871: t15.2023.10.20 val PER: 0.1678\n2025-10-18 14:02:02,872: t15.2023.10.22 val PER: 0.1058\n2025-10-18 14:02:02,873: t15.2023.11.03 val PER: 0.1459\n2025-10-18 14:02:02,874: t15.2023.11.04 val PER: 0.0239\n2025-10-18 14:02:02,875: t15.2023.11.17 val PER: 0.0202\n2025-10-18 14:02:02,876: t15.2023.11.19 val PER: 0.0200\n2025-10-18 14:02:02,877: t15.2023.11.26 val PER: 0.0710\n2025-10-18 14:02:02,877: t15.2023.12.03 val PER: 0.0788\n2025-10-18 14:02:02,878: t15.2023.12.08 val PER: 0.0593\n2025-10-18 14:02:02,879: t15.2023.12.10 val PER: 0.0670\n2025-10-18 14:02:02,879: t15.2023.12.17 val PER: 0.0988\n2025-10-18 14:02:02,880: t15.2023.12.29 val PER: 0.1002\n2025-10-18 14:02:02,882: t15.2024.02.25 val PER: 0.0857\n2025-10-18 14:02:02,882: t15.2024.03.08 val PER: 0.2077\n2025-10-18 14:02:02,883: t15.2024.03.15 val PER: 0.1907\n2025-10-18 14:02:02,884: t15.2024.03.17 val PER: 0.1088\n2025-10-18 14:02:02,885: t15.2024.05.10 val PER: 0.1426\n2025-10-18 14:02:02,886: t15.2024.06.14 val PER: 0.1372\n2025-10-18 14:02:02,887: t15.2024.07.19 val PER: 0.1721\n2025-10-18 14:02:02,887: t15.2024.07.21 val PER: 0.0786\n2025-10-18 14:02:02,888: t15.2024.07.28 val PER: 0.1132\n2025-10-18 14:02:02,889: t15.2025.01.10 val PER: 0.2713\n2025-10-18 14:02:02,890: t15.2025.01.12 val PER: 0.1216\n2025-10-18 14:02:02,891: t15.2025.03.14 val PER: 0.3003\n2025-10-18 14:02:02,892: t15.2025.03.16 val PER: 0.1806\n2025-10-18 14:02:02,893: t15.2025.03.30 val PER: 0.2437\n2025-10-18 14:02:02,894: t15.2025.04.13 val PER: 0.2011\n2025-10-18 14:05:18,041: Train batch 12200: loss: 1.78 grad norm: 47.89 time: 1.054\n2025-10-18 14:08:34,559: Train batch 12400: loss: 0.69 grad norm: 20.26 time: 1.347\n2025-10-18 14:11:53,819: Train batch 12600: loss: 1.44 grad norm: 23.74 time: 0.963\n2025-10-18 14:15:11,077: Train batch 12800: loss: 3.87 grad norm: 25.50 time: 1.166\n2025-10-18 14:18:30,379: Train batch 13000: loss: 1.67 grad norm: 21.09 time: 1.263\n2025-10-18 14:21:46,996: Train batch 13200: loss: 0.83 grad norm: 15.50 time: 0.866\n2025-10-18 14:25:05,040: Train batch 13400: loss: 0.70 grad norm: 13.94 time: 0.850\n2025-10-18 14:28:24,940: Train batch 13600: loss: 1.14 grad norm: 14.89 time: 1.151\n2025-10-18 14:31:45,843: Train batch 13800: loss: 0.77 grad norm: 13.48 time: 0.905\n2025-10-18 14:35:08,261: Train batch 14000: loss: 0.78 grad norm: 16.20 time: 0.905\n2025-10-18 14:35:08,262: Running test after training batch: 14000\n2025-10-18 14:35:51,627: Val batch 14000: PER (avg): 0.1202 CTC Loss (avg): 19.5162 time: 43.365\n2025-10-18 14:35:51,628: t15.2023.08.13 val PER: 0.0811\n2025-10-18 14:35:51,629: t15.2023.08.18 val PER: 0.0813\n2025-10-18 14:35:51,630: t15.2023.08.20 val PER: 0.0508\n2025-10-18 14:35:51,631: t15.2023.08.25 val PER: 0.0813\n2025-10-18 14:35:51,631: t15.2023.08.27 val PER: 0.1559\n2025-10-18 14:35:51,632: t15.2023.09.01 val PER: 0.0552\n2025-10-18 14:35:51,634: t15.2023.09.03 val PER: 0.1330\n2025-10-18 14:35:51,635: t15.2023.09.24 val PER: 0.0886\n2025-10-18 14:35:51,635: t15.2023.09.29 val PER: 0.1149\n2025-10-18 14:35:51,636: t15.2023.10.01 val PER: 0.1493\n2025-10-18 14:35:51,637: t15.2023.10.06 val PER: 0.0732\n2025-10-18 14:35:51,638: t15.2023.10.08 val PER: 0.1881\n2025-10-18 14:35:51,639: t15.2023.10.13 val PER: 0.1606\n2025-10-18 14:35:51,640: t15.2023.10.15 val PER: 0.1318\n2025-10-18 14:35:51,641: t15.2023.10.20 val PER: 0.1510\n2025-10-18 14:35:51,641: t15.2023.10.22 val PER: 0.1069\n2025-10-18 14:35:51,642: t15.2023.11.03 val PER: 0.1526\n2025-10-18 14:35:51,643: t15.2023.11.04 val PER: 0.0171\n2025-10-18 14:35:51,643: t15.2023.11.17 val PER: 0.0358\n2025-10-18 14:35:51,644: t15.2023.11.19 val PER: 0.0299\n2025-10-18 14:35:51,645: t15.2023.11.26 val PER: 0.0768\n2025-10-18 14:35:51,645: t15.2023.12.03 val PER: 0.0672\n2025-10-18 14:35:51,647: t15.2023.12.08 val PER: 0.0546\n2025-10-18 14:35:51,648: t15.2023.12.10 val PER: 0.0565\n2025-10-18 14:35:51,649: t15.2023.12.17 val PER: 0.1071\n2025-10-18 14:35:51,650: t15.2023.12.29 val PER: 0.0837\n2025-10-18 14:35:51,650: t15.2024.02.25 val PER: 0.0787\n2025-10-18 14:35:51,651: t15.2024.03.08 val PER: 0.2105\n2025-10-18 14:35:51,652: t15.2024.03.15 val PER: 0.1832\n2025-10-18 14:35:51,653: t15.2024.03.17 val PER: 0.1046\n2025-10-18 14:35:51,653: t15.2024.05.10 val PER: 0.1263\n2025-10-18 14:35:51,654: t15.2024.06.14 val PER: 0.1293\n2025-10-18 14:35:51,655: t15.2024.07.19 val PER: 0.1819\n2025-10-18 14:35:51,656: t15.2024.07.21 val PER: 0.0703\n2025-10-18 14:35:51,656: t15.2024.07.28 val PER: 0.1059\n2025-10-18 14:35:51,658: t15.2025.01.10 val PER: 0.2521\n2025-10-18 14:35:51,658: t15.2025.01.12 val PER: 0.1316\n2025-10-18 14:35:51,659: t15.2025.03.14 val PER: 0.3254\n2025-10-18 14:35:51,660: t15.2025.03.16 val PER: 0.1623\n2025-10-18 14:35:51,661: t15.2025.03.30 val PER: 0.2402\n2025-10-18 14:35:51,661: t15.2025.04.13 val PER: 0.1983\n2025-10-18 14:35:51,662: New best test PER 0.1245 --> 0.1202\n2025-10-18 14:35:51,662: Checkpointing model\n2025-10-18 14:35:53,092: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 14:39:08,958: Train batch 14200: loss: 0.74 grad norm: 17.02 time: 0.838\n2025-10-18 14:42:27,412: Train batch 14400: loss: 0.18 grad norm: 5.85 time: 0.842\n2025-10-18 14:45:47,435: Train batch 14600: loss: 0.93 grad norm: 20.83 time: 0.960\n2025-10-18 14:49:02,508: Train batch 14800: loss: 0.97 grad norm: 17.89 time: 0.945\n2025-10-18 14:52:20,102: Train batch 15000: loss: 0.86 grad norm: 11.16 time: 1.294\n2025-10-18 14:55:37,620: Train batch 15200: loss: 1.03 grad norm: 43.11 time: 0.968\n2025-10-18 14:58:53,853: Train batch 15400: loss: 0.66 grad norm: 20.36 time: 1.136\n2025-10-18 15:02:15,758: Train batch 15600: loss: 1.41 grad norm: 13.31 time: 0.892\n2025-10-18 15:05:33,886: Train batch 15800: loss: 0.71 grad norm: 11.13 time: 0.687\n2025-10-18 15:08:50,614: Train batch 16000: loss: 0.96 grad norm: 15.05 time: 0.933\n2025-10-18 15:08:50,616: Running test after training batch: 16000\n2025-10-18 15:09:36,738: Val batch 16000: PER (avg): 0.1181 CTC Loss (avg): 19.6140 time: 46.122\n2025-10-18 15:09:36,739: t15.2023.08.13 val PER: 0.0832\n2025-10-18 15:09:36,740: t15.2023.08.18 val PER: 0.0746\n2025-10-18 15:09:36,741: t15.2023.08.20 val PER: 0.0612\n2025-10-18 15:09:36,742: t15.2023.08.25 val PER: 0.0783\n2025-10-18 15:09:36,742: t15.2023.08.27 val PER: 0.1624\n2025-10-18 15:09:36,743: t15.2023.09.01 val PER: 0.0536\n2025-10-18 15:09:36,744: t15.2023.09.03 val PER: 0.1473\n2025-10-18 15:09:36,745: t15.2023.09.24 val PER: 0.0910\n2025-10-18 15:09:36,745: t15.2023.09.29 val PER: 0.1117\n2025-10-18 15:09:36,747: t15.2023.10.01 val PER: 0.1446\n2025-10-18 15:09:36,748: t15.2023.10.06 val PER: 0.0840\n2025-10-18 15:09:36,748: t15.2023.10.08 val PER: 0.1949\n2025-10-18 15:09:36,749: t15.2023.10.13 val PER: 0.1606\n2025-10-18 15:09:36,750: t15.2023.10.15 val PER: 0.1272\n2025-10-18 15:09:36,751: t15.2023.10.20 val PER: 0.1812\n2025-10-18 15:09:36,752: t15.2023.10.22 val PER: 0.0980\n2025-10-18 15:09:36,753: t15.2023.11.03 val PER: 0.1459\n2025-10-18 15:09:36,754: t15.2023.11.04 val PER: 0.0307\n2025-10-18 15:09:36,755: t15.2023.11.17 val PER: 0.0187\n2025-10-18 15:09:36,755: t15.2023.11.19 val PER: 0.0240\n2025-10-18 15:09:36,756: t15.2023.11.26 val PER: 0.0754\n2025-10-18 15:09:36,757: t15.2023.12.03 val PER: 0.0609\n2025-10-18 15:09:36,758: t15.2023.12.08 val PER: 0.0599\n2025-10-18 15:09:36,759: t15.2023.12.10 val PER: 0.0342\n2025-10-18 15:09:36,760: t15.2023.12.17 val PER: 0.0998\n2025-10-18 15:09:36,761: t15.2023.12.29 val PER: 0.0858\n2025-10-18 15:09:36,762: t15.2024.02.25 val PER: 0.0801\n2025-10-18 15:09:36,762: t15.2024.03.08 val PER: 0.1935\n2025-10-18 15:09:36,763: t15.2024.03.15 val PER: 0.1764\n2025-10-18 15:09:36,764: t15.2024.03.17 val PER: 0.1053\n2025-10-18 15:09:36,765: t15.2024.05.10 val PER: 0.1337\n2025-10-18 15:09:36,766: t15.2024.06.14 val PER: 0.1309\n2025-10-18 15:09:36,766: t15.2024.07.19 val PER: 0.1879\n2025-10-18 15:09:36,767: t15.2024.07.21 val PER: 0.0641\n2025-10-18 15:09:36,768: t15.2024.07.28 val PER: 0.1007\n2025-10-18 15:09:36,768: t15.2025.01.10 val PER: 0.2548\n2025-10-18 15:09:36,770: t15.2025.01.12 val PER: 0.1155\n2025-10-18 15:09:36,770: t15.2025.03.14 val PER: 0.2944\n2025-10-18 15:09:36,771: t15.2025.03.16 val PER: 0.1715\n2025-10-18 15:09:36,772: t15.2025.03.30 val PER: 0.2471\n2025-10-18 15:09:36,772: t15.2025.04.13 val PER: 0.1812\n2025-10-18 15:09:36,773: New best test PER 0.1202 --> 0.1181\n2025-10-18 15:09:36,773: Checkpointing model\n2025-10-18 15:09:38,221: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 15:12:55,517: Train batch 16200: loss: 0.60 grad norm: 12.87 time: 0.840\n2025-10-18 15:16:13,636: Train batch 16400: loss: 1.54 grad norm: 21.60 time: 0.900\n2025-10-18 15:19:34,539: Train batch 16600: loss: 0.26 grad norm: 16.00 time: 0.894\n2025-10-18 15:22:55,903: Train batch 16800: loss: 0.79 grad norm: 24.08 time: 1.204\n2025-10-18 15:26:15,918: Train batch 17000: loss: 0.66 grad norm: 12.79 time: 0.946\n2025-10-18 15:29:33,029: Train batch 17200: loss: 1.16 grad norm: 13.21 time: 0.855\n2025-10-18 15:32:52,361: Train batch 17400: loss: 0.42 grad norm: 8.79 time: 1.214\n2025-10-18 15:36:10,063: Train batch 17600: loss: 1.00 grad norm: 27.72 time: 0.846\n2025-10-18 15:39:30,647: Train batch 17800: loss: 0.95 grad norm: 21.97 time: 1.364\n2025-10-18 15:42:49,083: Train batch 18000: loss: 0.84 grad norm: 18.50 time: 1.207\n2025-10-18 15:42:49,084: Running test after training batch: 18000\n2025-10-18 15:43:29,370: Val batch 18000: PER (avg): 0.1181 CTC Loss (avg): 20.7056 time: 40.285\n2025-10-18 15:43:29,371: t15.2023.08.13 val PER: 0.0832\n2025-10-18 15:43:29,372: t15.2023.08.18 val PER: 0.0780\n2025-10-18 15:43:29,372: t15.2023.08.20 val PER: 0.0556\n2025-10-18 15:43:29,374: t15.2023.08.25 val PER: 0.0904\n2025-10-18 15:43:29,374: t15.2023.08.27 val PER: 0.1656\n2025-10-18 15:43:29,376: t15.2023.09.01 val PER: 0.0471\n2025-10-18 15:43:29,376: t15.2023.09.03 val PER: 0.1318\n2025-10-18 15:43:29,377: t15.2023.09.24 val PER: 0.0801\n2025-10-18 15:43:29,378: t15.2023.09.29 val PER: 0.1123\n2025-10-18 15:43:29,378: t15.2023.10.01 val PER: 0.1460\n2025-10-18 15:43:29,379: t15.2023.10.06 val PER: 0.0807\n2025-10-18 15:43:29,380: t15.2023.10.08 val PER: 0.1962\n2025-10-18 15:43:29,381: t15.2023.10.13 val PER: 0.1645\n2025-10-18 15:43:29,382: t15.2023.10.15 val PER: 0.1378\n2025-10-18 15:43:29,383: t15.2023.10.20 val PER: 0.1611\n2025-10-18 15:43:29,384: t15.2023.10.22 val PER: 0.1036\n2025-10-18 15:43:29,385: t15.2023.11.03 val PER: 0.1554\n2025-10-18 15:43:29,386: t15.2023.11.04 val PER: 0.0307\n2025-10-18 15:43:29,387: t15.2023.11.17 val PER: 0.0140\n2025-10-18 15:43:29,387: t15.2023.11.19 val PER: 0.0240\n2025-10-18 15:43:29,388: t15.2023.11.26 val PER: 0.0652\n2025-10-18 15:43:29,389: t15.2023.12.03 val PER: 0.0630\n2025-10-18 15:43:29,389: t15.2023.12.08 val PER: 0.0519\n2025-10-18 15:43:29,391: t15.2023.12.10 val PER: 0.0512\n2025-10-18 15:43:29,391: t15.2023.12.17 val PER: 0.1060\n2025-10-18 15:43:29,392: t15.2023.12.29 val PER: 0.0933\n2025-10-18 15:43:29,393: t15.2024.02.25 val PER: 0.0801\n2025-10-18 15:43:29,393: t15.2024.03.08 val PER: 0.1849\n2025-10-18 15:43:29,395: t15.2024.03.15 val PER: 0.1920\n2025-10-18 15:43:29,395: t15.2024.03.17 val PER: 0.0934\n2025-10-18 15:43:29,396: t15.2024.05.10 val PER: 0.1322\n2025-10-18 15:43:29,397: t15.2024.06.14 val PER: 0.1199\n2025-10-18 15:43:29,397: t15.2024.07.19 val PER: 0.1767\n2025-10-18 15:43:29,398: t15.2024.07.21 val PER: 0.0690\n2025-10-18 15:43:29,399: t15.2024.07.28 val PER: 0.0897\n2025-10-18 15:43:29,399: t15.2025.01.10 val PER: 0.2438\n2025-10-18 15:43:29,400: t15.2025.01.12 val PER: 0.1201\n2025-10-18 15:43:29,401: t15.2025.03.14 val PER: 0.3210\n2025-10-18 15:43:29,402: t15.2025.03.16 val PER: 0.1702\n2025-10-18 15:43:29,403: t15.2025.03.30 val PER: 0.2425\n2025-10-18 15:43:29,404: t15.2025.04.13 val PER: 0.1840\n2025-10-18 15:43:29,404: New best test PER 0.1181 --> 0.1181\n2025-10-18 15:43:29,405: Checkpointing model\n2025-10-18 15:43:30,835: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 15:46:49,437: Train batch 18200: loss: 2.03 grad norm: 31.91 time: 1.151\n2025-10-18 15:50:09,607: Train batch 18400: loss: 0.35 grad norm: 6.85 time: 1.095\n2025-10-18 15:53:29,322: Train batch 18600: loss: 0.67 grad norm: 15.29 time: 1.035\n2025-10-18 15:56:50,438: Train batch 18800: loss: 0.57 grad norm: 13.07 time: 0.969\n2025-10-18 16:00:13,155: Train batch 19000: loss: 0.38 grad norm: 8.58 time: 1.076\n2025-10-18 16:03:32,395: Train batch 19200: loss: 0.20 grad norm: 5.71 time: 1.051\n2025-10-18 16:06:52,341: Train batch 19400: loss: 0.32 grad norm: 16.99 time: 1.077\n2025-10-18 16:10:14,110: Train batch 19600: loss: 0.32 grad norm: 8.26 time: 0.969\n2025-10-18 16:13:36,044: Train batch 19800: loss: 0.20 grad norm: 7.63 time: 0.787\n2025-10-18 16:16:58,479: Train batch 20000: loss: 0.44 grad norm: 10.90 time: 0.872\n2025-10-18 16:16:58,481: Running test after training batch: 20000\n2025-10-18 16:17:41,953: Val batch 20000: PER (avg): 0.1156 CTC Loss (avg): 20.4240 time: 43.471\n2025-10-18 16:17:41,954: t15.2023.08.13 val PER: 0.0780\n2025-10-18 16:17:41,955: t15.2023.08.18 val PER: 0.0763\n2025-10-18 16:17:41,956: t15.2023.08.20 val PER: 0.0643\n2025-10-18 16:17:41,957: t15.2023.08.25 val PER: 0.0873\n2025-10-18 16:17:41,958: t15.2023.08.27 val PER: 0.1527\n2025-10-18 16:17:41,959: t15.2023.09.01 val PER: 0.0552\n2025-10-18 16:17:41,959: t15.2023.09.03 val PER: 0.1354\n2025-10-18 16:17:41,960: t15.2023.09.24 val PER: 0.0886\n2025-10-18 16:17:41,961: t15.2023.09.29 val PER: 0.1008\n2025-10-18 16:17:41,962: t15.2023.10.01 val PER: 0.1460\n2025-10-18 16:17:41,963: t15.2023.10.06 val PER: 0.0926\n2025-10-18 16:17:41,964: t15.2023.10.08 val PER: 0.1976\n2025-10-18 16:17:41,965: t15.2023.10.13 val PER: 0.1629\n2025-10-18 16:17:41,965: t15.2023.10.15 val PER: 0.1180\n2025-10-18 16:17:41,966: t15.2023.10.20 val PER: 0.1577\n2025-10-18 16:17:41,967: t15.2023.10.22 val PER: 0.0969\n2025-10-18 16:17:41,968: t15.2023.11.03 val PER: 0.1459\n2025-10-18 16:17:41,968: t15.2023.11.04 val PER: 0.0273\n2025-10-18 16:17:41,969: t15.2023.11.17 val PER: 0.0140\n2025-10-18 16:17:41,971: t15.2023.11.19 val PER: 0.0259\n2025-10-18 16:17:41,971: t15.2023.11.26 val PER: 0.0681\n2025-10-18 16:17:41,972: t15.2023.12.03 val PER: 0.0693\n2025-10-18 16:17:41,973: t15.2023.12.08 val PER: 0.0439\n2025-10-18 16:17:41,973: t15.2023.12.10 val PER: 0.0486\n2025-10-18 16:17:41,975: t15.2023.12.17 val PER: 0.0977\n2025-10-18 16:17:41,975: t15.2023.12.29 val PER: 0.0851\n2025-10-18 16:17:41,976: t15.2024.02.25 val PER: 0.0758\n2025-10-18 16:17:41,977: t15.2024.03.08 val PER: 0.1679\n2025-10-18 16:17:41,978: t15.2024.03.15 val PER: 0.1864\n2025-10-18 16:17:41,979: t15.2024.03.17 val PER: 0.0934\n2025-10-18 16:17:41,980: t15.2024.05.10 val PER: 0.1337\n2025-10-18 16:17:41,980: t15.2024.06.14 val PER: 0.1183\n2025-10-18 16:17:41,981: t15.2024.07.19 val PER: 0.1773\n2025-10-18 16:17:41,982: t15.2024.07.21 val PER: 0.0586\n2025-10-18 16:17:41,982: t15.2024.07.28 val PER: 0.1007\n2025-10-18 16:17:41,984: t15.2025.01.10 val PER: 0.2438\n2025-10-18 16:17:41,985: t15.2025.01.12 val PER: 0.1201\n2025-10-18 16:17:41,985: t15.2025.03.14 val PER: 0.3033\n2025-10-18 16:17:41,986: t15.2025.03.16 val PER: 0.1584\n2025-10-18 16:17:41,987: t15.2025.03.30 val PER: 0.2460\n2025-10-18 16:17:41,988: t15.2025.04.13 val PER: 0.1926\n2025-10-18 16:17:41,988: New best test PER 0.1181 --> 0.1156\n2025-10-18 16:17:41,989: Checkpointing model\n2025-10-18 16:17:43,462: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 16:20:59,761: Train batch 20200: loss: 0.36 grad norm: 7.22 time: 0.921\n2025-10-18 16:24:18,139: Train batch 20400: loss: 0.09 grad norm: 3.37 time: 1.369\n2025-10-18 16:27:34,218: Train batch 20600: loss: 0.50 grad norm: 10.92 time: 1.170\n2025-10-18 16:30:51,759: Train batch 20800: loss: 0.60 grad norm: 72.21 time: 1.057\n2025-10-18 16:34:09,089: Train batch 21000: loss: 0.14 grad norm: 5.64 time: 0.872\n2025-10-18 16:37:29,228: Train batch 21200: loss: 0.29 grad norm: 10.74 time: 0.892\n2025-10-18 16:40:44,107: Train batch 21400: loss: 0.36 grad norm: 9.28 time: 0.782\n2025-10-18 16:43:58,609: Train batch 21600: loss: 0.31 grad norm: 7.09 time: 0.914\n2025-10-18 16:47:17,817: Train batch 21800: loss: 0.25 grad norm: 7.89 time: 0.802\n2025-10-18 16:50:36,355: Train batch 22000: loss: 0.60 grad norm: 12.68 time: 1.095\n2025-10-18 16:50:36,357: Running test after training batch: 22000\n2025-10-18 16:51:15,939: Val batch 22000: PER (avg): 0.1149 CTC Loss (avg): 21.0031 time: 39.582\n2025-10-18 16:51:15,940: t15.2023.08.13 val PER: 0.0769\n2025-10-18 16:51:15,941: t15.2023.08.18 val PER: 0.0754\n2025-10-18 16:51:15,941: t15.2023.08.20 val PER: 0.0588\n2025-10-18 16:51:15,942: t15.2023.08.25 val PER: 0.0768\n2025-10-18 16:51:15,943: t15.2023.08.27 val PER: 0.1576\n2025-10-18 16:51:15,944: t15.2023.09.01 val PER: 0.0511\n2025-10-18 16:51:15,944: t15.2023.09.03 val PER: 0.1366\n2025-10-18 16:51:15,945: t15.2023.09.24 val PER: 0.0825\n2025-10-18 16:51:15,946: t15.2023.09.29 val PER: 0.1047\n2025-10-18 16:51:15,947: t15.2023.10.01 val PER: 0.1440\n2025-10-18 16:51:15,947: t15.2023.10.06 val PER: 0.0840\n2025-10-18 16:51:15,948: t15.2023.10.08 val PER: 0.1908\n2025-10-18 16:51:15,948: t15.2023.10.13 val PER: 0.1637\n2025-10-18 16:51:15,949: t15.2023.10.15 val PER: 0.1272\n2025-10-18 16:51:15,951: t15.2023.10.20 val PER: 0.1779\n2025-10-18 16:51:15,951: t15.2023.10.22 val PER: 0.1024\n2025-10-18 16:51:15,952: t15.2023.11.03 val PER: 0.1425\n2025-10-18 16:51:15,952: t15.2023.11.04 val PER: 0.0341\n2025-10-18 16:51:15,953: t15.2023.11.17 val PER: 0.0171\n2025-10-18 16:51:15,953: t15.2023.11.19 val PER: 0.0240\n2025-10-18 16:51:15,954: t15.2023.11.26 val PER: 0.0623\n2025-10-18 16:51:15,955: t15.2023.12.03 val PER: 0.0683\n2025-10-18 16:51:15,955: t15.2023.12.08 val PER: 0.0506\n2025-10-18 16:51:15,957: t15.2023.12.10 val PER: 0.0552\n2025-10-18 16:51:15,958: t15.2023.12.17 val PER: 0.0925\n2025-10-18 16:51:15,958: t15.2023.12.29 val PER: 0.0858\n2025-10-18 16:51:15,959: t15.2024.02.25 val PER: 0.0857\n2025-10-18 16:51:15,960: t15.2024.03.08 val PER: 0.1863\n2025-10-18 16:51:15,961: t15.2024.03.15 val PER: 0.1720\n2025-10-18 16:51:15,961: t15.2024.03.17 val PER: 0.0900\n2025-10-18 16:51:15,962: t15.2024.05.10 val PER: 0.1322\n2025-10-18 16:51:15,963: t15.2024.06.14 val PER: 0.1120\n2025-10-18 16:51:15,964: t15.2024.07.19 val PER: 0.1780\n2025-10-18 16:51:15,964: t15.2024.07.21 val PER: 0.0607\n2025-10-18 16:51:15,965: t15.2024.07.28 val PER: 0.1081\n2025-10-18 16:51:15,966: t15.2025.01.10 val PER: 0.2521\n2025-10-18 16:51:15,967: t15.2025.01.12 val PER: 0.1139\n2025-10-18 16:51:15,968: t15.2025.03.14 val PER: 0.2959\n2025-10-18 16:51:15,968: t15.2025.03.16 val PER: 0.1610\n2025-10-18 16:51:15,970: t15.2025.03.30 val PER: 0.2333\n2025-10-18 16:51:15,970: t15.2025.04.13 val PER: 0.1840\n2025-10-18 16:51:15,971: New best test PER 0.1156 --> 0.1149\n2025-10-18 16:51:15,972: Checkpointing model\n2025-10-18 16:51:17,447: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 16:54:38,781: Train batch 22200: loss: 0.15 grad norm: 5.08 time: 0.810\n2025-10-18 16:57:58,744: Train batch 22400: loss: 0.33 grad norm: 10.60 time: 1.158\n2025-10-18 17:01:18,410: Train batch 22600: loss: 0.13 grad norm: 4.88 time: 0.972\n2025-10-18 17:04:33,443: Train batch 22800: loss: 0.46 grad norm: 13.78 time: 0.901\n2025-10-18 17:07:49,302: Train batch 23000: loss: 0.31 grad norm: 8.76 time: 1.262\n2025-10-18 17:11:07,633: Train batch 23200: loss: 0.32 grad norm: 7.81 time: 1.028\n2025-10-18 17:14:27,464: Train batch 23400: loss: 0.19 grad norm: 4.70 time: 0.924\n2025-10-18 17:17:49,467: Train batch 23600: loss: 0.27 grad norm: 6.28 time: 1.115\n2025-10-18 17:21:06,567: Train batch 23800: loss: 0.33 grad norm: 8.17 time: 0.930\n2025-10-18 17:24:28,177: Train batch 24000: loss: 0.12 grad norm: 4.92 time: 1.045\n2025-10-18 17:24:28,178: Running test after training batch: 24000\n2025-10-18 17:25:09,360: Val batch 24000: PER (avg): 0.1111 CTC Loss (avg): 20.4402 time: 41.182\n2025-10-18 17:25:09,361: t15.2023.08.13 val PER: 0.0759\n2025-10-18 17:25:09,362: t15.2023.08.18 val PER: 0.0712\n2025-10-18 17:25:09,363: t15.2023.08.20 val PER: 0.0516\n2025-10-18 17:25:09,364: t15.2023.08.25 val PER: 0.0783\n2025-10-18 17:25:09,365: t15.2023.08.27 val PER: 0.1656\n2025-10-18 17:25:09,366: t15.2023.09.01 val PER: 0.0471\n2025-10-18 17:25:09,367: t15.2023.09.03 val PER: 0.1259\n2025-10-18 17:25:09,368: t15.2023.09.24 val PER: 0.0813\n2025-10-18 17:25:09,368: t15.2023.09.29 val PER: 0.1002\n2025-10-18 17:25:09,369: t15.2023.10.01 val PER: 0.1347\n2025-10-18 17:25:09,370: t15.2023.10.06 val PER: 0.0786\n2025-10-18 17:25:09,371: t15.2023.10.08 val PER: 0.1813\n2025-10-18 17:25:09,372: t15.2023.10.13 val PER: 0.1528\n2025-10-18 17:25:09,373: t15.2023.10.15 val PER: 0.1252\n2025-10-18 17:25:09,374: t15.2023.10.20 val PER: 0.1544\n2025-10-18 17:25:09,375: t15.2023.10.22 val PER: 0.1013\n2025-10-18 17:25:09,375: t15.2023.11.03 val PER: 0.1465\n2025-10-18 17:25:09,377: t15.2023.11.04 val PER: 0.0375\n2025-10-18 17:25:09,377: t15.2023.11.17 val PER: 0.0171\n2025-10-18 17:25:09,378: t15.2023.11.19 val PER: 0.0160\n2025-10-18 17:25:09,379: t15.2023.11.26 val PER: 0.0587\n2025-10-18 17:25:09,380: t15.2023.12.03 val PER: 0.0588\n2025-10-18 17:25:09,381: t15.2023.12.08 val PER: 0.0519\n2025-10-18 17:25:09,381: t15.2023.12.10 val PER: 0.0460\n2025-10-18 17:25:09,382: t15.2023.12.17 val PER: 0.0852\n2025-10-18 17:25:09,383: t15.2023.12.29 val PER: 0.0728\n2025-10-18 17:25:09,384: t15.2024.02.25 val PER: 0.0674\n2025-10-18 17:25:09,384: t15.2024.03.08 val PER: 0.1764\n2025-10-18 17:25:09,385: t15.2024.03.15 val PER: 0.1720\n2025-10-18 17:25:09,386: t15.2024.03.17 val PER: 0.0921\n2025-10-18 17:25:09,387: t15.2024.05.10 val PER: 0.1322\n2025-10-18 17:25:09,388: t15.2024.06.14 val PER: 0.1215\n2025-10-18 17:25:09,388: t15.2024.07.19 val PER: 0.1674\n2025-10-18 17:25:09,389: t15.2024.07.21 val PER: 0.0586\n2025-10-18 17:25:09,390: t15.2024.07.28 val PER: 0.0985\n2025-10-18 17:25:09,391: t15.2025.01.10 val PER: 0.2590\n2025-10-18 17:25:09,392: t15.2025.01.12 val PER: 0.1147\n2025-10-18 17:25:09,393: t15.2025.03.14 val PER: 0.2988\n2025-10-18 17:25:09,394: t15.2025.03.16 val PER: 0.1584\n2025-10-18 17:25:09,394: t15.2025.03.30 val PER: 0.2322\n2025-10-18 17:25:09,395: t15.2025.04.13 val PER: 0.1883\n2025-10-18 17:25:09,395: New best test PER 0.1149 --> 0.1111\n2025-10-18 17:25:09,396: Checkpointing model\n2025-10-18 17:25:10,860: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 17:28:28,623: Train batch 24200: loss: 0.38 grad norm: 8.23 time: 1.217\n2025-10-18 17:31:48,126: Train batch 24400: loss: 0.25 grad norm: 6.26 time: 1.259\n2025-10-18 17:35:11,569: Train batch 24600: loss: 0.28 grad norm: 6.91 time: 1.313\n2025-10-18 17:38:29,753: Train batch 24800: loss: 0.24 grad norm: 6.93 time: 0.788\n2025-10-18 17:41:46,912: Train batch 25000: loss: 0.21 grad norm: 6.56 time: 1.066\n2025-10-18 17:44:58,911: Train batch 25200: loss: 0.09 grad norm: 6.83 time: 1.017\n2025-10-18 17:48:18,865: Train batch 25400: loss: 0.10 grad norm: 4.57 time: 0.957\n2025-10-18 17:51:35,666: Train batch 25600: loss: 0.18 grad norm: 6.53 time: 0.917\n2025-10-18 17:54:52,150: Train batch 25800: loss: 0.15 grad norm: 5.68 time: 1.131\n2025-10-18 17:58:11,492: Train batch 26000: loss: 0.40 grad norm: 10.92 time: 0.816\n2025-10-18 17:58:11,493: Running test after training batch: 26000\n2025-10-18 17:58:51,417: Val batch 26000: PER (avg): 0.1109 CTC Loss (avg): 20.9790 time: 39.924\n2025-10-18 17:58:51,418: t15.2023.08.13 val PER: 0.0800\n2025-10-18 17:58:51,419: t15.2023.08.18 val PER: 0.0738\n2025-10-18 17:58:51,420: t15.2023.08.20 val PER: 0.0532\n2025-10-18 17:58:51,420: t15.2023.08.25 val PER: 0.0828\n2025-10-18 17:58:51,421: t15.2023.08.27 val PER: 0.1463\n2025-10-18 17:58:51,422: t15.2023.09.01 val PER: 0.0471\n2025-10-18 17:58:51,423: t15.2023.09.03 val PER: 0.1306\n2025-10-18 17:58:51,424: t15.2023.09.24 val PER: 0.0850\n2025-10-18 17:58:51,425: t15.2023.09.29 val PER: 0.0989\n2025-10-18 17:58:51,425: t15.2023.10.01 val PER: 0.1380\n2025-10-18 17:58:51,426: t15.2023.10.06 val PER: 0.0818\n2025-10-18 17:58:51,427: t15.2023.10.08 val PER: 0.1854\n2025-10-18 17:58:51,428: t15.2023.10.13 val PER: 0.1567\n2025-10-18 17:58:51,428: t15.2023.10.15 val PER: 0.1187\n2025-10-18 17:58:51,429: t15.2023.10.20 val PER: 0.1678\n2025-10-18 17:58:51,430: t15.2023.10.22 val PER: 0.1069\n2025-10-18 17:58:51,430: t15.2023.11.03 val PER: 0.1452\n2025-10-18 17:58:51,431: t15.2023.11.04 val PER: 0.0273\n2025-10-18 17:58:51,431: t15.2023.11.17 val PER: 0.0156\n2025-10-18 17:58:51,432: t15.2023.11.19 val PER: 0.0100\n2025-10-18 17:58:51,433: t15.2023.11.26 val PER: 0.0609\n2025-10-18 17:58:51,434: t15.2023.12.03 val PER: 0.0599\n2025-10-18 17:58:51,434: t15.2023.12.08 val PER: 0.0486\n2025-10-18 17:58:51,435: t15.2023.12.10 val PER: 0.0460\n2025-10-18 17:58:51,436: t15.2023.12.17 val PER: 0.0936\n2025-10-18 17:58:51,437: t15.2023.12.29 val PER: 0.0755\n2025-10-18 17:58:51,438: t15.2024.02.25 val PER: 0.0702\n2025-10-18 17:58:51,438: t15.2024.03.08 val PER: 0.1892\n2025-10-18 17:58:51,440: t15.2024.03.15 val PER: 0.1714\n2025-10-18 17:58:51,440: t15.2024.03.17 val PER: 0.0886\n2025-10-18 17:58:51,441: t15.2024.05.10 val PER: 0.1248\n2025-10-18 17:58:51,442: t15.2024.06.14 val PER: 0.1104\n2025-10-18 17:58:51,443: t15.2024.07.19 val PER: 0.1674\n2025-10-18 17:58:51,444: t15.2024.07.21 val PER: 0.0621\n2025-10-18 17:58:51,445: t15.2024.07.28 val PER: 0.0919\n2025-10-18 17:58:51,446: t15.2025.01.10 val PER: 0.2562\n2025-10-18 17:58:51,446: t15.2025.01.12 val PER: 0.1147\n2025-10-18 17:58:51,447: t15.2025.03.14 val PER: 0.2988\n2025-10-18 17:58:51,448: t15.2025.03.16 val PER: 0.1610\n2025-10-18 17:58:51,448: t15.2025.03.30 val PER: 0.2184\n2025-10-18 17:58:51,449: t15.2025.04.13 val PER: 0.1826\n2025-10-18 17:58:51,450: New best test PER 0.1111 --> 0.1109\n2025-10-18 17:58:51,450: Checkpointing model\n2025-10-18 17:58:52,934: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 18:02:10,816: Train batch 26200: loss: 0.53 grad norm: 14.35 time: 0.972\n2025-10-18 18:05:31,776: Train batch 26400: loss: 0.20 grad norm: 9.17 time: 1.296\n2025-10-18 18:08:46,588: Train batch 26600: loss: 0.11 grad norm: 5.71 time: 0.836\n2025-10-18 18:12:06,877: Train batch 26800: loss: 0.10 grad norm: 7.59 time: 0.743\n2025-10-18 18:15:27,365: Train batch 27000: loss: 0.19 grad norm: 6.23 time: 0.887\n2025-10-18 18:18:44,288: Train batch 27200: loss: 0.23 grad norm: 7.54 time: 0.914\n2025-10-18 18:22:03,196: Train batch 27400: loss: 0.38 grad norm: 12.69 time: 0.922\n2025-10-18 18:25:19,042: Train batch 27600: loss: 0.11 grad norm: 7.46 time: 1.113\n2025-10-18 18:28:36,207: Train batch 27800: loss: 0.30 grad norm: 8.68 time: 0.893\n2025-10-18 18:31:56,516: Train batch 28000: loss: 0.40 grad norm: 10.13 time: 0.968\n2025-10-18 18:31:56,517: Running test after training batch: 28000\n2025-10-18 18:32:36,990: Val batch 28000: PER (avg): 0.1100 CTC Loss (avg): 20.8612 time: 40.472\n2025-10-18 18:32:36,991: t15.2023.08.13 val PER: 0.0738\n2025-10-18 18:32:36,992: t15.2023.08.18 val PER: 0.0763\n2025-10-18 18:32:36,993: t15.2023.08.20 val PER: 0.0540\n2025-10-18 18:32:36,993: t15.2023.08.25 val PER: 0.0828\n2025-10-18 18:32:36,994: t15.2023.08.27 val PER: 0.1527\n2025-10-18 18:32:36,995: t15.2023.09.01 val PER: 0.0463\n2025-10-18 18:32:36,996: t15.2023.09.03 val PER: 0.1306\n2025-10-18 18:32:36,997: t15.2023.09.24 val PER: 0.0837\n2025-10-18 18:32:36,997: t15.2023.09.29 val PER: 0.1021\n2025-10-18 18:32:36,998: t15.2023.10.01 val PER: 0.1347\n2025-10-18 18:32:36,999: t15.2023.10.06 val PER: 0.0775\n2025-10-18 18:32:37,000: t15.2023.10.08 val PER: 0.1827\n2025-10-18 18:32:37,000: t15.2023.10.13 val PER: 0.1458\n2025-10-18 18:32:37,001: t15.2023.10.15 val PER: 0.1127\n2025-10-18 18:32:37,002: t15.2023.10.20 val PER: 0.1544\n2025-10-18 18:32:37,003: t15.2023.10.22 val PER: 0.1013\n2025-10-18 18:32:37,004: t15.2023.11.03 val PER: 0.1472\n2025-10-18 18:32:37,005: t15.2023.11.04 val PER: 0.0307\n2025-10-18 18:32:37,005: t15.2023.11.17 val PER: 0.0156\n2025-10-18 18:32:37,006: t15.2023.11.19 val PER: 0.0100\n2025-10-18 18:32:37,007: t15.2023.11.26 val PER: 0.0630\n2025-10-18 18:32:37,007: t15.2023.12.03 val PER: 0.0588\n2025-10-18 18:32:37,009: t15.2023.12.08 val PER: 0.0499\n2025-10-18 18:32:37,009: t15.2023.12.10 val PER: 0.0420\n2025-10-18 18:32:37,010: t15.2023.12.17 val PER: 0.0790\n2025-10-18 18:32:37,011: t15.2023.12.29 val PER: 0.0824\n2025-10-18 18:32:37,011: t15.2024.02.25 val PER: 0.0730\n2025-10-18 18:32:37,012: t15.2024.03.08 val PER: 0.1707\n2025-10-18 18:32:37,013: t15.2024.03.15 val PER: 0.1682\n2025-10-18 18:32:37,014: t15.2024.03.17 val PER: 0.0865\n2025-10-18 18:32:37,015: t15.2024.05.10 val PER: 0.1278\n2025-10-18 18:32:37,016: t15.2024.06.14 val PER: 0.1278\n2025-10-18 18:32:37,016: t15.2024.07.19 val PER: 0.1707\n2025-10-18 18:32:37,017: t15.2024.07.21 val PER: 0.0566\n2025-10-18 18:32:37,018: t15.2024.07.28 val PER: 0.0963\n2025-10-18 18:32:37,019: t15.2025.01.10 val PER: 0.2534\n2025-10-18 18:32:37,020: t15.2025.01.12 val PER: 0.1101\n2025-10-18 18:32:37,021: t15.2025.03.14 val PER: 0.3077\n2025-10-18 18:32:37,021: t15.2025.03.16 val PER: 0.1623\n2025-10-18 18:32:37,022: t15.2025.03.30 val PER: 0.2195\n2025-10-18 18:32:37,023: t15.2025.04.13 val PER: 0.1854\n2025-10-18 18:32:37,024: New best test PER 0.1109 --> 0.1100\n2025-10-18 18:32:37,025: Checkpointing model\n2025-10-18 18:32:38,515: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 18:35:56,223: Train batch 28200: loss: 0.16 grad norm: 7.39 time: 0.806\n2025-10-18 18:39:16,233: Train batch 28400: loss: 0.15 grad norm: 5.17 time: 0.928\n2025-10-18 18:42:34,625: Train batch 28600: loss: 0.26 grad norm: 9.18 time: 1.250\n2025-10-18 18:45:55,057: Train batch 28800: loss: 0.25 grad norm: 14.23 time: 1.047\n2025-10-18 18:49:12,128: Train batch 29000: loss: 0.10 grad norm: 3.70 time: 0.877\n2025-10-18 18:52:31,564: Train batch 29200: loss: 0.17 grad norm: 5.76 time: 0.933\n2025-10-18 18:55:57,090: Train batch 29400: loss: 0.16 grad norm: 5.97 time: 1.304\n2025-10-18 18:59:16,478: Train batch 29600: loss: 0.18 grad norm: 6.22 time: 1.370\n2025-10-18 19:02:35,882: Train batch 29800: loss: 0.08 grad norm: 7.25 time: 0.960\n2025-10-18 19:05:51,725: Train batch 30000: loss: 0.22 grad norm: 11.14 time: 0.987\n2025-10-18 19:05:51,726: Running test after training batch: 30000\n2025-10-18 19:06:31,767: Val batch 30000: PER (avg): 0.1085 CTC Loss (avg): 20.9147 time: 40.040\n2025-10-18 19:06:31,768: t15.2023.08.13 val PER: 0.0738\n2025-10-18 19:06:31,769: t15.2023.08.18 val PER: 0.0704\n2025-10-18 19:06:31,770: t15.2023.08.20 val PER: 0.0477\n2025-10-18 19:06:31,770: t15.2023.08.25 val PER: 0.0904\n2025-10-18 19:06:31,771: t15.2023.08.27 val PER: 0.1543\n2025-10-18 19:06:31,772: t15.2023.09.01 val PER: 0.0471\n2025-10-18 19:06:31,773: t15.2023.09.03 val PER: 0.1295\n2025-10-18 19:06:31,774: t15.2023.09.24 val PER: 0.0765\n2025-10-18 19:06:31,775: t15.2023.09.29 val PER: 0.1059\n2025-10-18 19:06:31,775: t15.2023.10.01 val PER: 0.1361\n2025-10-18 19:06:31,776: t15.2023.10.06 val PER: 0.0753\n2025-10-18 19:06:31,777: t15.2023.10.08 val PER: 0.1786\n2025-10-18 19:06:31,777: t15.2023.10.13 val PER: 0.1466\n2025-10-18 19:06:31,778: t15.2023.10.15 val PER: 0.1200\n2025-10-18 19:06:31,778: t15.2023.10.20 val PER: 0.1544\n2025-10-18 19:06:31,779: t15.2023.10.22 val PER: 0.1036\n2025-10-18 19:06:31,780: t15.2023.11.03 val PER: 0.1445\n2025-10-18 19:06:31,780: t15.2023.11.04 val PER: 0.0341\n2025-10-18 19:06:31,781: t15.2023.11.17 val PER: 0.0187\n2025-10-18 19:06:31,782: t15.2023.11.19 val PER: 0.0140\n2025-10-18 19:06:31,783: t15.2023.11.26 val PER: 0.0594\n2025-10-18 19:06:31,784: t15.2023.12.03 val PER: 0.0525\n2025-10-18 19:06:31,784: t15.2023.12.08 val PER: 0.0439\n2025-10-18 19:06:31,785: t15.2023.12.10 val PER: 0.0434\n2025-10-18 19:06:31,786: t15.2023.12.17 val PER: 0.0811\n2025-10-18 19:06:31,789: t15.2023.12.29 val PER: 0.0803\n2025-10-18 19:06:31,789: t15.2024.02.25 val PER: 0.0716\n2025-10-18 19:06:31,790: t15.2024.03.08 val PER: 0.1792\n2025-10-18 19:06:31,791: t15.2024.03.15 val PER: 0.1720\n2025-10-18 19:06:31,792: t15.2024.03.17 val PER: 0.0886\n2025-10-18 19:06:31,793: t15.2024.05.10 val PER: 0.1278\n2025-10-18 19:06:31,794: t15.2024.06.14 val PER: 0.1167\n2025-10-18 19:06:31,794: t15.2024.07.19 val PER: 0.1602\n2025-10-18 19:06:31,795: t15.2024.07.21 val PER: 0.0559\n2025-10-18 19:06:31,797: t15.2024.07.28 val PER: 0.0904\n2025-10-18 19:06:31,797: t15.2025.01.10 val PER: 0.2328\n2025-10-18 19:06:31,798: t15.2025.01.12 val PER: 0.1101\n2025-10-18 19:06:31,799: t15.2025.03.14 val PER: 0.2885\n2025-10-18 19:06:31,800: t15.2025.03.16 val PER: 0.1571\n2025-10-18 19:06:31,801: t15.2025.03.30 val PER: 0.2322\n2025-10-18 19:06:31,801: t15.2025.04.13 val PER: 0.1740\n2025-10-18 19:06:31,802: New best test PER 0.1100 --> 0.1085\n2025-10-18 19:06:31,803: Checkpointing model\n2025-10-18 19:06:33,280: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 19:09:50,408: Train batch 30200: loss: 0.13 grad norm: 4.08 time: 0.968\n2025-10-18 19:13:10,149: Train batch 30400: loss: 0.17 grad norm: 7.54 time: 1.169\n2025-10-18 19:16:30,631: Train batch 30600: loss: 0.19 grad norm: 25.40 time: 0.698\n2025-10-18 19:19:51,071: Train batch 30800: loss: 0.10 grad norm: 4.03 time: 0.924\n2025-10-18 19:23:11,996: Train batch 31000: loss: 0.04 grad norm: 4.49 time: 0.869\n2025-10-18 19:26:35,150: Train batch 31200: loss: 0.16 grad norm: 6.57 time: 1.369\n2025-10-18 19:29:52,647: Train batch 31400: loss: 0.12 grad norm: 4.01 time: 0.909\n2025-10-18 19:33:09,695: Train batch 31600: loss: 0.21 grad norm: 7.66 time: 0.973\n2025-10-18 19:36:26,026: Train batch 31800: loss: 0.06 grad norm: 2.94 time: 0.801\n2025-10-18 19:39:40,685: Train batch 32000: loss: 0.16 grad norm: 6.16 time: 1.207\n2025-10-18 19:39:40,687: Running test after training batch: 32000\n2025-10-18 19:40:20,111: Val batch 32000: PER (avg): 0.1088 CTC Loss (avg): 20.9108 time: 39.423\n2025-10-18 19:40:20,112: t15.2023.08.13 val PER: 0.0780\n2025-10-18 19:40:20,112: t15.2023.08.18 val PER: 0.0687\n2025-10-18 19:40:20,113: t15.2023.08.20 val PER: 0.0500\n2025-10-18 19:40:20,114: t15.2023.08.25 val PER: 0.0753\n2025-10-18 19:40:20,114: t15.2023.08.27 val PER: 0.1543\n2025-10-18 19:40:20,116: t15.2023.09.01 val PER: 0.0430\n2025-10-18 19:40:20,116: t15.2023.09.03 val PER: 0.1211\n2025-10-18 19:40:20,117: t15.2023.09.24 val PER: 0.0740\n2025-10-18 19:40:20,118: t15.2023.09.29 val PER: 0.1040\n2025-10-18 19:40:20,119: t15.2023.10.01 val PER: 0.1374\n2025-10-18 19:40:20,119: t15.2023.10.06 val PER: 0.0775\n2025-10-18 19:40:20,120: t15.2023.10.08 val PER: 0.1827\n2025-10-18 19:40:20,121: t15.2023.10.13 val PER: 0.1427\n2025-10-18 19:40:20,121: t15.2023.10.15 val PER: 0.1173\n2025-10-18 19:40:20,122: t15.2023.10.20 val PER: 0.1611\n2025-10-18 19:40:20,123: t15.2023.10.22 val PER: 0.1036\n2025-10-18 19:40:20,123: t15.2023.11.03 val PER: 0.1493\n2025-10-18 19:40:20,124: t15.2023.11.04 val PER: 0.0375\n2025-10-18 19:40:20,125: t15.2023.11.17 val PER: 0.0171\n2025-10-18 19:40:20,125: t15.2023.11.19 val PER: 0.0160\n2025-10-18 19:40:20,126: t15.2023.11.26 val PER: 0.0580\n2025-10-18 19:40:20,127: t15.2023.12.03 val PER: 0.0536\n2025-10-18 19:40:20,127: t15.2023.12.08 val PER: 0.0479\n2025-10-18 19:40:20,128: t15.2023.12.10 val PER: 0.0434\n2025-10-18 19:40:20,129: t15.2023.12.17 val PER: 0.0842\n2025-10-18 19:40:20,130: t15.2023.12.29 val PER: 0.0776\n2025-10-18 19:40:20,130: t15.2024.02.25 val PER: 0.0702\n2025-10-18 19:40:20,131: t15.2024.03.08 val PER: 0.1807\n2025-10-18 19:40:20,133: t15.2024.03.15 val PER: 0.1739\n2025-10-18 19:40:20,133: t15.2024.03.17 val PER: 0.0886\n2025-10-18 19:40:20,134: t15.2024.05.10 val PER: 0.1322\n2025-10-18 19:40:20,135: t15.2024.06.14 val PER: 0.1199\n2025-10-18 19:40:20,136: t15.2024.07.19 val PER: 0.1635\n2025-10-18 19:40:20,137: t15.2024.07.21 val PER: 0.0552\n2025-10-18 19:40:20,137: t15.2024.07.28 val PER: 0.0890\n2025-10-18 19:40:20,138: t15.2025.01.10 val PER: 0.2342\n2025-10-18 19:40:20,139: t15.2025.01.12 val PER: 0.1232\n2025-10-18 19:40:20,140: t15.2025.03.14 val PER: 0.2885\n2025-10-18 19:40:20,140: t15.2025.03.16 val PER: 0.1636\n2025-10-18 19:40:20,142: t15.2025.03.30 val PER: 0.2172\n2025-10-18 19:40:20,143: t15.2025.04.13 val PER: 0.1783\n2025-10-18 19:43:41,513: Train batch 32200: loss: 0.13 grad norm: 5.18 time: 0.827\n2025-10-18 19:46:59,746: Train batch 32400: loss: 0.10 grad norm: 4.12 time: 0.900\n2025-10-18 19:50:19,628: Train batch 32600: loss: 0.17 grad norm: 6.68 time: 1.040\n2025-10-18 19:53:36,961: Train batch 32800: loss: 0.09 grad norm: 3.21 time: 1.025\n2025-10-18 19:56:53,012: Train batch 33000: loss: 0.08 grad norm: 3.46 time: 0.905\n2025-10-18 20:00:11,905: Train batch 33200: loss: 0.78 grad norm: 10.63 time: 1.050\n2025-10-18 20:03:36,401: Train batch 33400: loss: 0.11 grad norm: 4.37 time: 1.289\n2025-10-18 20:06:56,376: Train batch 33600: loss: 0.13 grad norm: 4.54 time: 0.864\n2025-10-18 20:10:13,555: Train batch 33800: loss: 0.05 grad norm: 1.74 time: 0.977\n2025-10-18 20:13:30,741: Train batch 34000: loss: 0.06 grad norm: 4.35 time: 0.938\n2025-10-18 20:13:30,742: Running test after training batch: 34000\n2025-10-18 20:14:10,383: Val batch 34000: PER (avg): 0.1080 CTC Loss (avg): 21.0009 time: 39.640\n2025-10-18 20:14:10,384: t15.2023.08.13 val PER: 0.0728\n2025-10-18 20:14:10,385: t15.2023.08.18 val PER: 0.0712\n2025-10-18 20:14:10,386: t15.2023.08.20 val PER: 0.0500\n2025-10-18 20:14:10,386: t15.2023.08.25 val PER: 0.0813\n2025-10-18 20:14:10,387: t15.2023.08.27 val PER: 0.1495\n2025-10-18 20:14:10,388: t15.2023.09.01 val PER: 0.0446\n2025-10-18 20:14:10,389: t15.2023.09.03 val PER: 0.1306\n2025-10-18 20:14:10,389: t15.2023.09.24 val PER: 0.0716\n2025-10-18 20:14:10,390: t15.2023.09.29 val PER: 0.1059\n2025-10-18 20:14:10,391: t15.2023.10.01 val PER: 0.1380\n2025-10-18 20:14:10,391: t15.2023.10.06 val PER: 0.0775\n2025-10-18 20:14:10,392: t15.2023.10.08 val PER: 0.1759\n2025-10-18 20:14:10,393: t15.2023.10.13 val PER: 0.1482\n2025-10-18 20:14:10,394: t15.2023.10.15 val PER: 0.1173\n2025-10-18 20:14:10,396: t15.2023.10.20 val PER: 0.1577\n2025-10-18 20:14:10,396: t15.2023.10.22 val PER: 0.0958\n2025-10-18 20:14:10,397: t15.2023.11.03 val PER: 0.1472\n2025-10-18 20:14:10,398: t15.2023.11.04 val PER: 0.0273\n2025-10-18 20:14:10,398: t15.2023.11.17 val PER: 0.0156\n2025-10-18 20:14:10,399: t15.2023.11.19 val PER: 0.0140\n2025-10-18 20:14:10,400: t15.2023.11.26 val PER: 0.0587\n2025-10-18 20:14:10,401: t15.2023.12.03 val PER: 0.0515\n2025-10-18 20:14:10,401: t15.2023.12.08 val PER: 0.0473\n2025-10-18 20:14:10,402: t15.2023.12.10 val PER: 0.0460\n2025-10-18 20:14:10,403: t15.2023.12.17 val PER: 0.0800\n2025-10-18 20:14:10,403: t15.2023.12.29 val PER: 0.0755\n2025-10-18 20:14:10,405: t15.2024.02.25 val PER: 0.0702\n2025-10-18 20:14:10,405: t15.2024.03.08 val PER: 0.1778\n2025-10-18 20:14:10,406: t15.2024.03.15 val PER: 0.1776\n2025-10-18 20:14:10,407: t15.2024.03.17 val PER: 0.0830\n2025-10-18 20:14:10,408: t15.2024.05.10 val PER: 0.1263\n2025-10-18 20:14:10,408: t15.2024.06.14 val PER: 0.1215\n2025-10-18 20:14:10,409: t15.2024.07.19 val PER: 0.1648\n2025-10-18 20:14:10,411: t15.2024.07.21 val PER: 0.0524\n2025-10-18 20:14:10,411: t15.2024.07.28 val PER: 0.0890\n2025-10-18 20:14:10,412: t15.2025.01.10 val PER: 0.2424\n2025-10-18 20:14:10,412: t15.2025.01.12 val PER: 0.1124\n2025-10-18 20:14:10,413: t15.2025.03.14 val PER: 0.2929\n2025-10-18 20:14:10,414: t15.2025.03.16 val PER: 0.1597\n2025-10-18 20:14:10,416: t15.2025.03.30 val PER: 0.2115\n2025-10-18 20:14:10,416: t15.2025.04.13 val PER: 0.1797\n2025-10-18 20:14:10,417: New best test PER 0.1085 --> 0.1080\n2025-10-18 20:14:10,418: Checkpointing model\n2025-10-18 20:14:11,900: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 20:17:36,624: Train batch 34200: loss: 0.13 grad norm: 6.92 time: 0.901\n2025-10-18 20:20:56,755: Train batch 34400: loss: 0.09 grad norm: 16.23 time: 1.077\n2025-10-18 20:24:21,305: Train batch 34600: loss: 0.20 grad norm: 14.45 time: 0.850\n2025-10-18 20:27:41,709: Train batch 34800: loss: 0.07 grad norm: 2.84 time: 0.740\n2025-10-18 20:31:00,571: Train batch 35000: loss: 0.13 grad norm: 5.02 time: 0.896\n2025-10-18 20:34:15,097: Train batch 35200: loss: 0.10 grad norm: 4.57 time: 0.979\n2025-10-18 20:37:30,132: Train batch 35400: loss: 0.09 grad norm: 3.73 time: 0.705\n2025-10-18 20:40:46,326: Train batch 35600: loss: 0.19 grad norm: 14.96 time: 0.869\n2025-10-18 20:44:03,158: Train batch 35800: loss: 0.01 grad norm: 0.37 time: 1.211\n2025-10-18 20:47:20,825: Train batch 36000: loss: 0.14 grad norm: 5.19 time: 0.823\n2025-10-18 20:47:20,826: Running test after training batch: 36000\n2025-10-18 20:48:00,850: Val batch 36000: PER (avg): 0.1092 CTC Loss (avg): 21.0018 time: 40.023\n2025-10-18 20:48:00,851: t15.2023.08.13 val PER: 0.0759\n2025-10-18 20:48:00,852: t15.2023.08.18 val PER: 0.0721\n2025-10-18 20:48:00,852: t15.2023.08.20 val PER: 0.0508\n2025-10-18 20:48:00,853: t15.2023.08.25 val PER: 0.0858\n2025-10-18 20:48:00,854: t15.2023.08.27 val PER: 0.1495\n2025-10-18 20:48:00,854: t15.2023.09.01 val PER: 0.0446\n2025-10-18 20:48:00,855: t15.2023.09.03 val PER: 0.1318\n2025-10-18 20:48:00,856: t15.2023.09.24 val PER: 0.0716\n2025-10-18 20:48:00,856: t15.2023.09.29 val PER: 0.1027\n2025-10-18 20:48:00,857: t15.2023.10.01 val PER: 0.1387\n2025-10-18 20:48:00,858: t15.2023.10.06 val PER: 0.0753\n2025-10-18 20:48:00,860: t15.2023.10.08 val PER: 0.1719\n2025-10-18 20:48:00,860: t15.2023.10.13 val PER: 0.1490\n2025-10-18 20:48:00,861: t15.2023.10.15 val PER: 0.1220\n2025-10-18 20:48:00,862: t15.2023.10.20 val PER: 0.1611\n2025-10-18 20:48:00,862: t15.2023.10.22 val PER: 0.1058\n2025-10-18 20:48:00,863: t15.2023.11.03 val PER: 0.1486\n2025-10-18 20:48:00,864: t15.2023.11.04 val PER: 0.0273\n2025-10-18 20:48:00,865: t15.2023.11.17 val PER: 0.0156\n2025-10-18 20:48:00,865: t15.2023.11.19 val PER: 0.0160\n2025-10-18 20:48:00,866: t15.2023.11.26 val PER: 0.0580\n2025-10-18 20:48:00,867: t15.2023.12.03 val PER: 0.0536\n2025-10-18 20:48:00,868: t15.2023.12.08 val PER: 0.0466\n2025-10-18 20:48:00,868: t15.2023.12.10 val PER: 0.0526\n2025-10-18 20:48:00,869: t15.2023.12.17 val PER: 0.0800\n2025-10-18 20:48:00,870: t15.2023.12.29 val PER: 0.0769\n2025-10-18 20:48:00,870: t15.2024.02.25 val PER: 0.0660\n2025-10-18 20:48:00,871: t15.2024.03.08 val PER: 0.1835\n2025-10-18 20:48:00,872: t15.2024.03.15 val PER: 0.1714\n2025-10-18 20:48:00,872: t15.2024.03.17 val PER: 0.0858\n2025-10-18 20:48:00,874: t15.2024.05.10 val PER: 0.1337\n2025-10-18 20:48:00,875: t15.2024.06.14 val PER: 0.1230\n2025-10-18 20:48:00,876: t15.2024.07.19 val PER: 0.1674\n2025-10-18 20:48:00,877: t15.2024.07.21 val PER: 0.0524\n2025-10-18 20:48:00,877: t15.2024.07.28 val PER: 0.0897\n2025-10-18 20:48:00,878: t15.2025.01.10 val PER: 0.2424\n2025-10-18 20:48:00,879: t15.2025.01.12 val PER: 0.1116\n2025-10-18 20:48:00,880: t15.2025.03.14 val PER: 0.2973\n2025-10-18 20:48:00,881: t15.2025.03.16 val PER: 0.1636\n2025-10-18 20:48:00,882: t15.2025.03.30 val PER: 0.2218\n2025-10-18 20:48:00,882: t15.2025.04.13 val PER: 0.1783\n2025-10-18 20:51:23,000: Train batch 36200: loss: 0.07 grad norm: 2.98 time: 0.789\n2025-10-18 20:54:47,686: Train batch 36400: loss: 0.05 grad norm: 2.87 time: 1.093\n2025-10-18 20:58:09,201: Train batch 36600: loss: 0.22 grad norm: 8.57 time: 1.279\n2025-10-18 21:01:26,594: Train batch 36800: loss: 0.09 grad norm: 2.39 time: 0.906\n2025-10-18 21:04:42,634: Train batch 37000: loss: 0.06 grad norm: 2.08 time: 0.925\n2025-10-18 21:08:00,652: Train batch 37200: loss: 0.14 grad norm: 19.29 time: 0.993\n2025-10-18 21:11:20,048: Train batch 37400: loss: 0.08 grad norm: 4.09 time: 1.051\n2025-10-18 21:14:37,024: Train batch 37600: loss: 0.09 grad norm: 4.32 time: 0.706\n2025-10-18 21:17:55,620: Train batch 37800: loss: 0.13 grad norm: 5.33 time: 0.815\n2025-10-18 21:21:16,102: Train batch 38000: loss: 0.17 grad norm: 6.35 time: 1.203\n2025-10-18 21:21:16,103: Running test after training batch: 38000\n2025-10-18 21:21:55,970: Val batch 38000: PER (avg): 0.1079 CTC Loss (avg): 20.8020 time: 39.866\n2025-10-18 21:21:55,971: t15.2023.08.13 val PER: 0.0748\n2025-10-18 21:21:55,972: t15.2023.08.18 val PER: 0.0687\n2025-10-18 21:21:55,973: t15.2023.08.20 val PER: 0.0524\n2025-10-18 21:21:55,973: t15.2023.08.25 val PER: 0.0843\n2025-10-18 21:21:55,974: t15.2023.08.27 val PER: 0.1479\n2025-10-18 21:21:55,975: t15.2023.09.01 val PER: 0.0438\n2025-10-18 21:21:55,976: t15.2023.09.03 val PER: 0.1247\n2025-10-18 21:21:55,977: t15.2023.09.24 val PER: 0.0728\n2025-10-18 21:21:55,978: t15.2023.09.29 val PER: 0.1002\n2025-10-18 21:21:55,978: t15.2023.10.01 val PER: 0.1387\n2025-10-18 21:21:55,979: t15.2023.10.06 val PER: 0.0753\n2025-10-18 21:21:55,980: t15.2023.10.08 val PER: 0.1732\n2025-10-18 21:21:55,981: t15.2023.10.13 val PER: 0.1474\n2025-10-18 21:21:55,982: t15.2023.10.15 val PER: 0.1206\n2025-10-18 21:21:55,983: t15.2023.10.20 val PER: 0.1644\n2025-10-18 21:21:55,984: t15.2023.10.22 val PER: 0.1058\n2025-10-18 21:21:55,985: t15.2023.11.03 val PER: 0.1452\n2025-10-18 21:21:55,985: t15.2023.11.04 val PER: 0.0273\n2025-10-18 21:21:55,986: t15.2023.11.17 val PER: 0.0156\n2025-10-18 21:21:55,987: t15.2023.11.19 val PER: 0.0140\n2025-10-18 21:21:55,988: t15.2023.11.26 val PER: 0.0594\n2025-10-18 21:21:55,989: t15.2023.12.03 val PER: 0.0504\n2025-10-18 21:21:55,990: t15.2023.12.08 val PER: 0.0419\n2025-10-18 21:21:55,990: t15.2023.12.10 val PER: 0.0473\n2025-10-18 21:21:55,991: t15.2023.12.17 val PER: 0.0821\n2025-10-18 21:21:55,992: t15.2023.12.29 val PER: 0.0748\n2025-10-18 21:21:55,992: t15.2024.02.25 val PER: 0.0688\n2025-10-18 21:21:55,994: t15.2024.03.08 val PER: 0.1835\n2025-10-18 21:21:55,994: t15.2024.03.15 val PER: 0.1707\n2025-10-18 21:21:55,995: t15.2024.03.17 val PER: 0.0858\n2025-10-18 21:21:55,996: t15.2024.05.10 val PER: 0.1218\n2025-10-18 21:21:55,996: t15.2024.06.14 val PER: 0.1246\n2025-10-18 21:21:55,997: t15.2024.07.19 val PER: 0.1668\n2025-10-18 21:21:55,998: t15.2024.07.21 val PER: 0.0545\n2025-10-18 21:21:55,999: t15.2024.07.28 val PER: 0.0853\n2025-10-18 21:21:56,000: t15.2025.01.10 val PER: 0.2452\n2025-10-18 21:21:56,001: t15.2025.01.12 val PER: 0.1116\n2025-10-18 21:21:56,002: t15.2025.03.14 val PER: 0.2959\n2025-10-18 21:21:56,003: t15.2025.03.16 val PER: 0.1597\n2025-10-18 21:21:56,003: t15.2025.03.30 val PER: 0.2207\n2025-10-18 21:21:56,005: t15.2025.04.13 val PER: 0.1755\n2025-10-18 21:21:56,005: New best test PER 0.1080 --> 0.1079\n2025-10-18 21:21:56,006: Checkpointing model\n2025-10-18 21:21:57,495: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 21:25:16,945: Train batch 38200: loss: 0.24 grad norm: 7.99 time: 1.053\n2025-10-18 21:28:32,888: Train batch 38400: loss: 0.17 grad norm: 6.69 time: 0.848\n2025-10-18 21:31:50,952: Train batch 38600: loss: 0.15 grad norm: 8.13 time: 1.095\n2025-10-18 21:35:07,598: Train batch 38800: loss: 0.06 grad norm: 2.97 time: 0.958\n2025-10-18 21:38:27,506: Train batch 39000: loss: 0.07 grad norm: 3.67 time: 1.224\n2025-10-18 21:41:46,232: Train batch 39200: loss: 0.11 grad norm: 4.78 time: 0.780\n2025-10-18 21:45:03,216: Train batch 39400: loss: 0.21 grad norm: 7.74 time: 0.891\n2025-10-18 21:48:19,134: Train batch 39600: loss: 0.13 grad norm: 4.95 time: 1.027\n2025-10-18 21:51:39,363: Train batch 39800: loss: 0.07 grad norm: 4.10 time: 0.872\n2025-10-18 21:54:56,715: Running test after training batch: 39999\n2025-10-18 21:55:35,910: Val batch 39999: PER (avg): 0.1070 CTC Loss (avg): 20.8463 time: 39.195\n2025-10-18 21:55:35,911: t15.2023.08.13 val PER: 0.0707\n2025-10-18 21:55:35,912: t15.2023.08.18 val PER: 0.0696\n2025-10-18 21:55:35,913: t15.2023.08.20 val PER: 0.0524\n2025-10-18 21:55:35,914: t15.2023.08.25 val PER: 0.0828\n2025-10-18 21:55:35,914: t15.2023.08.27 val PER: 0.1463\n2025-10-18 21:55:35,915: t15.2023.09.01 val PER: 0.0422\n2025-10-18 21:55:35,916: t15.2023.09.03 val PER: 0.1259\n2025-10-18 21:55:35,916: t15.2023.09.24 val PER: 0.0728\n2025-10-18 21:55:35,917: t15.2023.09.29 val PER: 0.0989\n2025-10-18 21:55:35,918: t15.2023.10.01 val PER: 0.1347\n2025-10-18 21:55:35,919: t15.2023.10.06 val PER: 0.0797\n2025-10-18 21:55:35,920: t15.2023.10.08 val PER: 0.1719\n2025-10-18 21:55:35,920: t15.2023.10.13 val PER: 0.1451\n2025-10-18 21:55:35,921: t15.2023.10.15 val PER: 0.1226\n2025-10-18 21:55:35,922: t15.2023.10.20 val PER: 0.1577\n2025-10-18 21:55:35,923: t15.2023.10.22 val PER: 0.1036\n2025-10-18 21:55:35,923: t15.2023.11.03 val PER: 0.1425\n2025-10-18 21:55:35,924: t15.2023.11.04 val PER: 0.0307\n2025-10-18 21:55:35,925: t15.2023.11.17 val PER: 0.0171\n2025-10-18 21:55:35,925: t15.2023.11.19 val PER: 0.0140\n2025-10-18 21:55:35,926: t15.2023.11.26 val PER: 0.0580\n2025-10-18 21:55:35,927: t15.2023.12.03 val PER: 0.0525\n2025-10-18 21:55:35,927: t15.2023.12.08 val PER: 0.0419\n2025-10-18 21:55:35,929: t15.2023.12.10 val PER: 0.0473\n2025-10-18 21:55:35,930: t15.2023.12.17 val PER: 0.0800\n2025-10-18 21:55:35,930: t15.2023.12.29 val PER: 0.0728\n2025-10-18 21:55:35,931: t15.2024.02.25 val PER: 0.0660\n2025-10-18 21:55:35,932: t15.2024.03.08 val PER: 0.1778\n2025-10-18 21:55:35,933: t15.2024.03.15 val PER: 0.1707\n2025-10-18 21:55:35,934: t15.2024.03.17 val PER: 0.0844\n2025-10-18 21:55:35,934: t15.2024.05.10 val PER: 0.1263\n2025-10-18 21:55:35,935: t15.2024.06.14 val PER: 0.1215\n2025-10-18 21:55:35,936: t15.2024.07.19 val PER: 0.1641\n2025-10-18 21:55:35,937: t15.2024.07.21 val PER: 0.0531\n2025-10-18 21:55:35,938: t15.2024.07.28 val PER: 0.0868\n2025-10-18 21:55:35,938: t15.2025.01.10 val PER: 0.2410\n2025-10-18 21:55:35,939: t15.2025.01.12 val PER: 0.1116\n2025-10-18 21:55:35,940: t15.2025.03.14 val PER: 0.2944\n2025-10-18 21:55:35,940: t15.2025.03.16 val PER: 0.1610\n2025-10-18 21:55:35,941: t15.2025.03.30 val PER: 0.2184\n2025-10-18 21:55:35,941: t15.2025.04.13 val PER: 0.1755\n2025-10-18 21:55:35,942: New best test PER 0.1079 --> 0.1070\n2025-10-18 21:55:35,943: Checkpointing model\n2025-10-18 21:55:37,412: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint\n2025-10-18 21:55:37,687: Best avg val PER achieved: 0.10703\n2025-10-18 21:55:37,688: Total training time: 677.59 minutes\n","output_type":"stream"}],"execution_count":12}]}